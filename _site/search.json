[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "Probability is the mathematics of random events. In the context of modern statistics, probability theory provides a framework for studying the properties of samples, estimators, and inferences (the subjects of STAT 426 and STAT 427). This course will cover the axiomatic formulation of probability and formalize familiar concepts, including basic probability rules, random variables, distributions, and expectations. The course will also introduce concepts that play central roles in statistics, such as joint distributions, transformations, and conditional expectations.\nInstructor: Trevor Ruiz (he/him/his) [email] [website]\nLearning assistant: Edy Reynolds [email]\nClass meetings:\nOffice hours (OH) and learning assistant hours (LAH):\nFor office hours, drop ins are welcome (25-236) but availability is not guaranteed without an appointment. For LA hours, availability is drop-in only on a first come, first served basis."
  },
  {
    "objectID": "syllabus.html#catalog-description",
    "href": "syllabus.html#catalog-description",
    "title": "Course Syllabus",
    "section": "Catalog Description",
    "text": "Catalog Description\nRigorous development of probability theory. Probability axioms, combinatorial methods, conditional and marginal probability, independence, random variables, univariate and multivariate probability distributions, conditional distributions, transformations, order statistics, expectation and variance. Use of statistical simulation throughout the course. 4 lectures. Prerequisite: MATH 241; MATH 248 or CSC 348; and STAT 305. Recommended: STAT 301."
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Course Syllabus",
    "section": "Textbook",
    "text": "Textbook\nThe following text is required:\n\nHogg, McKean, & Craig, Introduction to Mathematical Statistics, 8th edition, Pearson.\n\nWe will cover chapters 1 through 3, and homework assignments will be drawn from the text.\nYou can purchase or rent a print or electronic copy through the bookstore, or rent an electronic copy for $11/mo through the [publisher’s website]. Additionally, a desk copy is available in StatLab (25-107B)."
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Course Syllabus",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of the course, successful students will:\n\n[L1] be familiar with the basic approaches to the definition of probability;\n[L2] understand basic theory to construct probability models for both discrete and continuous random variables;\n[L3] understand and be able to use distribution functions;\n[L4] understand the meaning and the applications of joint probability and joint distribution functions;\n[L5] understand the concept of expectations with respect to a given probability function;\n[L6] understand the meaning of conditional and marginal probability functions."
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Course Syllabus",
    "section": "Assessments",
    "text": "Assessments\nYour attainment of learning outcomes will be measured by homework assignments, in-class quizzes, and a comprehensive final exam. These are described below, with the relative contributions to final grades indicated parenthetically.\n\nHomework (20%). Homework problems will be given every class and due the next class meeting. Typically, problem sets will be very short (3 problems), and we will devote some time to working on them together in class on the day they are assigned. Homework assignments will be collected and checked for completion but not graded; therefore, it is your responsibility to ensure your work is correct by checking solutions when provided, consulting with classmates, and attending office hours.\nQuizzes (60%). Quizzes will be given approximately biweekly in class. These are summative assessments and you should prepare for them as you would an exam. You will be allowed 50 minutes to complete each quiz and up to 4 (one-sided) pages of notes. We will review solutions in class following each quiz, during which time you will be expected to self-assess your work before it is submitted for grading. You will have the opportunity to submit revisions to earn back a portion of missed credit.\nFinal exam (20%). A comprehensive common final exam will be given on Saturday, December 7, tentatively from 1:10pm – 4pm, with time and location to be confirmed as soon as such are issued by the registrar. The final will be open-book and open-note.\n\nYour scores will be recorded in Canvas for your reference along with an estimate of your running course total on a 0-100 scale. Tentatively, letter grades will span the following ranges: A (90, 100]; B (80, 90]; C (65, 80]; D (50, 65]; F [0, 50]. Please note these are rough estimates and subject to change without notice. Please also note that failure to adhere to course policies may result in a lower letter grade than would otherwise be assigned."
  },
  {
    "objectID": "syllabus.html#tentative-schedule",
    "href": "syllabus.html#tentative-schedule",
    "title": "Course Syllabus",
    "section": "Tentative schedule",
    "text": "Tentative schedule\nSubject to change at instructor discretion.\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings\nAssignments\n\n\n\n\n1 (9/23)\nSet theory, probability axioms\n1.2, 1.2.1, 1.2.2, 1.3\n\n\n\n2 (9/30)\nCounting methods\n1.3.1, 1.3.2\n\n\n\n3 (10/7)\nConditional probability, independence\n1.4, 1.4.1\nQuiz 1 (M)\n\n\n4 (10/14)\nRandom variables\n1.5, 1.6, 1.7\n\n\n\n5 (10/21)\nExpectation, transformations\n1.8, 1.9, 1.6.1, 1.7.2\nQuiz 2 (M)\n\n\n6 (10/28)\nRandom vectors, joint distributions\n2.1, 2.2, 2.3, 2.4\n\n\n\n7 (11/4)\nRandom vectors, joint distributions\n2.1, 2.2, 2.3, 2.4\nQuiz 3 (M)\n\n\n8 (11/12) Veteran’s day observed 11/11\nMultivariate transformations, order statistics\n2.6, 2.8\n\n\n\n9 (11/18)\nCommon distributions\n3.1, 3.2, 3.3\nQuiz 4 (M)\n\n\nFall break (11/25)\n\n\n\n\n\n10 (12/2)\nThe normal distribution\n3.4, 3.5\nQuiz 5 (M)\n\n\nFinals\n\n\nCommon final Saturday 12/7, time TBD"
  },
  {
    "objectID": "syllabus.html#tips-for-success",
    "href": "syllabus.html#tips-for-success",
    "title": "Course Syllabus",
    "section": "Tips for success",
    "text": "Tips for success\nI want you to succeed in this course. Below are three simple but effective habits:\n\ndon’t ignore the reading\n\nbefore class: skim the assigned reading for key concepts, definitions, and theorems\nafter class: reread carefully, paying special attention to portions discussed in class, and review relevant proofs and other derivations in detail\n\nform a study group with a regular meeting time\nbefore quizzes, do these two things in the following order:\n\nprepare a set of summary notes covering definitions and theorems based on the text, class notes, and your notes\ntry practice problems from the relevant sections in the book\n\nfind the “Goldilocks problems”: not too hard and not too obvious\ngo for variety (some proofs, some applied problems, multiple topics)\n\n\n\nIf you find yourself falling behind at any point during the quarter, or feel you are struggling with the course, please come and talk with me. The sooner you reach out, the more options I’ll have to help you."
  },
  {
    "objectID": "syllabus.html#policies",
    "href": "syllabus.html#policies",
    "title": "Course Syllabus",
    "section": "Policies",
    "text": "Policies\n\nTime commitment\nSTAT425 is a four-credit course, which corresponds to a minimum time commitment of 12 hours per week, including class meetings, reading, assignments, and study time.\nIn order to succeed in the course, you should expect to invest between 12 and 16 hours per week on average. Please let me know if you are regularly exceeding this amount or if you need help managing your time efficiently in the course. While I aim to keep the workload fairly even throughout the quarter, you should allow an extra hour or two in your schedule to accommodate week to week variations in workload as needed.\n\n\nAttendance and absences\nRegular attendance is essential for success in the course and required per University policy. Absences should be excusable, but you do not need to notify me unless you anticipate an extended absence or will miss an in-class assessment; I trust you to adhere to Cal Poly norms and policies regarding class attendance.\nIf you are absent and miss an in-class quiz, please contact me to arrange for a make-up; in this circumstance, University academic integrity policies prohibit you from discussing the quiz with students who took it in class.\n\n\nCollaboration\nCollaboration with classmates is encouraged on homework assignments but not allowed on quizzes or the final exam. If you work with a group on homework problems, you are expected to be an active contributor and prepare your own solutions in your own words and writing, and by submitting your work you are attesting that you have met this expectation. You should not distribute or accept copies of written solutions under any circumstances.\n\n\nCommunication and email\nI encourage you to ask questions in class and during office hours, since that is the only certain means of obtaining a response within a guaranteed time frame.\nI respond to most email within 24 weekday hours, but I cannot guarantee this response time and I occasionally miss messages altogether (though I try not to). I don’t answer emails at night or on weekends, so while you are welcome to write me outside of business hours, please don’t expect a reply until the following business day. I also sometimes get behind on answering emails, so please wait a few days (preferably one week if it’s not pressing) before sending a follow up or reminder.\nPlease do not ask technical questions about problems or course material by email.\n\n\nLate and missing work\nI understand that unexpected circumstances may arise and require you to temporarily rearrange your priorities and commitments on occasion during the quarter. You may, at any time during the quarter and without notice or penalty, use the following personal exceptions:\n\nturn in two homework assignments up to one week late\nmiss one homework assignment altogether\n\nOnce your personal exceptions are exhausted, homework assignments turned in up to one week late will be awarded 50% credit unless an extension is granted in advance, and missing assignments will be counted at zero credit.\nNo other late work — i.e., quizzes, quiz revisions, homework assignments turned in more than one week late, or the final exam — will be accepted unless an exception to this policy is granted. I will consider exceptions for personal and medical emergencies or other similarly unforeseeable circumstances.\n\n\nGrades and assessments\nI make my best effort to assess your work fairly and accurately and to apply assessment criteria consistently across the class. While I sometimes do so imperfectly, I am also aware that granting adjustments to scores or grades can disadvantage more reticent students and favor those more comfortable approaching me about credit awarded on course assessments.\nSo, in consideration of maintaining fairness and consistency, I ask that you limit requests for reassessment to clear mistakes, discrepancies, or oversights; and I also ask that you please do let me know if you think such an error has likely occurred. Please raise any such issues in a timely manner and not at the end of the quarter.\nPer University policy, faculty have final responsibility for grading criteria and grading judgment and have the right to alter student assessment or other parts of the syllabus during the term. It is not appropriate to attempt to negotiate scores or final grades. Once the term has concluded, final grades will only be changed in the case of clerical errors, without exception. If you feel your grade is unfairly assigned at the end of the course, you have the right to appeal it according to the procedure outlined here.\n\n\nAccommodations\nIt is University policy to provide, on a flexible and individualized basis, reasonable accommodations to students who have disabilities that may affect their ability to participate in course activities or to meet course requirements. Accommodation requests should be made through the Disability Resource Center (DRC).\n\n\nConduct and Academic Integrity\nYou are expected to be aware of and adhere to University policy regarding academic integrity and conduct. Detailed information on these policies, and potential repercussions of policy violations, can be found via the Office of Student Rights & Responsibilities (OSRR)."
  },
  {
    "objectID": "archive/f23/exams/midterm1.html",
    "href": "archive/f23/exams/midterm1.html",
    "title": "Midterm 1",
    "section": "",
    "text": "Instructions: read each problem carefully and provide answers in the space below. Your answers should display your reasoning clearly. If you use results from class in your answer, you can simply write, “by a theorem from class…”, and need not identify the result by name. You do not need to justify small steps in this way. You also do not need to calculate decimals or perform any burdensome calculations, but you should simplify your answers as much as possible. Please feel free to ask any clarifying questions about the problems. You have 50 minutes to complete the exam. Good luck!\n\nConsider a dart board comprising concentric circles with radii \\(r, \\frac{2}{3}r, \\frac{1}{3}r\\), as depicted below. Suppose this board is mounted on a unit square, and that for an inexperienced player, the probability of hitting each region is simply the area of that region. Assume that a throw will land within the unit square. The scoring rules are: 0 points are awarded if the board is missed, 1 point is awarded for hitting the outermost circle, 2 points are awarded for hitting the middle circle, and 3 points are awarded for hitting the inner circle. Find an expression for the probability of scoring \\(i\\) points on a single throw and show (or argue) that it is a valid probability measure.\n\n\n\n\n\n\n\nImagine you work a summer job at a theme park. At the entrance there are 5 ticket booths, and on a particular morning, there are 200 people waiting at the entrance. Everyone waiting must choose one line and buy a ticket individually. You’re working at the first ticket booth, and it takes you exactly 2 minutes to process each transaction. After you finish selling the morning tickets, you’ll take a break. If customers select their lines at random and no new customers arrive, what is the probability that you’ll be on break in exactly an hour?\nSuppose you’ve developed a machine learning model to classify activity patterns as “at work” or “not at work” based on user cell phone data. In testing prior to deployment, you assess performance on timepoints sampled from 1000 users whose information was not used in the development of the model; of those, 800 users were working during the sampled timepoints. Denote the event that a user is at work by \\(W\\) and the event that a user is predicted as at work by \\(V\\). Suppose you observe that for a randomly selected user from this test group: \\[\n\\begin{align*}\nP(V\\;|W) &= 0.9 \\\\\nP\\left(V^C\\;|W^C\\right) &= 0.3\n\\end{align*}\n\\]\n\nHow many of your model predictions were correct?\nWhat is the probability that the prediction was correct for a randomly selected user in the test group?\nIf your model predicts that a user in the test group is not at work, what is the probability that the prediction is correct?"
  },
  {
    "objectID": "archive/f23/exams/final-studyguide.html",
    "href": "archive/f23/exams/final-studyguide.html",
    "title": "Midterm 2 study guide",
    "section": "",
    "text": "Details and scope\nThe final exam will be given at the following times:\n\nSection 1 Wednesday, December 13, 4:10pm – 7:00pm in Construction Innovations Center C201\nSection 2 Friday, December 15, 4:10pm – 7:00pm in Construction Innovations Center C201\n\nYou are expected to sit for the exam with your scheduled section unless prior arrangements have been made. Logistical details are as follows:\n\n5 questions in length\ncalculators allowed but not required\nthree 8.5 x 11 pages of notes are allowed\nscratch paper provided\ntable of common distributions provided\n\nThe exam is comprehensive but focuses more on the latter portion of the class — random variables, expectations, and joint distributions. You can expect one question pertaining to probability axioms, basic probability rules, conditional probability, or probability on finite sample spaces; the remaining questions will focus on distributions of random variables and random vectors.\n\n\nKey concepts\nThe following is a short list of central concepts from the course.\n\nProbability axioms and basic probability calculus\nConditional probability\nIndependence of events\nBayes’ rule\nProbability on finite sample spaces\nRandom variables\nCumulative distribution function\nProbability mass function\nProbability density function\nExpectation of a function of a random variable\nMean, variance, and moments of a random variable\nMoment generating function\nCommon parametric families (e.g., Bernoulli, binomial, Gaussian, etc.)\nRandom vectors\nJoint PMF/PDFs\nTransformations of random vectors\nExpectation of a function of a random vector\nCovariance\nCorrelation\nConditional distributions\nIndependence of random variables\n\nThe final involves using these concepts for problem-solving based on definitions, associated properties, and examples developed in class and on homework assignments. Questions assess (a) your understanding of definitions and (b) your ability to apply them in a problem-solving context. Usually, the best starting point for resolving a problem is to identify givens and apply relevant definitions to move towards a solution; some credit is always awarded for answers that interpret problems appropriately. Thus, a good starting point for review is to cement your fluency with definitions of the above concepts and your understanding of how concepts are related.\n\n\nPreparations\nSuggested preparations are:\n\nReview class notes (posted and your own)\nReview homeworks, midterms, and posted solutions\nWork the practice problems below\nPrepare your notesheet(s)\n\nBe sure to also leave some time between your studying and sitting for the exam.\n\n\nPractice problems\nThese problems are intended to help you prepare. They are not representative of the length of exam problems, but are closely related to exam questions in some instances.\n\nLet \\(X \\sim N(\\mu, 1)\\).\n\nFind the distribution of \\(X^2\\) when \\(\\mu = 0\\). Is this from a common family?\nFind the distribution of \\(X^2\\) when \\(\\mu \\neq 0\\). Is this from a common family?\n\nSuppose \\(X, Y\\) are each binary random variables with joint distribution given by the table below. Based on the table:\n\nFind the marginal distributions\nDetermine whether \\(X \\perp Y\\)\nFind the conditional distribution of \\(X\\) given \\(Y = 0\\)\n\n\n\n\n\n\n\\(X = 0\\)\n\\(X = 1\\)\n\n\n\n\n\\(Y = 0\\)\n0.1\n0.4\n\n\n\\(Y = 1\\)\n0.2\n0.3\n\n\n\n\nLet \\((X, Y)\\) be distributed according to the PDF \\(f(x, y) = e^{-y}\\) for \\(0 &lt; x &lt; y &lt; \\infty\\).\n\nFind the marginal and conditional distributions of \\(X, Y, X|Y, Y|X\\)\nFind the distribution of \\(X + Y\\)\n\nLet \\(X, Y\\) be random variables and consider \\(U_1 = a_1X + b_1Y + c_1\\) and \\(U_2 = a_2X + b_2Y + c_2\\) for constants \\(a_1, a_2, b_1, b_2, c_1, c_2\\). Express \\(\\text{cov}(U_1, U_2)\\) in terms of the variances and covariance of \\(X, Y\\).\nLet \\(X, Y\\) be independent Poisson random variables, each with parameter \\(\\lambda\\). Find the MGF of \\(U = aX + bY\\).\nLet \\(X, Y\\) be uniformly distributed on the unit circle. Find the PDF and show that the random variables are uncorrelated, but dependent."
  },
  {
    "objectID": "archive/f23/exams/midterm1-studyguide.html",
    "href": "archive/f23/exams/midterm1-studyguide.html",
    "title": "Midterm 1 study guide",
    "section": "",
    "text": "This may be updated with further advice, suggestions, and the like — check back.\n\nMidterm details and scope\nThe midterm exam will be given in class on Thursday, October 19. Here are the logistical details:\n\n50 minutes in duration\nyou’re allowed one double-sided note sheet on standard 8.5 x 11 paper\nscratch paper will be available\n3 problems in length\nno calculator required\n\nProblems will draw on material from weeks 1-3, plus material covered Monday of week 4. You can expect one probability rules problem (week 1), one counting problem (week 2), and one conditional probability problem (week 3). Problems will emphasize application of course concepts; you need not expect to write\n\n\nTopics to review\n\nProbability rules\n\nset concepts and operations\nset cardinality\nDeMorgan’s laws\nevents, disjoint events, partitions\nprobability axioms\nprobability rules\n\nfinite additivity\ncomplement rule\nmonotonicity\nunion rule\nlimits for monotone sequences of events\ncountable sub-additivity (Boole’s inequality)\nfinite sub-additivity\nBonferroni’s inequality\n\nconstruction of probability measures on finite sample spaces\n\n\n\nCounting\n\nmultiplication rules for counting\ncounting rules for combinations of elements drawn from a finite set\n\nordered and drawn with replacement\nordered and drawn without replacement\nunordered and drawn with replacement\nunordered and drawn without replacement\n\n\n\n\nConditional probability\n\ndefinition of conditional probability\nmultiplication rules for conditional probability\nlaw of total probability\ndefinition of independence, pairwise independence, and mutual independence\nBayes’ theorem\nodds form of Bayes’ theorem\n\n\n\n\nExample problems\n\nSuppose you are performing \\(k\\) hypothesis tests at exact level \\(\\alpha\\), and assume all hypotheses are true. In other words, if \\(R_i\\) denotes the event that the \\(i\\)th hypothesis is rejected, i.e., of making an error, then \\(P(R_i) = \\alpha\\). However, when considering the tests as a group, there is a higher probability of making an error, as the individual error rates compound.It can be shown from the inclusion-exclusion principle that: \\[\nP\\left(\\bigcup_{i = 1}^k R_i\\right) \\geq \\sum_{i = 1}^k P(R_i) - \\sum_{1 \\leq i &lt; j \\leq k} P(R_i \\cap R_j)\n\\]\n\nUse Boole’s inequality to find an upper bound for the probability of making at least one error.\nUse the fact above to derive a lower bound for the probability of making at least one error, assuming that the tests are independent.\nCompute the bound for performing 20 tests at \\(\\alpha = 0.05\\).\nFind the smallest number of tests for which the lower bound in (ii) exceeds \\(\\frac{1}{2}\\). (Hint: write \\(0.05\\) as \\(\\frac{5}{100}\\); you may find it useful to know that \\(41^2 = 1681\\).)\n\n\nRemark: this last part is maybe a little more involved in terms of calculation than what I’d put on an exam.\n\nSuppose you are reading an article on admission rates at a selective college for different demographic groups reporting that 20% of applicants from underrepresented groups were admitted in 2022-2023 compared with 10% of other applicants.\n\nWhat is the probability that a randomly selected incoming student is from an underreppresented group if there were twice as many applicants from non-underrepresented groups?\nWhat is the probability that a randomly selected incoming student is from an underrepresented group if there were ten times as many applicants from non-underrepresented groups?\nUnder the scenario in (i), find the probability that a randomly selected applicant was admitted.\n\nImagine you’re out trick-or-treating with a friend and you arrive at a house handing out only two treats: Kit Kats and Snickers. The homeowner has mixed them up in a large pillowcase. You and your friend can each draw \\(n\\) pieces of candy; assume either treat is equally likely to be selected on each draw. Supposing that when you arrive, there are \\(N &gt; 2n\\) Snickers and \\(M &gt; 2n\\) Kit Kats, and you draw first, find an expression for the probability that you draw \\(k_1\\) Snickers and your friend draws \\(k_2\\) Snickers.\n\nExtra practice: find an expression for the probability that your friend draws \\(k_2\\) Snickers.\n\n\nSuggestions\nReview your notes, homeworks, and the posted lecture notes. Think a bit about what would be most useful to put on your note sheet, and consider prioritizing (i) things you are likely to forget (counting formulae, for instance) and (ii) definitions or results you’d like to have handy for reference (e.g., definition of conditional probability). You don’t need to use all of the space you’re allowed; sometimes fewer notes that are easier to navigate are more useful than dense notes.\nTry some example problems from the book. Look to the end of chapters 1 and 2, and pick a few that seem comfortable and relate to specific material you’d like to practice. No need to choose difficult problems — this can actually be counterproductive. Remember, you want practice and repetition, not a struggle.\nWhen you sit to take the test, read the whole exam first before beginning work. Take a moment to think, and start with the problem you feel most confident in — this can help to build a bit of momentum to carry you through any portions you find challenging."
  },
  {
    "objectID": "archive/f23/exams/midterm2-studyguide.html",
    "href": "archive/f23/exams/midterm2-studyguide.html",
    "title": "Midterm 2 study guide",
    "section": "",
    "text": "This may be updated with further advice, suggestions, and the like — check back.\n\nMidterm details and scope\nThe midterm exam will be given in class on Thursday, November 16. Here are the logistical details:\n\n50 minutes in duration\nyou’re allowed one double-sided note sheet on standard 8.5 x 11 paper\nscratch paper will be available\n3 problems in length\nno calculator required, but calculators allowed\na table of common distributions will be provided\n\nThe exam will focus on material covered since the last exam, i.e., from weeks 4 through 7: random variables, common distributions, and expectation. Questions will focus on your ability to use distribution functions (CDF, PDF/PMF, MGF) to characterize distributions of random variables and find expected values. You can expect at least one question to require you to apply these concepts in the context of an unfamiliar distribution.\n\n\nTopics for review\nFor this exam, you should be prepared to demonstrate you understand the following:\n\ndefinition of a random variable\ndefinition and properties of the cumulative distribution function\ndistinction between discrete and continuous random variables\ndefinition and properties of the probability mass function and the probability density function\ndefinition and properties of expected value\nhow to compute expectations of functions of discrete and continuous random variables\ndefinition of variance\nhow to compute the variances of discrete and continuous random variables\ndefinition and use of moment generating functions\nPDF/PMFs, expectations, variances, and moment generating functions for common probability distributions\nproperties of the Gaussian distribution\n\nPlease note that you will be provided with a table of the distributions we covered in class, so you do not need to include these on your note sheet.\n\n\nTo prepare\nReview posted course notes, focusing especially on definitions, results (lemma, theorem, corollary), and keywords in blue appearing outside of the proof and example boxes. Consult also your notes from class; see if you can boil the main ideas and results down to a few pages of notes. Skim the examples and identify which results and concepts are being used in each example. While you’re not responsible for reproducing the proofs or calculations that we cover in class, they do illustrate useful techniques that may come in handy.\n\n\nPractice problems\nThe problems below vary in length and difficulty.\n\nLet \\(\\log X \\sim N(\\mu, \\sigma^2)\\). If \\(k\\) is an arbitrary positive integer, find an expression for \\(\\mathbb{E}X^k\\) and use it to compute the first three moments.\nIntuitively, a log transformation should reduce the skewness of a distribution. The skewness of a random variable is defined as the third standardized moment, i.e., the expectation \\(\\mathbb{E}\\left[\\left(\\frac{X - \\mu}{\\sigma}\\right)^3\\right] = \\frac{\\mu_3 - 3\\mu_1\\sigma^2 - \\mu_1^3}{\\sigma^3}\\), where \\(\\mu_k\\) denotes the \\(k\\)th moment and \\(\\sigma\\) denotes the square root of the variance, i.e., \\(\\sigma = \\sqrt{\\mu_2 - \\mu_1^2}\\). Let \\(\\log(X) \\sim N(0, 1)\\); show that the skewness of \\(X\\) is larger than the skewness of \\(\\log(X)\\).\nShow that the skewness of a gamma random variable decreases with the value of the parameter \\(\\alpha\\).\nLet \\(\\log(X) \\sim \\Gamma(\\alpha, \\beta)\\). Find the mean and variance of \\(X\\).\nSuppose that you have an hour to answer letters each day, and the number of letters that arrive follows a Poisson distribution. If \\(k\\) letters arrive, you can dedicate \\(\\frac{1}{k}\\) hours to answering each letter. What is the distribution of answering times? Let \\(X \\sim \\text{Poisson}(\\lambda)\\) and \\(Y = \\frac{1}{X}\\). Find the PMF of \\(Y\\).\nLet \\(f(x) = c\\left[1 - (x - 1)^2\\right]\\). Find the value of \\(c\\) and support set for which \\(f\\) is a PDF.\nLet \\(X \\sim \\text{exponential}(\\beta)\\) and \\(c &gt; 0\\) and define \\[\nY = \\begin{cases}\n  1 &,\\; X &gt; c \\\\\n  0 &,\\; X \\leq c\n\\end{cases}\n\\] Find the distribution of \\(Y\\) and its mean and variance.\n\nTextbook problems: end of sections 3.1 – 3.3, 3.8, 4.1.\n\n\nCommon distributions\nYou will have access during the test to this table of common distributions.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nPDF/PMF\nSupport\nParameters\nMean\nVariance\nMGF\n\n\n\n\nDiscrete uniform\n\\(\\frac{1}{n}\\)\n\\(\\{a_1, \\dots, a_n\\}\\)\nnone\n\\(\\bar{a}\\)\n\\(\\frac{1}{n}\\sum_{i = 1}^n (a_i - \\bar{a})^2\\)\n\\(\\frac{1}{n}\\sum_{i = 1}^n e^{ta_i}\\)\n\n\nBernoulli\n\\(p^x (1 - p)^{1 - x}\\)\n\\(\\{0, 1\\}\\)\n\\(p \\in (0, 1)\\)\n\\(p\\)\n\\(p(1 - p)\\)\n\\(1 - p + pe^t\\)\n\n\nGeometric\n\\((1 - p)^x p\\)\n\\(\\mathbb{N}\\)\n\\(p \\in (0, 1)\\)\n\\(\\frac{1 - p}{p}\\)\n\\(\\frac{1 - p}{p^2}\\)\n\\(\\frac{p}{1 - (1 - p)e^t}\\)\n\\(t &lt; -\\log(1 - p)\\)\n\n\nBinomial\n\\({n \\choose p} p^x (1 - p)^{n - x}\\)\n\\(\\{0, 1, \\dots, n\\}\\)\n\\(n \\in \\mathbb{Z}^+\\)\n\\(p \\in (0, 1)\\)\n\\(np\\)\n\\(np(1 - p)\\)\n\\(\\left(1 - p + pe^t\\right)^n\\)\n\n\nNegative binomial\n\\({r + x - 1 \\choose x} p^r (1 - p)^x\\)\n\\(\\mathbb{N}\\)\n\\(r \\in \\mathbb{Z}^+\\)\n\\(p \\in (0, 1)\\)\n\\(\\frac{r(1 - p)}{p}\\)\n\\(\\frac{r(1 - p)}{p^2}\\)\n\\(\\left(\\frac{p}{1 - (1 - p)e^t}\\right)^r\\)\n\\(t &lt; -\\log(1 - p)\\)\n\n\nPoisson\n\\(\\frac{\\lambda^xe^{-\\lambda}}{x!}\\)\n\\(\\mathbb{N}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\\(\\exp\\left\\{-\\lambda(1 - e^t)\\right\\}\\)\n\n\nContinuous uniform\n\\(\\frac{1}{b - a}\\)\n\\((a, b)\\)\n\\(-\\infty &lt; a &lt; b &lt; \\infty\\)\n\\(\\frac{b + a}{2}\\)\n\\(\\frac{(b - a)^2}{12}\\)\n\\(\\begin{cases}\\frac{e^{ta} - e^{tb}}{t(b - a)} &,\\;t\\neq 0 \\\\ 1 &,\\; t = 0 \\end{cases}\\)\n\n\nGaussian\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2}\\)\n\\(\\mathbb{R}\\)\n\\(\\mu \\in \\mathbb{R}, \\sigma &gt; 0\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\exp\\left\\{\\mu t + \\frac{1}{2}t^2\\sigma^2\\right\\}\\)\n\n\nGamma\n\\(\\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}}\\)\n\\(x &gt; 0\\)\n\\(\\alpha &gt; 0, \\beta &gt; 0\\)\n\\(\\alpha\\beta\\)\n\\(\\alpha\\beta^2\\)\n\\(\\left(1 - \\beta t\\right)^{-\\alpha}\\)"
  },
  {
    "objectID": "archive/f23/notes/week10-independence.html",
    "href": "archive/f23/notes/week10-independence.html",
    "title": "Conditional distributions and independence",
    "section": "",
    "text": "The concept of conditional probability extends naturally to distributions of random variables. This extension is useful for describing probability distributions that depend on another variable. For example: data on health outcomes by age provides information about the conditional distribution of outcomes as it depends on age; the distribution of daytime temperatures depends on time of year and location; the distributions of measures of educational attainment depend on socioeconomic indicators. In all of these cases, we are considering the conditional distribution of a variable of interest (health outcomes, daytime temperatures, educational attainment) given other variables (age, time/location, socioeconomic variables); this may be of use in modeling the dependence, correcting by conditioning variables to obtain joint distributions, or a variety of other purposes. Here we’ll focus on the relationship between joint probability distributions and conditional distributions, and illustrate some applications of conditional distributions.\nIf \\((X_1, X_2)\\) is a random variable with some joint distribution, consider the events \\(\\{X_1 \\in A\\}\\) and \\(\\{X_2 \\in B\\}\\) and assume \\(P(X_2 \\in B) &gt; 0\\). The conditional probability of the former event given the latter is: \\[\nP\\left(X_1 \\in A | X_2 \\in B\\right) = \\frac{P(X_1 \\in A, X_2 \\in B)}{P(X_2 \\in B)}\n\\] The probability in the numerator can be computed from the joint distribution, and the probability in the denominator can be computed from the marginal distribution of \\(X_2\\). As a function of the set \\(A\\), this can be viewed as a probability measure, and suggests that the distribution function (PMF/PDF) that characterizes it can be obtained from the joint and marginal PMF/PDF.\nIf \\(X_1, X_2\\) are discrete and \\(x_2\\) is any value such that \\(P(X_2 = x_2) &gt; 0\\), then the conditional PMF of \\(X_1\\) given \\(X_2 = x_2\\) is: \\[\nP(X_1 = x_1 | X_2 = x_2) = \\frac{P(X_1 = x_1, X_2 = x_2)}{P(X_2 = x_2)}\n\\] It is easy to verify that this is a PMF since it takes values between 0 and 1 and it is easy to check that summing over the marginal support of \\(X_1\\) gives 1: \\[\n\\sum_{x_1} P(X_1 = x_1 | X_2 = x_2) = \\frac{\\sum_{x_1}P(X_1 = x_1, X_2 = x_2)}{P(X_2 = x_2)} = \\frac{P(X_2 = x_2)}{P(X_2 = x_2)} = 1\n\\] The functional form in any particular instance will be the same for every value of \\(x_2\\), so this is often referred to as the conditional distribution of \\(X_1\\) given \\(X_2\\), without reference to the specific value on which one is conditioning.\nIn the continuous case, the contional PDF is constructed in analogous fashion: \\[\nf_{1|2}(x_1) = \\frac{f(x_1, x_2)}{f_2 (x_2)}\n\\] This is a well-defined PDF for every \\(x_2\\) in the marginal support of \\(X_2\\), since it is clearly nonnegative and: \\[\n\\int_{-\\infty}^\\infty f_{1|2}(x_1)dx_1 = \\frac{\\int_{-\\infty}^\\infty f(x_1, x_2)dx_1}{f_2(x_2)} = \\frac{f_2(x_2)}{f_2(x_2)} = 1\n\\]\n\n\n\n\n\n\nExample: finding a conditional PDF\n\n\n\nLet \\(X_1, X_2\\) be uniform on the triangle \\(0 &lt; x_1 &lt; x_2 &lt; 1\\), so that: \\[\nf(x_1, x_2) = 2\n\\;,\\qquad 0 &lt; x_1 &lt; x_2 &lt; 1\n\\] The marginal distributions are then given by the PDFs: \\[\n\\begin{align*}\nf_1(x_1) &= \\int_{x_1}^1 2 dx_2 = 2(1 - x_1) \\;,\\quad 0 &lt; x_1 &lt; 1 \\\\\nf_2(x_2) &= \\int_0^{x_2} 2 dx_1 = 2x_2 \\;,\\quad 0 &lt; x_2 &lt; 1\n\\end{align*}\n\\] So the conditional distribution of \\(X_1\\) given \\(X_2\\) is: \\[\nf_{1|2}(x_1) = \\frac{f(x_1, x_2)}{f_2(x_2)} = \\frac{2}{2x_2} = \\frac{1}{x_2}\n\\;,\\quad\n0 &lt; x_1 &lt; x_2\n\\] Notice here that the support set must be determined based on the joint distribution. This is a uniform distribution on the interval \\((0, x_2)\\). One may write: \\[\n(X_1 | X_2 = x_2)  \\sim \\text{uniform}(0, x_2)\n\\]\nCheck your understanding Show that \\((X_2|X_1 = x_1) \\sim \\text{uniform}(x_1, 1)\\).\n\n\n\nConditional expectation\nThe conditional expectation of \\(g(X_1)\\) given \\(X_2\\) is defined as: \\[\n\\mathbb{E}[g(X_1) | X_2 = x_2] = \\begin{cases}\n  \\sum_{x_1} g(x_1) P(X_1 = x_1 | X_2 = x_2) \\quad\\text{(discrete case)}\\\\\n  \\int_{-\\infty}^\\infty g(x_1) f_{1 | 2}(x_1)dx_1 \\quad\\text{(continuous case)}\n  \\end{cases}\n\\]\nThat is, conditional expectation is simply an expected value computed in the usual way but using the conditional mass or density function in place of the marginal. Similarly, the conditional variance is defined as:\n\\[\n\\text{var}[X_1 | X_2 = x_2] = \\begin{cases}\n  \\sum_{x_1} (x_1 - \\mathbb{E}(X_1|X_2 = x_2))^2 P(X_1 = x_1 | X_2 = x_2) \\quad\\text{(discrete case)}\\\\\n  \\int_{-\\infty}^\\infty (x_1 - \\mathbb{E}(X_1|X_2 = x_2))^2 f_{1 | 2}(x_1)dx_1 \\quad\\text{(continuous case)}\n  \\end{cases}\n\\]\n\n\n\n\n\n\nExample: computing a conditional mean\n\n\n\nIn the previous example, the conditional expectations are easy to find based on the properties of the uniform distribution: \\[\n\\begin{align*}\n\\mathbb{E}[X_1 | X_2 = x_2] &= \\frac{x_2}{2} \\\\\n\\mathbb{E}[X_2 | X_1 = x_1] &= \\frac{x_1 + 1}{2}\n\\end{align*}\n\\] By comparison, the marginal expectations can be found to be: \\[\n\\begin{align*}\n\\mathbb{E}X_1 &= \\int_0^1 x_1\\cdot 2(1 - x_1) dx_1 = \\frac{1}{3} \\\\\n\\mathbb{E}X_2 &= \\int_0^1 x_2 \\cdot 2x_2 dx_2 = \\frac{2}{3}\n\\end{align*}\n\\]\n\n\nConsider the example immediately above, and notice that the conditional means are functions of the value of the other “conditioning” variable. That will be generally true, that is: \\[\n\\mathbb{E}[X_1|X_2 = x_2] = h(x_2)\n\\] Thus, the conditional expectation can be considered as a function of the random variable \\(X_2\\) and therefore as itself a random variable, that is, \\[\n\\mathbb{E}(X_1|X_2) = h(X_2)\n\\] So conditional expectations are themselves random variables with their own distributions, means, variances, and the like. The same is true of conditional variances. This leads, among other things, to two classic results regarding iterated expectations.\nTheorem (Total expectation). For any random variables \\(X, Y\\): \\[\\mathbb{E}X = \\mathbb{E}\\left[\\mathbb{E}(X|Y)\\right]\\]\nWe will review the proof in class. Check that the result holds in the example above.\nTheorem (Total variance). For any random variables \\(X, Y\\): \\[\\text{var}X = \\text{var}\\left[\\mathbb{E}(X|Y)\\right] + \\mathbb{E}\\left[\\text{var}(X|Y)\\right]\\]\nWe will review the proof in class. Check that the result holds in the example above.\n\n\nIndependence\nIntuitively, random variables are independent if the value of one does not affect the distribution of another. In other words, \\(X, Y\\) are independent if for every \\(B\\): \\[\nP(X \\in A | Y \\in B) = P(X \\in A)\n\\] Just as with independent events, however, we do not define independent random variables according to whether conditional and marginal probabilities match, but rather according to whether joint probabilities factor. That is, \\(X\\) and \\(Y\\) are independent just in case for every \\(A, B\\): \\[\nP(X \\in A, Y \\in B) = P(X \\in A)P(Y \\in B)\n\\] In terms of distribution functions, this is equivalent to the following condition: \\[\n\\begin{cases}\nf(x, y) = f(x)f(y) \\quad&\\text{(continuous case)} \\\\\nP(X = x, Y = y) = P(X = x)P(Y = y) \\quad&\\text{(discrete case)}\n\\end{cases}\n\\]\nWe write \\(X \\perp Y\\) to indicate that the random variables are independent. Since the condition above involves knowing the marginal PDF/PMFs, the following theorem provides a useful heuristic for checking independence.\nTheorem (factorization theorem). \\(X \\perp Y\\) if and only if there exist functions \\(g, h\\) such that, if \\(f\\) is the joint PMF/PDF: \\[\nf(x, y) = g(x)h(y)\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nLet \\(X_1, X_2\\) be distributed according to the joint PDF: \\[\nf(x_1, x_2) = \\frac{1}{2\\pi}\\exp\\left\\{-\\frac{1}{2}\\left[(x_1 - \\mu_1)^2 + (x_2 - \\mu_2)^2\\right]\\right\\}\n\\;,\\quad (x_1, x_2) \\in \\mathbb{R}^2\n\\] The factorization theorem entails almost immediately that \\(X_1 \\perp X_2\\), without knowing the marginal distributions, since the joint density can be written: \\[\nf(x_1, x_2) = \\underbrace{\\left[\\frac{1}{2\\pi}\\exp\\left\\{-\\frac{1}{2}(x_1 - \\mu_1)^2\\right\\}\\right]}_{g(x_1)}\n\\underbrace{\\left[\\exp\\left\\{-\\frac{1}{2} (x_2 - \\mu_2)^2\\right\\}\\right]}_{h(x_2)}\n\\;,\\quad x_1 \\in \\mathbb{R}, x_2 \\in \\mathbb{R}\n\\] Notice that the support set also has to be expressible as a Cartesian product.\n\n\nLemma. If \\(X\\perp Y\\) then the support of \\((X, Y)\\) is a Cartesian product.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nThe contrapositive tells us that if the joint support set of any random variables is not a Cartesian product, then they cannot be independent. Moreover, the lemma makes it rather straightforward to establish the following result.\nCorrollary. If \\(X \\perp Y\\) and \\(g, h\\) are functions whose expectations exist then: \\[\n\\mathbb{E}\\left[g(X)h(Y)\\right] = \\mathbb{E}[g(X)]\\mathbb{E}[h(Y)]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\nConditional probability models\nNotice that the definitions of conditional PDF/PMFs entail that the joint PDF/PMFs can be obtained from either conditional and the remaining marginal: \\[\n\\begin{align*}\nP(X_1 = x_1, X_2 = x_2) &= P(X_1 = x_1 | X_2 = x_2) P(X_2 = x_2) \\quad\\text{(discrete case)} \\\\\nf(x_1, x_2) &= f_{1 | 2}(x_1) f_2 (x_2) \\quad\\text{(continuous case)}\n\\end{align*}\n\\]\nThis allows one to construct models for multivariate processes from conditional distributions in a hierarchical fashion. For example: \\[\n\\begin{align*}\n(X_1 | X_2 = x_2) \\sim f(x_1) \\\\\nX_2 \\sim g(x_2)\n\\end{align*}\n\\] We’ll explore this idea a little in class."
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#hi-im-trevor",
    "href": "archive/f23/notes/week0-intro.html#hi-im-trevor",
    "title": "Welcome to STAT 425!",
    "section": "Hi I’m Trevor",
    "text": "Hi I’m Trevor\nYou can call me Trevor (if you like).\n\nPronouns: he/him/his\nHometown: Galesville, MD\nFun fact: I like to juggle (clubs, balls)\nOffice: Building 25 Room 236 (25-236)\nEmail: truiz01@calpoly.edu\nWeb: tdruiz.com"
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#warm-up",
    "href": "archive/f23/notes/week0-intro.html#warm-up",
    "title": "Welcome to STAT 425!",
    "section": "Warm-up",
    "text": "Warm-up\nIn groups of 3-4:\n\ngo around and say your name, hometown, and current class standing\nwrite down 3 words, concepts, facts, statements, ideas, or phrases you associate with probability (can be one per person or by consensus, your choice)\n\nAfter 5 minutes I’ll go around the room, ask you to share, and transcribe."
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#what-is-probability",
    "href": "archive/f23/notes/week0-intro.html#what-is-probability",
    "title": "Welcome to STAT 425!",
    "section": "What is probability?",
    "text": "What is probability?\nBased on our collective facts, can we…\n\nidentify common themes?\nposit a rough definition or description consistent with our ideas?"
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#interpretations-of-probability",
    "href": "archive/f23/notes/week0-intro.html#interpretations-of-probability",
    "title": "Welcome to STAT 425!",
    "section": "Interpretations of probability",
    "text": "Interpretations of probability\nThe expression \\(Pr(\\text{Millicent will go for a hike}) = 0.8\\) represents the statement ‘there’s an 80% chance Millicent will go for a hike’. But what does that mean?\n\n(subjective) we are 80% certain that Millicent will go for a hike\n(objective) if we keep presenting Millicent with the same opportunity and circumstances, she’ll go for a hike on 8 out of every 10 occasions"
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#interpretations-of-probability-1",
    "href": "archive/f23/notes/week0-intro.html#interpretations-of-probability-1",
    "title": "Welcome to STAT 425!",
    "section": "Interpretations of probability",
    "text": "Interpretations of probability\nThe subjective and objective interpretations of probability have names.\n\nepistemic interpretation: probability statements indicate degrees of belief\naleatory interpretation: probability statements indicate frequencies of occurrence\n\nThe mathematics we’ll study are agnostic on the question of interpretation, though classical examples tend to align more naturally with the aleatory view (games of chance, dice rolls, etc.)."
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#an-informal-definition",
    "href": "archive/f23/notes/week0-intro.html#an-informal-definition",
    "title": "Welcome to STAT 425!",
    "section": "An informal definition",
    "text": "An informal definition\nProbability is the mathematics of random events.\nIn this class we’ll develop a formal language for probability in terms of outcomes and events in a sample space:\n\noutcome — result of an experiment or random process\nevent — statement about outcomes\nsample space — collection of all possible outcomes\n\nUsing that language, we’ll construct familiar concepts of random variables, distributions, and their properties.\nSince you already have some exposure/intuition from STAT 305 (or similar), the challenging part of this class for most of you will be the novel formalism and the level of problem-solving expected."
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#stat-425-at-a-glance",
    "href": "archive/f23/notes/week0-intro.html#stat-425-at-a-glance",
    "title": "Welcome to STAT 425!",
    "section": "STAT 425 at a glance",
    "text": "STAT 425 at a glance\nScope: we’ll cover probability axioms and properties, random variables, univariate and joint distributions and their properties.\nFormat: we’ll develop a set of course notes in class on the whiteboard following outlines that will be posted in advance of each class meeting; these will cover definitions, properties, theorems, and worked examples.\nMaterials: bring note-taking apparatus (pen/paper, tablet, etc.) and an internet-connected device to each class meeting. There is no required textbook, and the recommended text is available in StatLab (25-107B).\nAssessments: weekly homeworks (7); monthly midterms (2); quarterly final exam (1)."
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#learning-outcomes",
    "href": "archive/f23/notes/week0-intro.html#learning-outcomes",
    "title": "Welcome to STAT 425!",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n(…with emphasis on key phrases)\n\nuse the axiomatic construction of probability to derive properties of probability measures and conditional probability measures, and apply definitions and properties to solve probability problems \nconstruct probability models for discrete and continuous random variables, develop familiarity with common probability distributions, and use distribution functions to derive properties such as expectations and variances \ndetermine joint distributions for collections of random variables, and use joint distribution functions to (a) derive properties such as covariance and correlation, (b) to determine conditional and marginal distributions, and (c) derive distributions of transformations and functions of one or more random variables \n\nNote the frequent occurrence of the word use! I want you to understand and be able to use the concepts we discuss in class."
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#assessment-and-evaluation",
    "href": "archive/f23/notes/week0-intro.html#assessment-and-evaluation",
    "title": "Welcome to STAT 425!",
    "section": "Assessment and evaluation",
    "text": "Assessment and evaluation\n\nHomeworks (50%). Assigned/collected Thursdays; collaboration allowed; evaluated based on completeness, organization, and correctness of selected answers; lowest score dropped from final grade calculation.\nMidterms (30%). Given in class Thursdays of weeks 4 and 8; note sheet allowed; evaluated based on completeness and correctness; higher score weighted more heavily in final grade calculation.\nFinal exam (20%). Administered as scheduled by Registrar; note sheet allowed; cumulative but emphasizes later material; evaluated based on completeness and correctness.\n\nWe aim to return graded work within one week, except for the final exam, which I’ll keep.\nLetter grades (to within \\(\\pm 5\\)): A: 90-100. B: 75-90. C: 60-75. D: 50-60."
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#select-policies",
    "href": "archive/f23/notes/week0-intro.html#select-policies",
    "title": "Welcome to STAT 425!",
    "section": "Select policies",
    "text": "Select policies\n\nTime. Expect about 12-16 hours per week, including class time. Let me know if you’re exceeding this amount so I can help.\nCollaboration. Allowed within the class only. Collaborations must reflect a legitimate shared effort, and names of collaborators should be written on submissions.\nAttendance. Everyone is allowed two ‘free’ absences without notice at any time for any reason. Subsequent absences must be excusable and you should notify me by email.\nDeadlines. Everyone is allowed two ‘free’ late homework submissions without notice at any time for any reason. Subsequent late submissions will be evaluated for 75% credit."
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#select-policies-1",
    "href": "archive/f23/notes/week0-intro.html#select-policies-1",
    "title": "Welcome to STAT 425!",
    "section": "Select policies",
    "text": "Select policies\n\nGrades. Please report any discrepancies/errors in evaluation promptly (within 1 week of receiving an evaluated submission). Attempting to negotiate grades or seeking reevaluations after final grades are posted is not appropriate.\nCommunication. I prefer office hours and before/in/after class discussion for most matters. I try to respond to email within 48 weekday hours. Please wait a week before sending reminders, unless it’s time sensitive.\nConduct. Instances of academic dishonesty will be reported to OSRR and consequences may range from loss of credit to automatic failure, depending on the severity of the act."
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#general-advice",
    "href": "archive/f23/notes/week0-intro.html#general-advice",
    "title": "Welcome to STAT 425!",
    "section": "General advice",
    "text": "General advice\nI want each of you to succeed in this course and am here to help. I have a few recommendations:\n\nspend 15-30 minutes skimming the suggested textbook sections each week in the StatLab\ntake advantage of the collaboration policy and work with classmates on homework\nuse your free absences and late submissions wisely\ncome to me early with any obstacles or difficulties you’re facing\ntake advantage of office hours; it’s time each week that I set aside specifically for you"
  },
  {
    "objectID": "archive/f23/notes/week0-intro.html#for-next-time",
    "href": "archive/f23/notes/week0-intro.html#for-next-time",
    "title": "Welcome to STAT 425!",
    "section": "For next time",
    "text": "For next time\n\nComplete intake survey\nCheck the website before class for a lecture outline\n\n\n\n\n\n© 2023 Trevor Ruiz"
  },
  {
    "objectID": "archive/f23/notes/week1-probability.html",
    "href": "archive/f23/notes/week1-probability.html",
    "title": "Probability measures",
    "section": "",
    "text": "Consider an experiment or random process and define the sample space to be:\n\\[\nS: \\text{ set of all possible outcomes}\n\\]\nAn event is a subset \\(E \\subseteq S\\).1 Its complement is defined as the set \\(E^C = S\\setminus E\\). Note that \\(\\left(E^C\\right)^C = E\\). It is immediate from previous results that, given any events \\(A, B\\):\n\\[\n(A\\cup B)^C = A^C \\cap B^C\n\\qquad\\text{and}\\qquad\n(A\\cap B)^C = A^C \\cup B^C\n\\] Recursive application of this property yields DeMorgan’s laws:\n\\[\n\\left[\\bigcup_{i = 1}^n A_i\\right]^C = \\bigcap A_i^c\n\\qquad\\text{and}\\qquad\n\\left[\\bigcap_{i = 1}^n A_i\\right]^C = \\bigcup A_i^c\n\\]\nThe proof is by induction, and we’ll review it in class. The base case is already established. For the inductive step, one need only reapply the base case replacing \\(A\\) by \\(\\bigcup_{i = 1}^n A_i\\) and \\(B\\) by \\(A_{n + 1}\\).\nTwo events \\(E_1, E_2 \\subseteq S\\) are disjoint just in case they share no outcomes, that is, if \\(E_1 \\cap E_2 = \\emptyset\\).\nA collection of events \\(\\{E_i\\}\\) is mutually disjoint just in case every pair is disjoint, that is, if:\n\\[\nE_i \\cap E_j = \\emptyset\n\\quad\\text{for all}\\quad\ni \\neq j\n\\]\nA partition is a mutually disjoint collection whose union contains an event of interest. Usually, one speaks of a partition of the sample space \\(S\\), i.e., a collection \\(\\{E_i\\}\\) such that:\n\n\\(\\{E_i\\}\\) are mutually disjoint events\n\\(\\bigcup_i E_i = S\\)\n\nNote that \\(\\{E, E^C\\}\\) always form a partition of the sample space for any event \\(E\\).\n\n\n\n\n\n\nCheck your understanding\n\n\n\nLet \\(S = [0, 2], E_1 = (0, 1), E_2 = \\{0, 1, 2\\}, E_3 = (0, 2)\\).\n\nAre \\(E_1\\) and \\(E_2\\) disjoint?\nAre \\(E_2\\) and \\(E_3\\) disjoint?\nDoes the collection \\(\\{E_1, E_2, E_3\\}\\) form a partition of \\(S\\)?\nFind the set \\(E_4\\) such that \\(\\{E_1, E_2, E_4\\}\\) partition \\(S\\)."
  },
  {
    "objectID": "archive/f23/notes/week1-probability.html#footnotes",
    "href": "archive/f23/notes/week1-probability.html#footnotes",
    "title": "Probability measures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, not every subset of \\(S\\) is necessarily an event. This won’t matter especially for this course, but it’s worth mentioning here.↩︎\nIn these notes \\(P\\) is explicitly defined as having range \\([0, 1]\\), so this property may seem redundant. However, most sources simply define \\(P\\) as a real-valued function, in which case this property must be derived. The property is included here to underscore that the \\([0, 1]\\) range is in fact a consequence of the axioms, and not an assumption.↩︎"
  },
  {
    "objectID": "archive/f23/notes/week4-gambler.html",
    "href": "archive/f23/notes/week4-gambler.html",
    "title": "The gambler’s ruin problem",
    "section": "",
    "text": "This is a classic probability problem equivalent to a simple random walk on the nonnegative integers. The set-up is that a gambler plays a game where on each play, they win one dollar with probability \\(p\\) and lose one dollar with probability \\(1-p\\). They keep playing until either they go broke or win \\(k\\) dollars. What is the probability that they leave a winner?\nThe sample space in this problem is all possible sequences of plays. This is a big collection, and we won’t be able to resolve the probabilities of events by counting arguments. However, we do know, given that the gambler has \\(j\\) dollars after some play, their possible fortune at the next play and the associated probabilities; this information can be used to resolve the problem.\nDefine \\(X_n\\) to be the gambler’s fortune after \\(n\\) plays, for \\(n = 0, 1, 2, \\dots\\); \\(X_n = j\\) defines the event that the gambler has \\(j\\) dollars at play \\(n\\), for each \\(j\\) and \\(n\\). The problem set-up is that for every \\(n, j\\): \\[\nP(X_{n + 1} = x \\;| X_n = j)\n= \\begin{cases}\n  p, &x = j + 1 \\\\\n  1 - p, &x = j - 1\n\\end{cases}\n\\]\nNow we are interested in the probability that the gambler wins \\(k\\) dollars; denote this event by \\(W_k\\). This will change over the course of play and depend on how much they have at any given time. So consider: \\[\na_j = P(W_k \\;| X_n = j)\n\\qquad\\text{for each } j = 0, \\dots, k\n\\]\nNote that \\(a_0 = 0\\), since the gambler cannot win if they have no money left to play (i.e., \\(W_k \\cap \\{X_n = 0\\} = \\emptyset\\) for every \\(n\\)), and \\(a_k = 1\\), since if the gambler has \\(k\\) dollars they have won (i.e., \\(W_k \\supset \\{X_n = k\\}\\) for every \\(n\\)). Now, the probability of winning depends only on the most recent play; the plays before are irrelevant. Thus, \\(P(W_k \\;| X_{n + 1}, X_n) = P(W_k \\;| X_{n + 1})\\) (This is known as the Markov property.) Using the law of total (conditional) probability (see HW3 for a proof), we have that for each \\(j\\): \\[\n\\begin{align*}\nP(W_k \\;| X_n = j)\n&= P(W_k \\;| X_{n + 1} = j + 1) P(X_{n + 1} = j + 1\\;| X_n = j) \\\\\n&\\qquad + P(W_k \\;| X_{n + 1} = j - 1) P(X_{n + 1} = j - 1\\;| X_n = j)\n\\end{align*}\n\\] And therefore \\(a_j = pa_{j + 1} + (1 - p)a_{j - 1}\\). Some rearrangement yields that: \\[\na_{j + 1} - a_j = (a_j - a_{j - 1})\\left(\\frac{1 - p}{p}\\right)\n\\]\nSince \\(a_0 = 0\\), \\(a_2 - a_1 = a_1\\left(\\frac{1 - p}{p}\\right)\\), and then by recursion we obtain: \\[\na_{j + 1} - a_j = a_1 \\left(\\frac{1 - p}{p}\\right)^j\n\\qquad j = 1, 2, \\dots, k - 1\n\\] Then by writing \\(a_{j + 1} - a_1\\) as a telescoping sum \\(\\sum_{i = 1}^j (a_{i + 1} - a_i)\\), we can express \\(a_{j + 1}\\) as a geometric series and obtian: \\[\na_{j + 1} = \\sum_{i = 0}^j a_1 \\left(\\frac{1 - p}{p}\\right)^i\n=\\begin{cases}\na_1\\left[\\frac{1 - \\left(\\frac{1 - p}{p}\\right)^{j + 1}}{1 - \\left(\\frac{1 - p}{p}\\right)}\\right], &p\\neq \\frac{1}{2} \\\\\na_1 (j + 1), &p = \\frac{1}{2}\n\\end{cases}\n\\] Then, using the fact that \\(a_k = 1\\), we get: \\[\na_1 = \\begin{cases}\n\\frac{1 - \\left(\\frac{1 - p}{p}\\right)}{1 - \\left(\\frac{1 - p}{p}\\right)^{k}}, &p\\neq \\frac{1}{2} \\\\\n\\frac{1}{k}, &p = \\frac{1}{2}\n\\end{cases}\n\\] And finally, by substituting this into the expression above for \\(a_{j + 1}\\), we obtain for each \\(j = 0, \\dots, k\\): \\[\na_j = \\begin{cases}\n\\frac{1 - \\left(\\frac{1 - p}{p}\\right)^j}{1 - \\left(\\frac{1 - p}{p}\\right)^{k}}, &p\\neq \\frac{1}{2} \\\\\n\\frac{j}{k}, &p = \\frac{1}{2}\n\\end{cases}\n\\]\nWe can use this to solve the problem. For instance, if the game is fair (\\(p = \\frac{1}{2}\\)) and the gambler starts with $10, their probability of winning $100 is \\(P(W_{100}\\;|X_n = 10) = \\frac{10}{100} = 0.1\\). If \\(p = \\frac{2}{3}\\), the same probability is: \\[\nP(W_{100}\\;| X_n = 10) = \\frac{1 - \\frac{1}{2^{10}}}{1 - \\frac{1}{2^{100}}} \\approx 0.999\n\\]\nIf the gambler could play forever, what is the probability of getting infinitely rich? Take the limit in \\(k\\) to obtain: \\[\n\\lim_{k \\rightarrow \\infty} P(W_k\\;|X_n = j)\n= \\begin{cases}\n0, &p \\leq \\frac{1}{2}\\\\\n1 - \\left(\\frac{1 - p}{p}\\right)^j, &p &gt; \\frac{1}{2}\n\\end{cases}\n\\]\nIt’s actually somewhat interesting to plot the solution path in \\(j\\) for various \\(p, k\\). For instance, when \\(p = 0.49\\), and the game is only barely unfavorable, the probability of winning $200 is negligible until the gambler acquires most of the money they wish to win.\n\n\n\n\n\n\n\n\n\nFor an extension of the problem, consider finding the minimum \\(j\\) such that \\(P(W_k \\;| X_n = j) \\geq \\frac{1}{2}\\) in terms of \\(p\\), in other words, the smallest amount of money to start with that ensures favorable odds of winning, given \\(p\\). Or, try plotting solution paths in \\(p\\)."
  },
  {
    "objectID": "archive/f23/notes/week8-inequalities.html",
    "href": "archive/f23/notes/week8-inequalities.html",
    "title": "Expectation inequalities",
    "section": "",
    "text": "There are three ‘classical’ inequalities related to expectation of a random variable: the Markov inequality, the Chebyshev inequality, and Jensen’s inequality.\nMarkov inequality. Let \\(X\\) be a random variable. If \\(g(x) \\geq 0\\) on the support of \\(X\\), then for any real number \\(c &gt; 0\\): \\[\nP\\left(g(X) \\geq c\\right) \\leq \\frac{1}{c}\\mathbb{E}\\left[g(X)\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\nLet \\(A = \\{x \\in \\mathbb{R}: g(x) \\geq c\\}\\). Then if \\(X\\) is continuous, by hypothesis \\(g(x)f(x)\\) is nonnegative everywhere, so: \\[\n\\begin{align*}\n\\mathbb{E}\\left[g(X)\\right]\n&= \\int_\\mathbb{R} g(x)f(x)dx \\\\\n&= \\int_A g(x)f(x)dx + \\int_{\\mathbb{R}\\setminus A} g(x)f(x)dx \\\\\n&\\geq \\int_A g(x)f(x)dx \\\\\n&\\geq \\int_A c f(x)dx \\\\\n&= c P(X \\in A) \\\\\n&= c P\\left(g(X) \\geq c\\right)\n\\end{align*}\n\\] If \\(X\\) is discrete, then by hypothesis \\(g(x)P(X = x)\\) is nonnegative everywhere, so: \\[\n\\begin{align*}\n\\mathbb{E}\\left[g(X)\\right]\n&= \\sum_\\mathbb{R} g(x)P(X = x) \\\\\n&= \\sum_A g(x)P(X = x) + \\sum_{\\mathbb{R}\\setminus A} g(x)P(X = x) \\\\\n&\\geq \\sum_A g(x)P(X = x) \\\\\n&\\geq \\sum_A c P(X = x) \\\\\n&= c P\\left(g(X) \\geq c\\right)\n\\end{align*}\n\\]\n\n\nChebyshev inequality. For any random variable \\(X\\) whose first two moments exist, then for any real number \\(c &gt; 0\\): \\[\nP\\left(|X - \\mathbb{E}X| \\geq c\\right) \\leq \\frac{1}{c^2}\\text{var}(X)\n\\]\n\n\n\n\n\n\nProof\n\n\n\nLet \\(\\mu\\) denote the expected value of \\(X\\). Then since \\(g(x) = (x - \\mu)^2\\) is a nonnegative function everywhere, by Markov’s inequality one has: \\[\nP\\left(|X - \\mu| \\geq c \\right) = P\\left[(X - \\mu)^2 \\geq c^2\\right] \\leq \\frac{1}{c^2}\\mathbb{E}(X - \\mu)^2\n\\]\n\n\nThe Chebyshev inequality is sometimes written where \\(c\\) is replaced by a nonnegative multiple of the standard deviation of \\(X\\). If \\(\\sigma^2 = \\text{var}(X)\\), then one has: \\[\nP\\left(|X - \\mu| \\geq k\\sigma\\right) \\leq \\frac{1}{k^2}\n\\]\nThe Chebyshev inequality is important since it provides a means of bounding the probability of deviations from the mean. In particular, note that for any random variable, one has by the inequality: \\[\n\\begin{align*}\n&P\\left(|X - \\mu| \\geq 2\\sigma\\right) \\leq \\frac{1}{4} \\\\\n&P\\left(|X - \\mu| \\geq 3\\sigma\\right) \\leq \\frac{1}{9} \\\\\n&P\\left(|X - \\mu| \\geq 4\\sigma\\right) \\leq \\frac{1}{16} \\\\\n&P\\left(|X - \\mu| \\geq 5\\sigma\\right) \\leq \\frac{1}{25}\n\\end{align*}\n\\]\n\n\n\n\n\n\nExample\n\n\n\nSuppose that the random variable \\(X\\) represents the concentration of arsenic measured in soil samples, and data suggests that for a particular area the average value is 8.7 parts per million and the average deviation is 5.3 ppm. Levels above 20ppm are considered unsafe. Since 20 is 2.3 standard deviations from the mean, the probability that a sample returns a level within that many standard deviations is: \\[\nP\\left(|X - \\mu| &lt; 2.3\\sigma\\right) \\geq 1 - \\frac{1}{2.3^2} = 0.78\n\\] If we are willing to assume that the distribution of arsenic concentrations is symmetric, then the probability of obtaining a sample value above the safety threshold is about 0.11.\n\n\nThe Chebyshev inequality also has an important application in probability theory as providing a proof technique for the weak law of large numbers. If \\(X_n\\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\), then for any positive number \\(\\epsilon &gt; 0\\), \\(P(|X_n - \\mu|\\geq \\epsilon) \\leq \\frac{\\sigma^2}{\\epsilon^2n^2} \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\). Since \\(\\epsilon\\) may be chosen to be arbitrarily small, this tells us that \\(X_n\\) is arbitrarily close to \\(\\mu\\) for large \\(n\\) and with probability tending to one; in the limit, \\(X_\\infty = \\mu\\) with probability 1.\nThe last inequality pertains to convex (or concave) functions. A function \\(g\\) is convex on an open interval \\((a, b)\\) if for every \\(c \\in (0, 1)\\) and \\(a &lt; x &lt; y &lt; b\\): \\[\ng\\left(cx + (1 - c)y\\right) \\leq cg(x) + (1 - c)g(y)\n\\] If \\(g\\) is twice differentiable on \\((a, b)\\) then \\(g\\) is convex just in case either of the following conditions hold:\n\n\\(g'(x) \\leq g'(y)\\) for all \\(a &lt; x &lt; y &lt; b\\)\n\\(g''(x) \\geq 0\\) for all \\(a &lt; x &lt; b\\).\n\nThe function is said to be strictly convex if the above inequalities are strict. A function \\(g\\) is concave on an open interval \\((a, b)\\) just in case \\(-g\\) is convex.\nJensen’s inequality. Let \\(X\\) be a random variable. If \\(g\\) is convex and twice differentiable on the support of \\(X\\) and the expectation \\(\\mathbb{E}\\left[g(X)\\right]\\) exists then: \\[\ng\\left(\\mathbb{E}X\\right) \\leq \\mathbb{E}\\left[g(X)\\right]\n\\] The inequality is strict when \\(g\\) is strictly convex and \\(X\\) is not constant.\n\n\n\n\n\n\nProof\n\n\n\nLet \\(\\mu = \\mathbb{E}X\\). A second-order Taylor expansion of \\(g\\) about \\(\\mu\\) gives: \\[\ng(x) = g(\\mu) + g'(\\mu) (x - \\mu) + \\frac{1}{2} g''(r)(x - \\mu)^2 \\geq g(\\mu) + g'(\\mu) (x - \\mu)\n\\] Taking expectations gives: \\[\n\\mathbb{E}\\left[g(X)\\right] \\geq g(\\mu) + g'(\\mu) (\\mathbb{E}X - \\mu) = g(\\mu) = g\\left(\\mathbb{E}X\\right)\n\\]\n\n\nThe next example applies Jensen’s inequality to show that for positive numbers, the harmonic mean is smaller than the geometric mean and the geometric mean is smaller than the arithmetic mean. It illustrates an interesting and well-known technique of representing the arithmetic average of finitely many positive numbers as the expectation of a discrete uniform random variable.\n\n\n\n\n\n\nOrdering of means\n\n\n\nSuppose \\(A = \\{a_1, a_2, \\dots, a_n\\}\\) is a set of positive numbers \\(a_i\\). There are many types of averages one might consider. The arithmetic mean of the numbers is: \\[\n\\bar{a}_{AM} = \\frac{1}{n}\\sum_{i = 1}^n a_i\n\\] This is what most of us think of when we hear ‘mean’. However, there are other types of means. The geometric mean of the numbers is: \\[\n\\bar{a}_{GM} = \\left(\\prod_{i = 1}^n a_i\\right)^\\frac{1}{n}\n\\] The geometric mean is often used with percentages; in finance, for example, annualized growth over time for an asset is the geometric mean of percentage change in the asset value for each year in the time period.\nThere is also the harmonic mean, which is defined as: \\[\n\\bar{a}_{HM} = \\left(\\frac{1}{n}\\sum_{i = 1}^n \\frac{1}{a_i}\\right)^{-1}\n\\] This average is often used with rates and ratios.\nThe arithmetic mean can be expressed as an expectation. Let \\(X\\) be uniform on the set \\(A\\), so that \\(P(X = a_i) = \\frac{1}{n}\\) for each \\(a_i\\). Then: \\[\n\\mathbb{E}X = \\sum_i a_i P(X = a_i) = \\frac{1}{n} \\sum_{i} a_i = \\bar{a}_{AM}\n\\] Now consider \\(\\log(X)\\). Since the logarithm is a concave function, by Jensen’s inequality one has: \\[\n\\log\\left(\\mathbb{E}X\\right) \\geq \\mathbb{E}\\left[\\log(X)\\right] = \\frac{1}{n}\\sum_{i = 1}^n \\log (a_i) = \\log\\left(\\bar{a}_{GM}\\right)\n\\] Thus \\(\\log(\\bar{a}_{AM}) \\geq \\log(\\bar{a}_{GM})\\), so since \\(\\log\\) is monotone increasing one has that \\(\\bar{a}_{AM}\\geq\\bar{a}_{GM}\\).\nNow consider \\(\\frac{1}{X}\\); since the reciprocal function is convex, by Jensen’s inequality one has: \\[\n\\frac{1}{\\mathbb{E}X} \\leq \\mathbb{E}\\left(\\frac{1}{X}\\right) = \\frac{1}{n}\\sum_{i = 1}^n \\frac{1}{a_i} = \\frac{1}{\\bar{a}_{HM}}\n\\] So one also has that \\(\\bar{a}_{AM}\\geq\\bar{a}_{HM}\\). Moreover, since \\(b_i = \\frac{1}{a_i}\\) are positive numbers, one has by the result just established that \\(\\bar{b}_{AM} \\geq \\bar{b}_{GM}\\). But it is easy to check that \\(\\bar{b}_{AM} = \\bar{a}_{HM}^{-1}\\) and \\(\\bar{b}_{GM} = \\bar{a}_{GM}^-1\\). So one has, all together: \\[\n\\bar{a}_{HM} \\leq \\bar{a}_{GM} \\leq \\bar{a}_{AM}\n\\] This is true for any positive numbers."
  },
  {
    "objectID": "archive/f23/notes/week5-rvs.html",
    "href": "archive/f23/notes/week5-rvs.html",
    "title": "Random variables",
    "section": "",
    "text": "Informally, random variables are real-valued functions on probability spaces. Such functions induce probability measures on \\(\\mathbb{R}\\), which encompass all of the familiar probability distributions. These distributions are, essentially, probability measures on the set of real numbers; to formalize this, we need to be able to articulate probability spaces with \\(\\mathbb{R}\\) as the sample space.\nThe Borel sets comprise the smallest \\(\\sigma\\)-algebra containing all intervals contained in \\(\\mathbb{R}\\). We will denote this collection by \\(\\mathcal{B}\\); while it is possible to generate \\(\\mathcal{B}\\) constructively from the open intervals as a closure under complements, countable unions, and countable intersections, doing so rigorously is beyond the scope of this class. Importantly, however, \\(\\mathcal{B}\\) contains all intervals and all sets that can be formed from intervals; these will comprise our “events” of interest going forward.\n\nConcepts\nLet \\((S, \\mathcal{S}, P)\\) be a probability space. A random variable is a function \\(X: S \\longrightarrow \\mathbb{R}\\) such that for every \\(B \\in \\mathcal{B}\\), \\(X^{-1}(B) \\in \\mathcal{S}\\).\n\n\n\n\n\n\nExample: coin toss\n\n\n\nIf \\(S = \\{H, T\\}\\), and \\(\\mathcal{S} = 2^S = \\{ \\emptyset, \\{H\\}, \\{T\\}, \\{H, T\\}\\}\\), define: \\[\nX(s) = \\begin{cases}\n  1, &s = H \\\\\n  0, &s \\neq H\n\\end{cases}\n\\] Then \\(X\\) is a random variable since for any \\(B \\in \\mathcal{B}\\): \\[\nX^{-1} (B) = \\begin{cases}\n\\{H\\}, &1 \\in B \\\\\n\\{T\\}, &0 \\in B\n\\end{cases}\n\\] So \\(X^{-1}(B) \\in \\mathcal{S}\\).\n\n\nNot all functions are random variables. For example, take \\(S = \\{1, 2, 3\\}\\) and the trivial \\(\\sigma\\)-algebra \\(\\mathcal{S} = \\{\\emptyset, S\\}\\), and consider \\(X(s) = s\\); then \\(X^{-1}(2) = 2 \\not\\in\\mathcal{S}\\).\nThe condition that preimages of Borel sets be events ensures that we can associate probabilities to statements such as \\(a &lt; X(s) &lt; b\\) or, more generally \\(X \\in B\\). Specifically, we can assign such statements probabilities according to the outcomes in the underlying probability space that map to \\(B\\) under \\(X\\). Thus, random variables induce probability measures on \\(\\mathbb{R}\\).\nMore precisely, if \\((S, \\mathcal{S}, P)\\) is a probability space and \\(X:S \\longrightarrow\\mathbb{R}\\) is a random variable, then the induced probability measure (on \\(\\mathbb{R}\\)) is defined for any Borel set \\(B\\in\\mathcal{B}\\) as: \\[\nP_X (B) = P(X^{-1}(B))\n\\]\nThe induced measure \\(P_X\\) is known more commonly as a probability distribution or simply as a distribution: it describes how probabilities are distributed across the set of real numbers.\n\n\n\n\n\n\nExample\n\n\n\nYou’ve already seen some simple random variables. For example, on the probability space representing two dice rolls with all outcomes equally likely, that is, \\(S = \\{1, \\dots, 6\\}^2\\) with \\(\\mathcal{S} = 2^S\\) and \\(P(E) = \\frac{|E|}{36}\\), the function \\(X((i, j)) = i + j\\) is a random variable, because \\(X^{-1}(B) = \\{(i, j): i + j = x \\text{ for some } x \\in B\\} \\in 2^S\\). Moreover, the probability distribution associated with \\(X\\) is:\n\\[\nP_X(B) = \\frac{1}{36}\\sum_{x \\in B} |\\{(i, j) \\in S: i + j = x\\}|\n\\] As determined in a previous homework problem, \\(|\\{(i, j)\\in S: i + j = x\\}| &gt; 0\\) only for \\(x = 2, 3, \\dots, 12\\), and the associated probabilities are:\n\n\n\nx\n\\(P_X(\\{x\\})\\)\n\n\n\n\n2\n\\(\\frac{1}{36}\\)\n\n\n3\n\\(\\frac{2}{36}\\)\n\n\n4\n\\(\\frac{3}{36}\\)\n\n\n5\n\\(\\frac{4}{36}\\)\n\n\n6\n\\(\\frac{5}{36}\\)\n\n\n7\n\\(\\frac{6}{36}\\)\n\n\n8\n\\(\\frac{5}{36}\\)\n\n\n9\n\\(\\frac{4}{36}\\)\n\n\n10\n\\(\\frac{3}{36}\\)\n\n\n11\n\\(\\frac{2}{36}\\)\n\n\n12\n\\(\\frac{1}{36}\\)\n\n\n\n\n\n\n\nCumulative distribution functions\nWe must remember that the concept of a distribution arises relative to some underlying probability space. However, it is not necessary to work with the underlying measure — distributions, luckily, can be characterized using any of several real-valued functions. Arguably, the most fundamental of these is the cumulative distribution function (CDF).\nGiven any random variable \\(X:S \\longrightarrow \\mathbb{R}\\), define the cumulative distribution function (CDF) of \\(X\\) to be the function: \\[\nF_X(x) = P_X\\left((-\\infty, x]\\right) = P(\\{s \\in S: X(s) \\leq x\\})\n\\]\n\n\n\n\n\n\nCheck your understanding\n\n\n\nConsider the dice roll example above with the random variable \\(X((i, j)) = i + j\\). Fill in the table below, and then draw the CDF.\n\n\n\nx\n\\(P_X(\\{x\\})\\)\n\\(P(X \\leq x)\\)\n\n\n\n\n2\n\\(\\frac{1}{36}\\)\n\n\n\n3\n\\(\\frac{2}{36}\\)\n\n\n\n4\n\\(\\frac{3}{36}\\)\n\n\n\n5\n\\(\\frac{4}{36}\\)\n\n\n\n6\n\\(\\frac{5}{36}\\)\n\n\n\n7\n\\(\\frac{6}{36}\\)\n\n\n\n8\n\\(\\frac{5}{36}\\)\n\n\n\n9\n\\(\\frac{4}{36}\\)\n\n\n\n10\n\\(\\frac{3}{36}\\)\n\n\n\n11\n\\(\\frac{2}{36}\\)\n\n\n\n12\n\\(\\frac{1}{36}\\)\n\n\n\n\n\n\nIf the random variable is evident from context, the subscript can be omitted and one can write \\(F\\) instead of \\(F_X\\).\nTheorem. \\(F\\) is the CDF of a random variable if and only if it satisfies the following four properties:\n\n\\(\\lim_{x\\rightarrow -\\infty} F(x) = 0\\)\n\\(\\lim_{x\\rightarrow\\infty} F(x) = 1\\)\n\\(F\\) is monotone nondecreasing: \\(x \\leq y \\Rightarrow F(x) \\leq F(y)\\)\n\\(F\\) is right-continuous: \\(\\lim_{x\\downarrow x_0} F(x) = F(x_0)\\)\n\n\n\n\n\n\n\nProof\n\n\n\nWe will prove the ‘necessity’ part: that if a random variable has CDF \\(X\\), then it satisfies (i)-(iv) above. For sufficiency, one must construct a probability space and random variable \\(X\\) such that \\(X\\) has CDF \\(F\\); we will skip this argument, as it is beyond the scope of this class.\nFor (i), observe that \\(E_n = \\{s \\in S: X(s) \\leq -n\\}\\) is a nonincreasing sequence of sets for \\(n \\in\\mathbb{N}\\) and \\(\\lim_n E_n = \\bigcap_n E_n = \\emptyset\\). Then: \\[\n\\lim_{x \\rightarrow -\\infty} F(x) = \\lim_{n \\rightarrow \\infty} F(-n) = \\lim_{n \\rightarrow\\infty}P(E_n) = P\\left(\\lim_{n\\rightarrow\\infty} E_n\\right) = 0\n\\]\nFor (ii), observe that \\(E_n = \\{s \\in S: X(s) \\leq n\\}\\) is a nondecreasing sequence of sets for \\(n \\in\\mathbb{N}\\) and \\(\\lim_n E_n = \\bigcup_n E_n = S\\). Then: \\[\n\\lim_{x \\rightarrow \\infty} F(x) = \\lim_{n \\rightarrow \\infty} F(n) = \\lim_{n \\rightarrow\\infty}P(E_n) = P\\left(\\lim_{n\\rightarrow\\infty} E_n\\right) = 1\n\\] For (iii), note that if \\(x \\leq y\\) then \\(\\{s \\in S: X(s) \\leq x\\} \\subseteq \\{s \\in S: X(S) \\leq y\\}\\), so by monotonicity of probability \\(F(x) \\leq F(y)\\).\nFor (iv), let \\(\\{x_n\\}\\) be any decreasing sequence with \\(x_n \\rightarrow x_0\\). For instance, \\(x_n = x_0 + \\frac{1}{n}\\). Then the sequence of events \\(E_n = \\{s \\in S: X(s) \\leq x_n\\}\\) is nonincreasing, and \\(\\lim_n E_n = \\bigcap_n E_n = \\{x\\in S: X(s) \\leq x_0\\}\\), so: \\[\n\\begin{align*}\n\\lim_{x \\downarrow x_0} F(x)\n&= \\lim_{n\\rightarrow\\infty} F(x_n) \\\\\n&= \\lim_{n\\rightarrow\\infty} P(E_n) \\\\\n&= P\\left(\\lim_{n \\rightarrow\\infty} E_n\\right) \\\\\n&= P\\left(\\{x\\in S: X(s) \\leq x_0\\}\\right) \\\\\n&= F(x_0)\n\\end{align*}\n\\]\n\n\nThe portion of this theorem we didn’t prove is perhaps the more consequential part of the result, as it establishes that if \\(F\\) is any function satisfying properties (i)–(iv) then there exists a probability space and random variable \\(X\\) such that \\(F_X = F\\). This means that we can omit reference to the underlying probability space \\((S, \\mathcal{S}, P)\\) since some such space exists for any CDF. Thus, we will write probabilities simply as, e.g., \\(P(X \\leq x)\\) in place of \\(P_X((-\\infty, x])\\) or \\(P(\\{s \\in S: X(s) \\leq x\\})\\). Consistent with this change in notation, we will speak directly about probability distributions as distributions “of” (rather than “induced by”) random variables.\nIt’s important to remember that distributions and random variables are distinct concepts: distributions are probability measures (\\(P_X\\) above) and random variables are real-valued functions. Many random variables might have the same distribution, yet be distinct. Since CDFs are one class of functions that uniquely identify distributions, if two random variables \\(X, Y\\) have the same CDF (that is if \\(F_X = F_Y\\)) then they have the same distribution and we write \\(X \\stackrel{d}{=} Y\\)\nTwo convenient properties of CDFs are:\n\nIf \\(X\\) is a random variable with CDF \\(F\\), then \\(P(a &lt; X \\leq b) = F(b) - F(a)\\)\nIf \\(X\\) is a random variable with CDF \\(F\\), then \\(P(X &gt; a) = 1 - F(a)\\)\n\nThe proofs will be left as exercises. This section closes with a technical result that characterizes the probabilities of individual points \\(x\\in\\mathbb{R}\\).\nLemma. Let \\(X\\) be a random variable with CDF \\(F\\). Then \\(P(X = x) = F(x) - F(x^-)\\), where \\(F(x^-)\\) denotes the left-hand limit \\(\\lim_{z\\uparrow x} F(z)\\).\n\n\n\n\n\n\nProof\n\n\n\nDefine \\(E_n = \\{x - \\frac{1}{n} &lt; X \\leq x\\}\\); then \\(P(E_n) = F(x) - F\\left(x - \\frac{1}{n}\\right)\\) and \\(\\{E_n\\}\\) is a nonincreasing sequence with \\(\\lim_n E_n = \\{X = x\\}\\). Then: \\[\n\\begin{align*}\nP(X = x) &= P(\\lim_{n\\rightarrow\\infty} E_n) \\\\\n&= \\lim_{n\\rightarrow\\infty} P(E_n) \\\\\n&= \\lim_{n\\rightarrow\\infty} \\left[F(x) - F\\left(x - \\frac{1}{n}\\right)\\right] \\\\\n&= F(x) - \\lim_{n\\rightarrow\\infty} F\\left(x - \\frac{1}{n}\\right) \\\\\n&= F(x) - \\lim_{z\\uparrow x} F(x) \\\\\n&= F(x) - F(x^-)\n\\end{align*}\n\\]\n\n\nNotice the implication that if \\(F\\) is continuous everywhere, then \\(P(X = x) = 0\\) for every \\(x \\in \\mathbb{R}\\).\n\n\nDiscrete random variables\nIf a CDF takes on countably many values, then the corresponding random variable is said to be discrete. For discrete random variables, \\(P(x = x)\\) is called the probability mass function (PMF) and the probability of any event \\(E\\in\\mathcal{B}\\) is given by the summation: \\[\nP_X(E) = P(X \\in E) = \\sum_{x \\in E} P(X = x)\n\\]\nIt can be shown that discrete random variables take countably many values — i.e., \\(P(X = x) &gt; 0\\) for countably many \\(x\\in\\mathbb{R}\\). We call the set of points where the probability mass function is positive — \\(\\{x \\in \\mathbb{R}: P(X = x) &gt; 0\\}\\) — the support set (or simply the support) of \\(X\\) (or its distribution).\nTheorem. A function \\(f\\) is the PMF of a discrete random variable if and only if:\n\n\\(0 \\leq f(x) \\leq 1\\)\n\\(\\sum_{x \\in \\mathbb{R}} f(x) = 1\\)\n\n\n\n\n\n\n\nProof\n\n\n\nIf \\(f\\) is a PMF of a discrete random variable \\(X\\), then by definition the PMF is \\(f(x) = F(x) - F(x^-)\\). Since \\(F(x) \\leq F(x^-)\\) by monotonicity of \\(F\\), \\(f(x) \\geq 0\\) for every \\(x\\). Since \\(\\lim_{x \\rightarrow \\infty} F(x) = 1\\) and \\(F\\) is monotonic, \\(F(x) \\leq 1\\) for every \\(x\\); by construction \\(f(x) \\leq F(x) \\leq 1\\). So \\(0 \\leq f(x) \\leq 1\\).\nFor the converse implication, if \\(f\\) is a function satisfying (i)–(ii), then \\(f(x) &gt; 0\\) for only countably many values. To see this, consider \\(A_n = \\left\\{x \\in \\mathbb{R}: \\frac{1}{n + 1} \\leq f(x) &lt; \\frac{1}{n}\\right\\}\\) for \\(n = 1, 2, \\dots\\) with \\(A_0 = \\{x \\in \\mathbb{R}: f(x) = 1\\}\\); by construction we must have \\(\\sum_{x \\in \\mathbb{R}} f(x) \\geq \\sum_{x \\in A_n} f(x)\\) for every \\(n\\) since \\(f\\) is nonnegative per (i) and \\(A_n \\subseteq \\mathbb{R}\\). Now \\(f(x) \\geq \\frac{1}{n + 1}\\) on \\(A_n\\) so \\(\\sum_{x \\in A_n} f(x) \\geq \\sum_{x \\in A_n} \\frac{1}{n + 1}\\); but then if \\(A_n\\) is infinite the sum on the right diverges, and thus so does the sum over all \\(x \\in \\mathbb{R}\\) contrary to (ii). So (ii) entails that \\(|A_n| &lt; \\infty\\) for every \\(n\\), and thus the union \\(\\bigcup_{n = 1} A_n = \\{x\\in \\mathbb{R}: f(x) &gt; 0\\}\\) is a countable set.\nLet \\(S\\) denote the set \\(\\{x \\in \\mathbb{R}: f(x) &gt; 0\\}\\). Let \\(x_1, x_2, \\dots\\) denote the elements of \\(S\\) and \\(p_1, p_2, \\dots\\) denote the corresponding values of \\(f\\), that is, \\(p_i = f(x_i)\\). By hypothesis we have that \\(0 \\leq p_i \\leq 1\\) (condition (i)) and \\(\\sum_{i = 1}^\\infty p_i = 1\\) (condition (ii)). Let \\(P(E) = \\sum_{i: x_i \\in E} p_i\\) for \\(E \\in 2^S\\). Then \\(P\\) is a probability measure on \\((S, 2^S)\\) — it suffices to check that \\(P\\) satisfies the probability axioms.\n\nAxiom 1: \\(P(E) = \\sum_{i: x_i \\in E} p_i \\geq 0\\) since by hypothesis \\(p_i \\geq 0\\).\nAxiom 2: \\(P(S) = \\sum_{i: x_i \\in S} p_i = \\sum_{i = 1}^\\infty p_i = 1\\).\nAxiom 3: let \\(\\{E_j\\}\\) be disjoint and define \\(I_j = \\{i: x_i \\in E_j\\}\\). Note that \\(\\left\\{i: x_i \\in \\bigcup_j E_j\\right\\} = \\bigcup_j I_j\\) and \\(I_j \\cap I_k = \\emptyset\\) for \\(j \\neq k\\). Then: \\(P\\left(\\bigcup_j E_j\\right) = \\sum_{\\bigcup_j I_j} p_i = \\sum_j \\sum_{I_j} p_i = \\sum_j P(E_j)\\)\n\nSo \\((S, 2^S, P)\\) is a probability space. Now let \\(X\\) be the identity map \\(X(s) = s\\). \\(X\\) is a random variable, since \\(X^{-1}(B) = \\{x_j\\}_{j\\in J} \\in 2^S\\) for every \\(B \\in \\mathcal{B}\\), and its CDF is given by \\(F(x) = \\sum_{\\{i: x_i \\leq x\\}} p_i\\). This is a step function with countably many values, so \\(X\\) is a discrete random variable. Finally, it is easy to check that:\n\\[\nP(X = x) = F(x) - F(x^-) = \\begin{cases}\n  0 &x \\not\\in S \\\\\n  p_i &x = x_i\n\\end{cases}\n\\]\nSo \\(X\\) has PMF \\(P(X = x_i) = p_i = f(x_i)\\) as required.\n\n\nThis result shows that PMFs uniquely determine discrete distributions. It also establishes that the support set is countable, so the unique values can be enumerated as \\(\\{x_1, x_2, \\dots, x_i, \\dots \\}\\). We can recover the CDF from the PMF as \\(F(x) = \\sum_{x_i \\leq x} P(X = x_i)\\).\n\n\nContinuous random variables\nIf a CDF is absolutely continuous everywhere then the corresponding random variable is said to be continuous. In this case, \\(P(X = x) = 0\\) for every \\(x\\in \\mathbb{R}\\) and so we define instead the probability density function (PDF) to be the function \\(f\\) such that \\[\nF(x) = \\int_{-\\infty}^x f(z)dz\n\\]\nBy the fundamental theorem of calculus, one has that \\(f(x) = \\frac{d}{dx} F(x)\\). The probability of any event \\(E \\in \\mathcal{B}\\) is given by the integral: \\[\nP_X(E) = P(X \\in E) = \\int_E f(x)dx\n\\]\nFor continuous random variables, the support set is defined as the set of points with positive density, that is, \\(\\{x \\in \\mathbb{R}: f(x) &gt; 0\\}\\).\nSimilar to the theorem above for discrete distributions, it can be shown that a function \\(f\\) is the PDF of a continuous random variable if and only if:\n\n\\(f(x) \\geq 0\\)\n\\(\\int_\\mathbb{R} f(x)dx = 1\\)\n\nThe proof of this result is omitted.\n\n\n\n\n\n\nExercise: characterizing distributions\n\n\n\nFor each of the functions below, determine whether it is a CDF. Then, for the CDFs, identify whether a random variable with this distribution is discrete or continuous, and find or guesstimate the PMF/PDF if it exists. You can ignore the behavior at the endpoints, but to check your understanding, identify for each jump which value the function must take at the endpoint for it to be a valid CDF.\n\n\n\n\n\n\n\n\n\n\n\nThese results establish that all distributions (again, recall that technically distributions are probability measures on \\(\\mathbb{R}\\) induced by random variables) can be characterized by CDFs or PDF/PMFs. If a random variable \\(X\\) has the distribution given by the CDF \\(F\\) or the PDF/PMF \\(f\\), we write\n\\[\nX \\sim F(x) \\qquad\\text{or}\\qquad X\\sim f(x)\n\\]\nrespectively."
  },
  {
    "objectID": "archive/f23/notes/week9-transformations.html",
    "href": "archive/f23/notes/week9-transformations.html",
    "title": "Transformations",
    "section": "",
    "text": "Consider an arbitrary transformation of a random vector \\(X\\); we’ll use the generic notation \\(Y = T(X)\\). If \\(T\\) is one-to-one, then it has a well-defined inverse \\(X = T^{-1}(Y)\\). This entails that each vector component of \\(X\\) can be written as a function of the components of \\(Y\\), which makes the problem of finding the distribution of \\(Y\\) tractable using methods similar to those discussed earlier for transformations of random variables.\nTo simplify matters, consider the bivariate setting, so that \\(X = (X_1, X_2)\\). If \\(T\\) is one-to-one, then we can write \\(X\\) as: \\[\n\\begin{align*}\nX_1 &= w_1(Y_1, Y_2) \\\\\nX_2 &= w_2(Y_1, Y_2)\n\\end{align*}\n\\] If \\(S\\) denotes the support set of \\(X\\), let \\(\\mathcal{T}(S)\\) denote the image of \\(S\\) under the transformation \\(T\\).\nWhen \\(X\\) is a discrete random vector, the distribution of \\(Y\\) can be obtained by direct substitution using the joint mass function: \\[\nP(Y_1 = y_1, Y_2 = y_2) = P(X_1 = w_1(y_1, y_2), X_2 = w_2(y_1, y_2))\n\\;,\\quad\n(y_1, y_2) \\in \\mathcal{T}(S)\n\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nWhen \\(X\\) is continuous, finding the distribution of \\(Y = T(X)\\) is a little more complex, but when the inverse transformation is smooth — specifically, when first-order partial derivatives exist — the PDF of \\(Y\\) can be found using a change of variable technique.\nSince \\(T\\) is one-to-one, for any event \\(B \\in \\mathcal{T}(S)\\), \\(\\{Y\\in B\\}\\) and \\(\\{X \\in T^{-1}(B)\\}\\) are equivalent events, so one has that \\(P(Y \\in B) = P(X \\in T^{-1}(B))\\). As a result: \\[\nP(Y \\in B) = \\int\\int_{T^{-1}(B)} f(x_1, x_2) dx_1 dx_2\n\\] Now consider applying a multivariate change of variables to this integral by applying the transformation \\(T\\). Denoting the inverse transformations by \\(w_1 = w_1(y_1, y_2)\\) and \\(w_2 = w_2(y_1, y_2)\\) for short, from calculus one has: \\[\n\\int\\int_{T^{-1}(B)} f(x_1, x_2) dx_1 dx_2\n= \\int\\int_{T(T^{-1}(B))} f(w_1, w_2) |J| dw_1 dw_2\n\\] In the latter expression, \\(J\\) is the determinant of the Jacobian matrix, i.e., the determinant of the matrix of partial derivatives of the inverse transformation: \\[\nJ = \\left| \\begin{array}{cc}\n  \\frac{\\partial w_1}{\\partial y_1} &\\frac{\\partial w_1}{\\partial y_2} \\\\\n  \\frac{\\partial w_2}{\\partial y_1} &\\frac{\\partial w_2}{\\partial y_2} \\\\\n  \\end{array}\\right|\n\\] Thus one has: \\[\nP(Y \\in B) = \\int\\int_B f(w_1, w_2) |J| dw_1 dw_2\n\\] And since densities are unique to distributions, the PDF of \\(Y\\) must be: \\[\nf_Y(y_1, y_2) = f_X \\left(w_1(y_1, y_2), w_2(y_1, y_2)\\right) |J|\n\\] This provides a general formula for obtaining the PDF of a one-to-one transformation of a random vector; the technique extends directly to the multivariate case from the bivariate case, in the sense that: \\[\nf_Y (y_1, \\dots, y_n) = f_X(w_1, \\dots, w_n) |J|\n\\]\n\n\n\n\n\n\nExample: sum and difference\n\n\n\nLet \\(X = (X_1, X_2)\\) be a continuous random vector distributed according to joint PDF \\(f_X(x_1, x_2)\\) and supported on \\(S\\). What is the distribution of \\(X_1 + X_2\\)? What about the difference \\(X_1 - X_2\\)?\nFirst we’ll solve this problem in general, and then consider a specific density function. For the general solution, consider this transformation: \\[\n\\begin{align*}\nY_1 &= X_1 + X_2 \\\\\nY_2 &= X_1 - X_2\n\\end{align*}\n\\] This is one-to-one, and the inverse transformation is: \\[\n\\begin{align*}\nX_1 &= \\frac{1}{2}(Y_1 + Y_2) \\\\\nX_2 &= \\frac{1}{2}(Y_1 - Y_2)\n\\end{align*}\n\\] The support of \\(Y_1\\) will depend on the specific distribution of \\(X\\) under consideration, but \\(-Y_1 \\leq Y_2 \\leq Y_1\\), since the difference cannot exceed the positive or negative sum. We’ll simply write that \\(Y\\) is supported on \\(\\mathcal{T}(S)\\). The Jacobian determinant of this transformation is: \\[\nJ = \\left|\\begin{array}{cc}\n  \\frac{1}{2} &\\frac{1}{2} \\\\\n  \\frac{1}{2} &-\\frac{1}{2}\n  \\end{array}\\right|\n= -\\frac{1}{2}\n\\] So using the change of variables technique, the joint PDF of \\(Y\\) is: \\[\nf_Y (y_1, y_2) = \\frac{1}{2} f_X\\left(\\frac{1}{2}(y_1 + y_2), \\frac{1}{2}(y_1 - y_2)\\right)\n\\;,\\quad (y_1, y_2) \\in \\mathcal{T}(S)\n\\] The marginals are: \\[\n\\begin{align*}\nf_{Y_1}(y_1) &= \\int \\frac{1}{2} f_X\\left(\\frac{1}{2}(y_1 + y_2), \\frac{1}{2}(y_1 - y_2)\\right)dy_2 \\\\\nf_{Y_2}(y_2) &= \\int \\frac{1}{2} f_X\\left(\\frac{1}{2}(y_1 + y_2), \\frac{1}{2}(y_1 - y_2)\\right)dy_1 \\\\\n\\end{align*}\n\\] For a specific example, consider \\(f_X(x_1, x_2) = \\frac{1}{4}\\exp\\left\\{-\\frac{1}{2}(x_1 + x_2)\\right\\}\\) supported on the positive quadrant \\(x_1 &gt; 0, x_2 &gt; 0\\). Using the expression above, the joint distribution is: \\[\nf_Y(y_1, y_2) = \\frac{1}{8}\\exp\\left\\{-\\frac{1}{2} y_1 \\right\\}\n\\;,\\quad\ny_1 &gt; 0, -y_1 &lt; y_2 &lt; y_1\n\\] So the distribution of the sum \\(X_1 + X_2\\) is the marginal distribution of \\(Y_1\\), which is characterized by the density: \\[\nf_{Y_1} (y_1) = \\int_{-y_1}^{y_1} \\frac{1}{8}\\exp\\left\\{-\\frac{1}{2} y_1 \\right\\} = \\frac{1}{4} y_1 \\exp\\left\\{-\\frac{1}{2}y_1\\right\\}\n\\;,\\quad\ny_1 &gt; 0\n\\] A bit of rearrangement of the density reveals that \\(Y_1\\) follows a gamma distribution with parameters \\(\\alpha = 2, \\beta = 2\\).\nThe distribution of the difference \\(X_1 - X_2\\) is the marginal of \\(Y_2\\); we’ll find this distribution in class."
  },
  {
    "objectID": "archive/f23/index.html",
    "href": "archive/f23/index.html",
    "title": "Fall 2023 Materials (archived)",
    "section": "",
    "text": "[Fall 2023 course syllabus]\n\nWeek 0 (9/21/23)\nCourse introduction.\n\nClass meetings\nThursday: course introduction [slides]\n\n\nAssignments\nComplete intake survey\n\n\n\nWeek 1 (9/25/23)\nSets and set operations; probability spaces; properties of probability.\n\nSuggested reading\nDG&S 1.4 — 1.6\n\n\nLecture notes\n[sets] [probability measures]\n\n\nClass meetings\nMonday: sets\nTuesday: sets, cont’d\nWednesday: probability spaces\nThursday: properties of probability measures\n\n\nAssignment\nHomework 1 due Thursday 10/5 [solutions]\n\n\n\nWeek 2 (10/2/23)\nProbability on finite sample spaces; counting rules and applications.\n\nSuggested reading\nDG&S 1.7 — 1.10\n\n\nLecture notes\n[counting]\n\n\nClass meetings\nMonday: wrap-up; probability on finite sample spaces\nTuesday: counting principles and multiplication rules\nWednesday: combinations and permutations\nThursday: combinations and permutations\n\n\nAssignment\nHomework 2 due Thursday 10/12 [solutions]\n\n\n\nWeek 3 (10/9/23)\nConditional probability; independence; Bayes’ theorem.\n\nSuggested reading:\nDG&S 2.1 — 2.3\n\n\nLecture notes\n[conditional probability]\n\n\nClass meetings:\nMonday: conditional probability and basic properties\nTuesday: independence\nWednesday: Bayes’ theorem\nThursday: odds and conditional odds\n\n\nAssignment\n\nHomework 3 due Thursday 10/19 [solutions]\n\n\n\n\nWeek 4 (10/16/23)\nGambler’s ruin; introduction to random variables; midterm review\nMidterm study guide — check back for updates [solutions to practice problems]\n\nSuggested reading:\nDG&S 2.4\n\n\nLecture notes\n[gambler’s ruin]\n\n\nClass meetings:\nMonday: the gambler’s ruin problem\nTuesday: midterm review session\nWednesday: random variables\nThursday: Midterm 1 [solutions]\n\n\nAssignment\n\nNone — enjoy!\n\n\n\n\nWeek 5 (10/23/23)\nDiscrete and continuous random variables; characterizing probability distributions\n\nSuggested reading:\nDG&S 3.1 — 3.3\n\n\nLecture notes\n[random variables]\n\n\nClass meetings:\nMonday: definition of a random variable; the CDF\nTuesday: discrete and continuous random variables\nWednesday: characterizing distributions\nThursday: common distributions\n\n\nAssignment\n\nHomework 4 due Thursday 11/2 [solutions]\n\n\n\n\nWeek 6 (10/30/23)\nCommon families of probability distributions\n\nSuggested reading:\nThis week lectures will cover a blend of material in DG&S 4.1 – 4.4 and 5.1 – 5.6, but some material in those sections is deferred until later sections.\n\n\nLecture notes\n[common distributions]\n\n\nClass meetings:\nMonday: probability distributions based on Bernoulli trials\nTuesday: expected value; common discrete and continuous distributions\nWednesday: the Gaussian/normal distribution\nThursday: moments; the moment generating function\n\n\nAssignment\n\nPlease fill out feedback survey by the end of the week\nHomework 5 due Thursday 11/9 [solutions]\n\n\n\n\nWeek 7 (11/6/23)\nMoments and moment generating functions; the gamma distribution\n\nSuggested reading\nDG&S section 4.4, select material from sections 5.1 – 5.6, and section 5.7\n\n\nLecture notes\n[moments]\n\n\nClass meetings:\nMonday: moments and moment generating functions\nTuesday: the gamma distribution\nWednesday: recap of common distributions and their properties\nThursday: practice problems\n\n\nAssignment\n\nHomework 6 due Thursday 11/16 [solutions]\n\n\n\n\nWeek 8 (11/13/23)\nReview of random variables and common distributions; expectation inequalities\n[midterm 2 study guide]\n[practice test] [solutions]\n\nSuggested reading:\nHogg & Craig section 1.8\n\n\nLecture notes\n[expectation inequalities]\n\n\nClass meetings:\nMonday: review and practice problems\nTuesday: review and practice problems\nWednesday: expectation inequalities\nThursday: Midterm 2 in class [solutions]\n\n\nAssignment\nNone; enjoy!\n\n\n\nWeek 9 (11/27/23)\nMultivariate distributions and transformations\n\nSuggested reading:\nDG&S 3.4 –3.5\n\n\nLecture notes\n[multivariate distributions] [multivariate transformations]\n\n\nClass meetings:\nMonday: random vectors\nTuesday: joint distribution functions\nWednesday: transformations of continuous random vectors\nThursday: the sum of jointly uniform random variables on the unit square\n\n\nAssignment\nMidterm corrections due Thursday 11/30 Homework 7 due Thursday 12/7 [solutions]\n\n\n\nWeek 10 (12/4/23)\nCovariance, correlation, independence, and conditional distributions\n\nLecture notes\n[covariance and correlation]\n[conditional distributions]\n\n\nClass meetings:\nMonday: covariance and correlation\nTuesday: conditional distributions\nWednesday: independence\nThursday: wrapping up\n\n\n\nFinal exam week (12/11/23)\n[final study guide] [final exam solutions]\n\nOffice hours\nTuesday: 1pm-3pm in 25-107G\nWednesday: 1pm-3pm in 25-107G\n\n\nFinal exams\nHeld in usual classroom (186-C201)\nSection 1 Wednesday 12/13/23 4:10pm – 7pm\nSection 2 Friday 12/15/23 4:10pm – 7pm"
  },
  {
    "objectID": "archive/f23/hw/hw4.html",
    "href": "archive/f23/hw/hw4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Please prepare your solutions neatly, numbered, and in order; ideally, you’ll write up a final clean copy after completing all problems on scratch paper. Please note that if a prompt includes a question, you’re expected to support answers with reasoning, even if the prompt does not explicitly ask for a justification. Provide your name at the top of your submission, and if you collaborate with other students in the class, please list their names at the top of your submission beneath your own.\n\n(Random walk) Consider the natural numbers \\(\\mathbb{N} = \\{0, 1, 2, \\dots\\}\\), and imagine a random process whereby given a particular location on the number line you move one step to the right with probability \\(p\\) and one step to the left with probability \\(1 - p\\). If you reach zero, the process stops.\n\nFind the probability that you make it to 50 if you start at 1 and \\(p = 0.6\\).\nFind the probability that you make it to 50 if you start at 1 and \\(p = 0.4\\).\nFind an expression for the smallest starting point required to make it to \\(n\\) with probability \\(q\\) if the odds favor moving to the right.\nFind an expression for the smallest starting point required to make it to \\(n\\) with probability \\(q\\) if the odds favor moving to the left.\nUse your answers in (iii)-(iv) to find the smallest starting point for which the process will make it to 50 with probability \\(0.8\\) when \\(p=0.45\\) and when \\(p = 0.55\\).\nWhat is the minimum starting point for which the process is more likely to diverge than not when \\(p = 0.55\\)?\n\nShow that if \\(X\\) is a discrete random variable, then it has a countable support set.\nUse the definition of the CDF to show that:\n\nIf \\(X\\) is a random variable with CDF \\(F\\), then \\(P(a &lt; X \\leq b) = F(b) - F(a)\\)\nIf \\(X\\) is a random variable with CDF \\(F\\), then \\(P(X &gt; a) = 1 - F(a)\\)\n\nLet \\(X\\) be a continuous random variable. Show that:\n\nfor every \\(x \\in \\mathbb{R}\\), \\(P(X = x) = 0\\) (Hint: use the lemma defining \\(P(X = x)\\) in terms of the CDF)\n\\(P(X \\leq x) = P(X &lt; x)\\) (Hint: use the result in (i))\n\n(Triangular distribution) Let \\(X\\) be a continuous random variable defined by the PDF: \\[\nf(x) = \\begin{cases}\n  0 &x &lt; -1 \\\\\n  x + 1 & -1 \\leq x &lt; 0 \\\\\n  1 - x & 0 \\leq x \\leq 1 \\\\\n  0 & x &gt; 1\n\\end{cases}\n\\] Sketch the density and then find the CDF of \\(X\\).\nLet \\(X\\) have a uniform distribution on the integers \\(\\{-3, -2, -1, 0, 1, 2, 3\\}\\). If \\(Y = X^2\\), find the CDF, PMF, and support of \\(Y\\), and sketch the CDF.\nLet \\(X\\) have a uniform distribution on the interval \\((0, 1)\\) and let \\(Y = -\\log(X)\\). Find the distribution of \\(Y\\).\n(Negative binomial) Consider performing repeated independent trials in which each trial has a fixed probability of success \\(p\\). Let \\(X\\) denote the number of failures before \\(r\\) successes are obtained. Find the PMF of \\(X\\)."
  },
  {
    "objectID": "archive/f23/hw/hw7.html",
    "href": "archive/f23/hw/hw7.html",
    "title": "Homework 7",
    "section": "",
    "text": "Show that if \\(X = (X_1, X_2)\\) is a random vector, then \\(\\mathbb{E}\\left[aX_1 + bX_2 + c\\right] = a\\mathbb{E}X_1 + b\\mathbb{E}X_2 + c\\). Establish the result in both the discrete and continuous cases.\n(Covariance formula) Show that \\(\\mathbb{E}\\left[(X - \\mathbb{E}X)(Y - \\mathbb{E}Y)\\right] = \\mathbb{E}(XY) - \\mathbb{E}X\\mathbb{E}Y\\).\nSuppose \\((X_1, X_2)\\) are uniformly distributed on the unit square, i.e., \\[f(x_1, x_2) = 1\\;,\\qquad 0 &lt; x_1 &lt; 1, 0 &lt; x_2 &lt; 1\\] Find the distribution of \\(Y = X_1 + X_2\\) by finding the CDF of \\(Y\\).\nConsider again the house hunting example from class where \\(X_1\\) denotes the number of bedrooms and \\(X_2\\) denotes the number of bathrooms, and for a randomly selected listing the vector \\((X_1, X_2)\\) has joint distribution:\n\n\n\n\n\n\\(x_1 = 0\\)\n\\(x_1 = 1\\)\n\\(x_1 = 2\\)\n\\(x_1 = 3\\)\n\n\n\n\n\\(x_2 = 1\\)\n0.1\n0.1\n0.2\n0\n\n\n\\(x_2 = 1.5\\)\n0\n0.1\n0.2\n0\n\n\n\\(x_2 = 2\\)\n0\n0\n0\n0.3\n\n\n\\(x_2 = 2.5\\)\n0\n0\n0\n0\n\n\n\nFind the covariance and correlation of \\(X_1\\) and \\(X_2\\).\n\nLet \\((X_1, X_2)\\) be independent exponential random variables with parameter \\(\\beta = 2\\), so that they have joint distribution \\[f(x_1, x_2) = \\frac{1}{4} \\exp\\left\\{-\\frac{1}{2}(x_1 + x_2)\\right\\}\\;,\\qquad x_1 &gt; 0, x_2 &gt; 0\\] Let \\(Y_1, Y_2\\) denote the sum and difference, respectively, of \\(X_1, X_2\\). Find the correlation \\(\\text{corr}(Y_1, Y_2)\\).\nSuppose that you arrive at work within a 2-minute window of your expected arrival time uniformly at random, and your expected arrival time may shift slightly depending on whether there are traffic delays. That is, if \\(X\\) denotes your arrival time where \\(X = 0\\) indicates you are exactly on time, and \\(Y\\) denotes the traffic delay, then assume: \\[\nX|Y \\sim \\text{uniform}(Y - 1, Y + 1)\n\\] If \\(Y \\sim \\text{exponential}(1)\\), find the mean arrival time \\(\\mathbb{E}X\\)."
  },
  {
    "objectID": "archive/f23/hw/hw2.html",
    "href": "archive/f23/hw/hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Please prepare your solutions neatly, numbered, and in order; ideally, you’ll write up a final clean copy after completing all problems on scratch paper. Please note that if a prompt includes a question, you’re expected to support answers with reasoning, even if the prompt does not explicitly ask for a justification. Provide your name at the top of your submission, and if you collaborate with other students in the class, please list their names at the top of your submission beneath your own.\n\n(Hypergeometric distribution) Imagine a clown car with 50 clowns; suppose that 20 of them are happy clowns and 30 of them are sad clowns.\n\nIf 10 clowns exit the car sequentially and at random, what is the probability that exactly 3 are sad clowns?\nIf 10 clowns exit the car sequentially and at random, what is the probability that exactly \\(k\\) are sad clowns? (Assume \\(0\\leq k \\leq 10\\).)\nIf \\(n\\) clowns exit the car sequentially and at random, what is the probability that exactly \\(k\\) are sad clowns? (Assume \\(0 \\leq k \\leq n\\) and \\(n &lt; 20\\).)\nIf the car contains \\(N\\) happy clowns and \\(M\\) sad clowns and \\(n \\leq N + M\\) exit the car sequentially and at random, what is the probability that \\(k\\) are happy clowns (for \\(0 \\leq k \\leq \\text{min}(n, N)\\))?\n\nConsider rolling two six-sided dice. The sample space is \\(S = \\{1, 2, 3, 4, 5, 6\\}^2\\). Assuming the dice are fair, each outcome \\((i, j)\\) has equal probability \\(p\\). Consider the event that the dice sum to \\(k\\): \\(E_k = \\{(i, j)\\in S: i + j = k\\}\\).\n\nFind \\(|S|\\) and \\(p\\).\nFind \\(|E_k|\\) in terms of \\(k\\).\nMake a table of the probabilities \\(P(E_k)\\).\nInterpret the event \\(\\bigcup_{k = 1}^m E_k\\) in words and find \\(P\\left(\\bigcup_{k = 1}^m E_k\\right)\\) (assuming \\(1\\leq m \\leq 12\\)).\nFind the probability of rolling a sum smaller than or equal to 8.\n\n\nHint: you may find the proof of SWR2 helpful in answering part (ii); however, there are multiple ways to solve the problem.\n\nVerify the following identities.\n\n\\({n \\choose k} = \\frac{k + 1}{n - k}{n \\choose k + 1}\\)\n\\({n + m \\choose m} = {n + m \\choose n}\\)\n\\({n \\choose 1} = {n \\choose n - 1} = n\\)\n\\({n \\choose k} = {n \\choose n - k}\\)\n\\({n \\choose k} = \\frac{n}{k}{n - 1 \\choose k - 1}\\)\n\nConsider a lottery where players can choose 12 numbers between 0 and 50 (including 0 and 50) and one winning combination is drawn by randomly selecting one number at a time. Suppose there are three prizes: the biggest prize is awarded to a match of all numbers in the winning combination in sequence; the second biggest prize is awarded to a match of all numbers in the winning combination, but not in sequence; and a smaller cash prize is awarded for matching all but one number in the winning combination, and not necessarily in sequence.\n\nWhat is the probability of winning each prize if player selections (and the winning combination) can include each number no more than once?\nWhat is the probability of winning any of the prizes?\n\n(Matching problem) Suppose that you have \\(n\\) letters addressed to distinct recipients and \\(n\\) envelopes addressed accordingly, and the letters are placed in the envelopes at random and mailed. Let \\(A_i = i\\text{th letter is placed in the correct envelope}\\).\n\nFind the probability that the \\(i\\)th letter is placed in the correct envelope: determine \\(P(A_i)\\).\nFind the probability that the \\(i\\)th and \\(j\\)th letters are placed in the correct envelopes: determine \\(P(A_i \\cap A_j)\\) assuming \\(1 \\leq i &lt; j \\leq n\\).\nFind \\(\\sum_{1 \\leq i &lt; j \\leq n} P(A_i \\cap A_j)\\). (Hint: how many ways are there to choose two letters?)\nFind the probability that the \\(i\\)th, \\(j\\)th, and \\(k\\)th letters are placed in the correct envelopes: determine \\(P(A_i \\cap A_j \\cap A_k)\\) assuming \\(1 \\leq i &lt; j &lt; k \\leq n\\).\nFind \\(\\sum_{1 \\leq i &lt; j &lt; k \\leq n} P(A_i \\cap A_j \\cap A_k)\\). (Hint: how many ways are there to choose three letters?)\nFind the probability that an arbitrary subcollection of \\(i\\) letters (say, letters \\(j_1, \\dots, j_i\\)) are all placed in the right envelopes: determine \\(P(A_{j_1}\\cap A_{j2} \\cap \\cdots \\cap A_{ji})\\) assuming \\(1 \\leq j_1 &lt; \\cdots &lt; j_i \\leq n\\).\nFind \\(\\sum_{1 \\leq j_1 &lt; \\cdots &lt; j_i \\leq 1} P(A_{j_1} \\cap \\cdots \\cap A_{j_i})\\).\nUse the inclusion-exclusion formula to find the probability that at least one letter is mailed to the correct recipient. What is the limit of this probability as \\(n \\rightarrow \\infty\\)?"
  },
  {
    "objectID": "archive/f23/hw/hw1.html",
    "href": "archive/f23/hw/hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Please prepare your solutions neatly, numbered, and in order; ideally, you’ll write up a final clean copy after completing all problems on scratch paper. Please note that if a prompt includes a question, you’re expected to support answers with reasoning, even if the prompt does not explicitly ask for a justification. Provide your name at the top of your submission, and if you collaborate with other students in the class, please list their names at the top of your submission beneath your own.\n\n(Serial systems) In a serial system, components are linked together in such a way that the system only works if every component works. For example, consider a string of Christmas lights; if one light goes out, the whole string goes out. Suppose that one has a serial system with \\(k\\) components that all function independently of one another. The state of the system can be represented by a binary vector \\(x = (x_1, \\dots, x_k)\\) where the coordinate \\(x_i\\) indicates whether the \\(i\\)th component is working. The relevant sample space is the set of all possible values of \\(x\\), that is, \\(S = \\{(x_1, \\dots, x_k): x_i \\in \\{0, 1\\}\\}\\), so that the system states are the outcomes, and the events are all possible subsets \\(\\mathcal{S} = 2^S\\). Let \\(E_i \\in \\mathcal{S}\\) denote the event that the \\(i\\)th component works.\n\nExpress the sample space \\(S\\) as a Cartesian product.\nExpress the event \\(E_i\\) as a set in terms of the system states \\(x \\in S\\).\nList two distinct outcomes included in \\(E_1\\) and two distinct outcomes included in \\(E_2\\).\nIs \\(\\{E_i\\}\\) a disjoint collection? Why or why not?\nFind the number of system states \\(|S|\\) and the number of possible events \\(|\\mathcal{S}|\\).\n\nContinuing the example in the previous problem, express each of the following events in terms of the collection \\(\\{E_i\\}\\).\n\nThe first component works and the second component fails.\nThe first three components work.\nThe system works.\nThe system fails.\nExactly one component fails.\n\nConsider the monotone sequences of sets defined by \\(A_n = [0, 1 + \\frac{1}{n})\\) and \\(B_n = [0, 1 - \\frac{1}{n})\\).\n\nIs \\(\\{A_n\\}\\) increasing or decreasing?\nIs \\(\\{B_n\\}\\) increasing or decreasing?\nTrue or false: \\(\\lim_{n \\rightarrow\\infty} A_n = \\lim_{n\\rightarrow\\infty} B_n\\)? Explain. (Hint: \\(x \\in \\bigcup_n C_n\\) just in case \\(x \\in C_n\\) for at least one \\(n\\); similarly, \\(x \\in \\bigcap_n C_n\\) just in case \\(x \\in C_n\\) for every \\(n\\).)\n\nConsider the “experiment” of rolling 2 six-sided dice, and denote the outcomes by pairs \\((i, j)\\) where \\(i, j \\in \\{1, 2, 3, 4, 5, 6\\}\\).\n\nWrite the sample space \\(S\\) for this experiment, assuming the order of the dice does not matter (i.e., \\((3, 2) = (2, 3)\\)), and find \\(|S|\\).\nIf \\(P(E) = 1\\) whenever \\(E = \\{(1, 1)\\}\\) and \\(P(E) = 0\\) otherwise for \\(E \\in 2^S\\), is \\(P\\) a valid probability measure? Why or why not?\nIf \\(P(E) = 1\\) whenever \\((1, 1) \\in E\\) and \\(P(E) = 0\\) otherwise for \\(E \\in 2^S\\), is \\(P\\) a valid probability measure? Why or why not?\n\n(Uniform distribution) [OPTIONAL] Consider the triple \\((S, \\mathcal{S}, P)\\) where:\n\\[\n\\begin{align*}\nS &= [0, 1] \\\\\n\\mathcal{S} &= \\left\\{A \\subseteq S: A \\text{ is a countable union or intersection of open or closed intervals or their complements}\\right\\} \\\\\nP(E) &= \\int_E dx,\\quad E\\in\\mathcal{S} \\qquad\\text{(i.e., total length of $E$)}\n\\end{align*}\n\\]\n\nShow that \\((S, \\mathcal{S}, P)\\) is a probability space by verifying the requisite conditions on \\(\\mathcal{S}\\) and \\(P\\).\nLet \\(\\mathcal{C}\\) denote the Cantor set. Show that \\(P(\\mathcal{C}) = 0\\).\n\nRemark: the integral \\(\\int_E dx\\) is defined as follows:\n\nfor contiguous intervals, \\(\\int_{(a, b)} dx = \\int_{[a, b]} dx = \\int_{[a, b)} dx = \\int_{(a, b]} dx = \\int_a^b dx\\)\nfor disjoint intervals \\(E_i\\), \\(\\int_{\\bigcup_i E_i} dx = \\sum_i \\int_{E_i} dx\\)\n\nLet \\((S, \\mathcal{S}, P)\\) be a probability space, and let \\(\\{E_i\\}\\) be a collection of events. Show that if \\(\\{E_i\\}\\) is a finite or countable partition of any event \\(A \\subseteq S\\), then \\(\\sum_i P(E_i) = P(A)\\).\n(Bonferroni inequality) Use results from class to show that \\(P\\left(\\bigcap_{i = 1}^n E_i\\right) \\geq 1 - \\sum_{i = 1}^n P\\left(E^C\\right)\\)."
  },
  {
    "objectID": "archive/f23/hw/hw3.html",
    "href": "archive/f23/hw/hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Please prepare your solutions neatly, numbered, and in order; ideally, you’ll write up a final clean copy after completing all problems on scratch paper. Please note that if a prompt includes a question, you’re expected to support answers with reasoning, even if the prompt does not explicitly ask for a justification. Provide your name at the top of your submission, and if you collaborate with other students in the class, please list their names at the top of your submission beneath your own.\n\nLet \\((S, \\mathcal{S}, P)\\) be any probability space. Show that if \\(\\{E_j\\}\\) is a partition of \\(S\\) with \\(P(E_j) &gt; 0\\), then for any events \\(A, B\\): \\[\nP(A\\;| B) = \\sum_j P(A\\;| B \\cap E_j) P(E_j\\;| B)\n\\]\nSickle-cell disease is an inherited condition that causes pain and damage to organs and muscles. It is recessive: people with two copies of the relevant allele have the disease, but people with only one copy are healthy. That is, if \\(A\\) is the sickle-cell allele and \\(a\\) is the neutral allele, people with \\(AA\\) have the disease, and people with \\(Aa\\) or \\(aa\\) do not. Suppose that for some study population the probability that an individual has each combination is as shown below. \\[\n\\begin{align*}\nP(AA) &= 0.02 \\\\\nP(Aa) &= 0.18 \\\\\nP(aa) &= 0.8\n\\end{align*}\n\\] Assume that parents reproduce independently of their having any of these genotypes and inherited alleles are selected independently and with equal probability from each parent.\n\nWhat are the unique combinations of parent genotypes?\nFor each possible combination from (i), find the probability that a child has sickle-cell disease given parent genotypes.\nFind the probability that a child from this population has sickle-cell disease.\n\nSuppose you are analyzing a diagnostic test for a condition that appears in 5% of the population. Let \\(T_+, T_-\\) denote the events that an individual obtaining a positive or negative test result, respectively, and \\(C_+, C_-\\) denote the events that an individual is a positive or negative case. Represent the detection rates for each type of case as follows: \\[\n\\begin{align*}\nP(T_+ \\;| C_+) &= a \\qquad\\text{(true positive rate)}\\\\\nP(T_- \\;| C_+) &= 1 - a \\qquad\\text{(false negative rate)}\\\\\nP(T_+ \\;| C_-) &= 1 - b \\qquad\\text{(false positive rate)}\\\\\nP(T_- \\;| C_-) &= b \\qquad\\text{(true negative rate)}\n\\end{align*}\n\\]\n\nFind the probability that the test correctly diagnoses a randomly chosen individual from the population in terms of \\(a, b\\).\nIf the test achieves an overall accuracy of 90% — so \\(P(\\text{test result is correct}) = \\frac{9}{10}\\) — and the true positive rate is \\(a = \\frac{8}{10}\\), what is the true negative rate \\(b\\)?\nFind \\(P(C_+\\;|T_+)\\) and \\(P(C_- \\;| T_-)\\) in terms of \\(a, b\\).\nIf the true positive rate is \\(a = 0.8\\), what true negative rate \\(b\\) produces a test for which \\(P(C_+\\;| T_+) = 0.9\\)?\nUnder the scenario in (iv), what is \\(P(C_-\\;| T_-)\\)?\nSuppose the test is being redesigned to ensure that \\(P(C_+ \\;|T_+) \\geq \\frac{9}{10}\\). If the best possible true positive rate for the test is \\(a = \\frac{9}{10}\\), what is the maximum false positive rate that achieves the redesign goal?"
  },
  {
    "objectID": "archive/f23/hw/hw6.html",
    "href": "archive/f23/hw/hw6.html",
    "title": "Homework 6",
    "section": "",
    "text": "Please prepare your solutions neatly, numbered, and in order; ideally, you’ll write up a final clean copy after completing all problems on scratch paper. Please note that if a prompt includes a question, you’re expected to support answers with reasoning, even if the prompt does not explicitly ask for a justification. Provide your name at the top of your submission, and if you collaborate with other students in the class, please list their names at the top of your submission beneath your own.\n\n(Normal-\\(\\chi^2\\) relationship) Show that if \\(Z\\sim N(0, 1)\\) then \\(Z^2 \\sim \\chi^2_1\\).\n(Stochastic ordering) Two random variables \\(X\\) and \\(Y\\) are stochastically ordered if either \\(F_X(x) \\leq F_Y(x)\\) or the reverse inequality is true for every \\(x \\in \\mathbb{R}\\). We say that: \\[X \\geq_{st} Y \\quad\\text{if}\\quad F_X(x) \\leq F_Y(x) \\quad\\text{for every}\\quad x\\in\\mathbb{R}\\] Show that the exponential distribution is stochastically ordered in its parameter: that is, if \\(X \\sim \\text{exponential}(\\alpha)\\) and \\(Y \\sim \\text{exponential}(\\alpha + c)\\) where \\(c &gt; 0\\), then \\(X \\geq_{st} Y\\). (Use the ‘rate’ parametrization: \\(f(x) = \\alpha e^{-\\alpha x}, x &gt; 0, \\alpha &gt; 0\\)).\n\n\n\n\nLet \\(X\\) be a random variable with moment generating function \\(m_X (t)\\). Define \\(s_X (t) = \\log\\left(m_X(t)\\right)\\). Show that \\(s'_X(0) = \\mathbb{E}X\\) and \\(s_X''(0) = \\text{var}(X)\\). Then use this approach to find the mean and variance of a random variable \\(X\\) when:\n\n\\(X\\sim N(\\mu, \\sigma^2)\\)\n\\(X \\sim \\Gamma(\\alpha, \\beta)\\)"
  },
  {
    "objectID": "archive/f23/hw/hw5.html",
    "href": "archive/f23/hw/hw5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Please prepare your solutions neatly, numbered, and in order; ideally, you’ll write up a final clean copy after completing all problems on scratch paper. Please note that if a prompt includes a question, you’re expected to support answers with reasoning, even if the prompt does not explicitly ask for a justification. Provide your name at the top of your submission, and if you collaborate with other students in the class, please list their names at the top of your submission beneath your own.\n\nReparametrize the uniform distribution in terms of center and length: propose an alternative PDF for the uniform \\((a, b)\\) distribution with one parameter that indicates the center of the interval and one that indicates the length of the interval. Verify that your proposal is a PDF, and find the expectation and variance. You may use the expectation and variance of the uniform in terms of the usual parametrization in your answer.\n(Poisson approximation to the binomial.) In class it was noted that the Poisson distribution is a limiting case for the binomial with \\(np = \\lambda\\) fixed as \\(n \\rightarrow \\infty\\). This fact can be leveraged to approximate binomial probabilities for large numbers of trials. Let \\(X_{n, p} \\sim \\text{binomial}(n, p)\\) and \\(Y_{n, p} \\sim \\text{Poisson}(np)\\). Then when \\(n\\) is large, \\(P(X_{n, p} = x) \\approx P(Y_{n, p} = x)\\). Fill in the following table:\n\n\n\n\n\n\n\n\n\n\n\n\\(n\\)\n\\(p\\)\n\\(P(X_{n, p} = 100)\\)\n\\(P(Y_{n, p} = 100)\\)\napproximation error\n\n\n\n\n1000\n0.1\n\n\n\n\n\n10000\n0.01\n\n\n\n\n\n100000\n0.001\n\n\n\n\n\n1000000\n0.0001\n\n\n\n\n\n\n\nThe \\(k\\)th factorial moment of a random variable \\(X\\) is \\(\\mathbb{E} \\left( \\frac{X!}{(X - k)!} \\right)\\), assuming the expectation exists.\n\nFind the second factorial moment of the Poisson distribution.\nFind the second factorial moment of the binomial distribution.\nUse your answers above to calculate the variance of each distribution. You do not need to re-compute the mean of each distribution.\n\n(Exponential distribution) Let \\(X \\sim F(x)\\) where \\(F(x) = 1 - e^{-\\alpha x}\\) for \\(x &gt; 0\\) and \\(F(x) = 0\\) for \\(x \\leq 0\\); assume \\(\\alpha &gt; 0\\). Find the mean and variance of \\(X\\).\n(Truncated Poisson) Let \\(X \\sim \\text{Poisson}(\\lambda)\\) and define \\(Y\\) by the probability mass function \\[\nP(Y = y) = \\frac{P(X = y)}{P(X &gt; 0)}\n\\;,\\qquad y = 1, 2, \\dots\n\\] Find an expression for the PMF of \\(Y\\), and calculate its mean and variance.\nWith \\(Y\\) as in the previous problem, show that \\(\\mathbb{E}\\log(Y) \\geq 0\\).\n(Probability integral transform) Let \\(X\\) be a continuous random variable with strictly increasing CDF \\(F\\), and let \\(Y = F(X)\\). Show that \\(Y \\sim \\text{uniform}(0, 1)\\). (If \\(F\\) is strictly increasing, then \\(F^{-1}(x)\\) is well-defined)."
  },
  {
    "objectID": "archive/f23/notes/week1-sets.html",
    "href": "archive/f23/notes/week1-sets.html",
    "title": "Sets",
    "section": "",
    "text": "Concepts\nA set is a collection of mathematical objects such as numbers, points, functions, or more sets.\nIf an object \\(x\\) is in a set \\(A\\), we say that \\(x\\) is an element of \\(A\\) and write \\(x \\in A\\) to mean “\\(x\\) is in \\(A\\)”. Otherwise, we write \\(x \\not\\in A\\).\nWe write a set by identifying its elements. For example:\n\\[\nA =\\{1, 2, 3, 4, 5\\}\n\\]\nTwo sets \\(A, B\\) are identical just in case they have exactly the same elements:\n\\[\nA = B\n\\quad\\Longleftrightarrow\\quad\nx \\in A \\Leftrightarrow x \\in B\n\\]\n\n\nSpecial sets\nThere are some special notations for the sets of numbers:\n\n\\(\\mathbb{R}\\): real numbers\n\\(\\mathbb{N}\\): natural numbers\n\\(\\mathbb{Z}\\): integers\n\\(\\mathbb{C}\\): complex numbers\n\\(\\mathbb{Q}\\): rational numbers\n\nSimilarly, open, closed, and half-open real intervals are denoted:\n\\[\n\\begin{align*}\n[a, b] = \\{x \\in \\mathbb{R}: a \\leq x \\leq b\\} \\\\\n(a, b] = \\{x \\in \\mathbb{R}: a &lt; x \\leq b\\} \\\\\n[a, b) = \\{x \\in \\mathbb{R}: a \\leq x &lt; b\\} \\\\\n(a, b) = \\{x \\in \\mathbb{R}: a &lt; x &lt; b\\}\n\\end{align*}\n\\]\n\n\nContainment\nIf all the elements of a set \\(A\\) are also in \\(B\\), then we say that \\(A\\) is contained in \\(B\\), or that \\(A\\) is a subset of \\(B\\), and write \\(A \\subseteq B\\). (Containment is a binary relation and defines a partial ordering on the set of all sets.)\n\\[\nA \\subseteq B\n\\quad\\Longleftrightarrow\\quad\nx \\in A \\Rightarrow x \\in B\n\\]\nFurther, \\(A\\) is a proper subset of \\(B\\) just in case there is at least one element of \\(B\\) that is not in \\(A\\):\n\\[\nA \\subset B\n\\quad\\Longleftrightarrow\\quad\nA \\subseteq B \\text{ and } A \\neq B\n\\]\nBoth relations are transitive:\n\nIf \\(A \\subseteq B\\) and \\(B \\subseteq C\\) then \\(A \\subseteq C\\)\nIf \\(A \\subset B\\) and \\(B \\subset C\\) then \\(A \\subset C\\)\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\nLet \\(A = [0, 1)\\) and \\(B = (0, 1]\\) and \\(C = (0, 2)\\)\n\nTrue or false, \\(A \\subset C\\)\nTrue or false, \\(B \\subset C\\)\n\nList all proper subset relations among \\(\\mathbb{R}, \\mathbb{Q}, \\mathbb{N}, \\mathbb{Z}, \\mathbb{C}\\).\n\n\n\n\n\nNull set\nLastly it may happen that a set contains no elements. This set is called the null set (or empty set) and is denoted \\(\\emptyset = \\{ \\}\\).\nFor a small brain teaser, prove that every set contains the null set: for any set \\(A\\), \\(\\emptyset \\subseteq A\\). Intuitively, this is true; but why, based on the definitions, must it hold? (Hint: the conditional “if \\(p\\) then \\(q\\)” is trivially true if \\(p\\) is always false.)\n\n\nSet operations\nThe two fundamental set operations are union and intersection. Let \\(A, B\\) be sets and define:\n\n(union) \\(A \\cup B = \\{x: x \\in A \\text{ or } x \\in B\\}\\)\n(intersection) \\(A \\cap B = \\{x: x \\in A \\text{ and } x \\in B\\}\\)\n\nBoth operations are associative:\n\\[\n\\begin{align*}\nA \\cup (B \\cup C) = (A \\cup B) \\cup C \\\\\nA \\cap (B \\cap C) = (A \\cap B) \\cap C\n\\end{align*}\n\\]\nAnd distributive:\n\\[\n\\begin{align*}\nA \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C) \\\\\nA \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\n\\end{align*}\n\\]\nProofs are direct and left as an exercise: apply definitions of union/intersection and show that the conditions specified by each construction are equivalent. The properties follow, essentially, from the meanings of “and” and “or”.\nThe operation of set difference corresponds to removing the elements of one set from another set:\n\\[\nA \\setminus B = \\{x: x \\in A \\text{ and } x \\not\\in B\\}\n\\]\nSet difference is not distributive over unions and intersections, but rather exhibits the following properties:\n\\[\n\\begin{align*}\nA \\setminus (B \\cap C) = (A \\setminus B) \\cup (A \\setminus C) \\\\\nA \\setminus (B \\cup C) = (A \\setminus B) \\cap (A \\setminus C)\n\\end{align*}\n\\]\nWe’ll review the proof in class.\nLastly, the Cartesian product between sets is the set of all possible pairs of elements from each set:\n\\[\nA \\times B = \\{(a, b): a \\in A, b \\in B\\}\n\\]\nThe \\(n\\)-fold product of a set \\(A\\) with itself is written \\(A^n\\):\n\\[\nA^n = \\{(a_1, \\dots, a_n): a_i \\in A, i = 1, \\dots, n\\}\n\\]\nThis is a handy construction, for example, in representing:\n\n\\(n\\)-dimensional real space (\\(\\mathbb{R}^n\\))\nan \\(n\\)-dimensional unit cube (\\([0, 1]^n\\))\n32-bit integers (\\(\\{0, 1\\}^{32}\\))\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\nLet \\(A = [0, 1)\\) and \\(B = (0, 1]\\) and write the following sets as real intervals.\n\n\\(A \\cup B\\)\n\\(A \\cap B\\)\n\\(B\\setminus A\\)\n\\((A \\cup B) \\setminus (A \\cap B)\\)\n\\((A \\cap B) \\setminus (A \\cup B)\\)\n\n\n\n\n\n\nCardinality\nThe cardinality of a set is the number of elements it contains, and is written \\(|A|\\). We say that:\n\n\\(A\\) is finite if \\(|A| &lt; \\infty\\)\n\\(A\\) is countable if \\(|A| = |\\mathbb{N}|\\)\n\\(A\\) is uncountable if \\(|A| &gt; |\\mathbb{N}|\\)\n\nThe power set of \\(A\\) is the set of all subsets of \\(A\\), and is written:\n\\[\n2^A = \\{B: B \\subseteq A\\}\n\\]\nIf \\(A\\) is finite, \\(|2^A| = 2^{|A|}\\); otherwise, \\(2^A\\) is uncountable. We’ll prove the finite case in class.\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\nIf \\(A = \\{0, 6, 12, 44, 190\\}\\), what is \\(|A|\\)?\nIf \\(A = \\{H, T\\}\\), list all the elements of \\(2^A\\).\nIf \\(A = \\{x \\in \\mathbb{N}: x \\leq 100 \\text{ and } x\\%2 = 0\\}\\), find \\(|A|\\) and \\(|2^A|\\)\n\n\n\n\n\nSequences of sets\nConsider a sequence of sets \\(A_1, A_2, \\dots\\). For short, we write the sequence \\(\\{A_i\\}_{i \\in I}\\) or simply \\(\\{A_i\\}\\), where the index set is implicit from context.\nDefine the union/intersection of the first \\(n\\) sets as follows:\n\\[\n\\begin{align*}\n\\bigcup_{i = 1}^n A_i = A_1 \\cup A_2 \\cup \\cdots \\cup A_n = \\{x: x \\in A_i \\text{ for some } i \\leq n\\} \\\\\n\\bigcap_{i = 1}^n A_i = A_1 \\cap A_2 \\cap \\cdots \\cap A_n = \\{x: x \\in A_i \\text{ for every } i \\leq n\\}\n\\end{align*}\n\\]\nSlightly more generally, one might define the union of a subcollection as \\(\\bigcup_{j \\in J} A_j\\) for some collection of indices in the index set \\(J \\subset \\mathbb{N}\\).\nNow, if the sequence is infinite, the union or intersection of all sets in the sequence is defined as:\n\\[\n\\begin{align*}\n\\bigcup_{n = 1}^\\infty A_n = \\{x: x \\in A_i \\text{ for some } i \\in \\mathbb{N} \\} \\\\\n\\bigcap_{n = 1}^\\infty A_n = \\{x: x \\in A_i \\text{ for every } i \\in \\mathbb{N}\\}\n\\end{align*}\n\\]\nThese are referred to as countable unions and countable intersections.\nWe say that a sequence is monotone if sets are sequentially nested, and more specifically, that the sequence is:\n\nnondecreasing if \\(A_i \\subseteq A_{i + 1}\\) for every \\(i \\in \\mathbb{N}\\)\nincreasing if \\(A_i \\subset A_{i + 1}\\) for every \\(i \\in \\mathbb{N}\\)\nnonincreasing if \\(A_i \\supseteq A_{i + 1}\\) for every \\(i \\in \\mathbb{N}\\)\ndecreasing if \\(A_i \\supset A_{i + 1}\\) for every \\(i \\in \\mathbb{N}\\)\n\nNote that increasing sequences are also nondecreasing, and that decreasing sequences are also nonincreasing. We define the limit of a monotone sequence of sets as the countable union or intersection:\n\\[\n\\begin{align*}\n\\lim_{n\\rightarrow\\infty} A_n = \\bigcap_{n = 1}^\\infty A_n \\quad\\text{(nonincreasing)} \\\\\n\\lim_{n\\rightarrow\\infty} A_n = \\bigcup_{n = 1}^\\infty A_n \\quad\\text{(nondecreasing)}\n\\end{align*}\n\\]\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\nIf \\(\\{A_n\\}\\) is a nondecreasing sequence, what is \\(\\bigcap_{n = 1}^\\infty A_n\\)?\nIf \\(\\{A_n\\}\\) is a nonincreasing sequence, what is \\(\\bigcup_{n = 1}^\\infty A_n\\)?\nIf \\(\\{A_n\\}\\) is a decreasing sequence, what is \\(\\bigcap_{i = 1}^n A_i\\)?\nIf \\(\\{A_n\\}\\) is a decreasing sequence, what is \\(\\bigcap_{n = 1}^\\infty A_n\\)?\nIf \\(\\{A_n\\}\\) is an increasing sequence, what is \\(\\bigcup_{i = 1}^n A_i\\)?\n\n\n\n\n\nThe Cantor set\nThe Cantor set is a subset of the unit interval with the counterintuitive distinction of having uncountably many points, but zero length. It is formed by recursively removing middle thirds, first from the unit interval, and then from each subinterval. This process is illustrated by the picture below.\n\n\n\nRecursively removing middle thirds.\n\n\nEach row represents a set consisting of the union of the shaded intervals, so we are considering a sequence of sets \\(\\{C_n\\}\\) where:\n\\[\n\\begin{align*}\nC_0 &= [0, 1] \\\\\nC_1 &= \\left[0, \\frac{1}{3}\\right] \\cup \\left[\\frac{2}{3}, 1\\right] \\\\\nC_2 &= \\left\\{\\left[0, \\frac{1}{9}\\right] \\cup \\left[\\frac{2}{9}, \\frac{1}{3}\\right]\\right\\} \\cup \\left\\{\\left[\\frac{2}{3}, \\frac{7}{9}\\right] \\cup \\left[\\frac{8}{9}, 1\\right]\\right\\} \\\\\n&\\vdots\n\\end{align*}\n\\] One way to write the recursion explicitly at an arbitrary step \\(n\\) is to consider the left endpoints \\(a_i^{(n - 1)}\\) of the intervals from the previous step in the recursion and express the \\(n\\)th step as:\n\\[\nC_n = \\bigcup_{i = 1}^{2^{n - 1}} \\left\\{ 3^{-n} \\left([0, 1]\\cup[2, 3]\\right) + a_i^{(n - 1)} \\right\\}\n\\]\n(In this expression, \\(c\\times[a, b] + d\\) is shorthand for \\([ca + d, cb + d]\\) and \\(\\times, +\\) distribute over unions.)\nNote that the sequence \\(\\{C_n\\}\\) is monotonic and strictly decreasing. The Cantor set is defined as the limit of this sequence:\n\\[\nC^* = \\lim_{n \\rightarrow \\infty} C_n = \\bigcap_{n = 0}^\\infty C_n\n\\]\nThe total length of any of the sets in the sequence is the sum of the lengths of the component intervals. The component intervals all have the same length \\(3^{-n}\\), so:\n\\[\n\\text{length}(C_n) = \\sum_{i = 1}^{2^n} 3^{-n} = \\left(\\frac{2}{3}\\right)^n\n\\longrightarrow 0 \\quad\n\\text{ as } \\quad n \\rightarrow \\infty\n\\]\nAlthough it requires proof that \\(\\text{length}(\\lim_n C_n) = \\lim_n \\text{length}(C_n)\\), this should seem plausible. (We will prove this result a little later.) Thus, then Cantor set has zero total length:\n\\[\n\\text{length}(C^*) = \\text{length}\\left(\\lim_{n \\rightarrow \\infty} C_n\\right) = \\lim_{n \\rightarrow \\infty}\\text{length}(C_n) = 0\n\\]\nFor this reason, and also from the construction of the Cantor set (as the limit of a sequence of countable unions of geometrically shrinking closed intervals) one would expect that \\(C^*\\) is countable. But in fact one can construct a one-to-one correspondence between the points in \\(C^*\\) and the points in the unit interval \\([0, 1]\\). (The trick is to observe that the ternary (base-3) decimal representation of any point in \\(C^*\\) only utilizes two unique digits and can thus be mapped to a binary decimal representation of a point in the unit interval in a way that is obviously bijective.) This establishes that \\(|C^*| = |[0, 1]|\\) — there are exactly the same number of points in the Cantor set as there are in the unit interval — and therefore that the Cantor set is uncountable!"
  },
  {
    "objectID": "archive/f23/notes/week7-expectation.html",
    "href": "archive/f23/notes/week7-expectation.html",
    "title": "Expectation",
    "section": "",
    "text": "Moments\nThe expected values of integer exponents of a random variable are known as moments of the random variable (or its distribution). We’ll write moments as: \\[\n\\mu_k = \\mathbb{E}X^k\n\\]\nWe say that the \\(k\\)th moment exists if \\(\\mathbb{E}\\left[|X|^k\\right] &lt; \\infty\\). It can be shown that if \\(\\mu_k &lt; \\infty\\) then also \\(\\mu_j &lt; \\infty\\) for every \\(j &lt; k\\); that is, if the \\(k\\)th moment exists, then so do all the lower moments.\n\n\n\n\n\n\nProof\n\n\n\nSuppose the \\(k\\)th moment exists, so that \\(\\mathbb{E}\\left[|X|^k\\right] &lt; \\infty\\), and let \\(j \\leq k\\) (assume \\(j, k\\) are integers). The \\(j\\)th moment exists just in case \\(\\mathbb{E}\\left[|X|^j\\right] &lt; \\infty\\). The strategy will be to show that \\(\\mathbb{E}\\left[|X|^j\\right] \\leq \\mathbb{E}\\left[|X|^k\\right]\\)\nIf \\(X\\) is continuous, then: \\[\n\\begin{align*}\n\\mathbb{E}\\left[|X^j|\\right]\n&= \\int_\\mathbb{R} |x|^j f(x) dx \\\\\n&= \\int_{|x| \\leq 1} |x|^j f(x) dx + \\int_{|x| &gt; 1} |x|^j f(x) dx \\\\\n&\\leq \\int_{|x| \\leq 1} f(x)dx + \\int_{|x| &gt; 1} |x|^k f(x) dx \\\\\n&\\leq \\int_\\mathbb{R} f(x)dx + \\int_\\mathbb{R} |x|^k f(x) dx \\\\\n&= 1 + \\mathbb{E}\\left[|x|^k\\right] \\\\\n&\\leq \\mathbb{E}\\left[|x|^k\\right]\n\\end{align*}\n\\]\nThe proof in the discrete case is a parallel argument but with sums in place of integrals, and is left as an exercise.\n\n\nThe centered moments of a random variable (or its distribution) are defined as: \\[\n\\tilde{\\mu}_k = \\mathbb{E}(X - \\mu_1)^k\n\\] Note that the mean of a random variable is its first moment. The variance is its second centered moment.\nThe moment sequence uniquely defines a random variable whenever moments exist for every \\(k \\in \\mathbb{N}\\) and the random variable has bounded support. The moments of a random variable can under many circumstances be obtained from the moment generating function (MGF) rather than direct calculation. The MGF is defined as the expectation \\[\nm_X (t) = \\mathbb{E}e^{tX}\n\\;,\\qquad\nt \\in (-h, h)\n\\] provided it exists for some \\(h&gt;0\\). This is called the moment generating function because: \\[\n\\frac{d^k}{d^k t} m_X (t)\\Big\\rvert_{t = 0}\n= \\mathbb{E}X^k e^{tX}\\Big\\rvert_{t = 0}\n= \\mathbb{E}X^k\n\\]\n\n\n\n\n\n\nExample: Poisson MGF\n\n\n\nIf \\(X \\sim \\text{Poisson}(\\lambda)\\) then for all \\(t\\in \\mathbb{R}\\): \\[\n\\begin{align*}\nm_X (t)\n&= \\mathbb{E}e^{tX} \\\\\n&= \\sum_{x = 0}^\\infty e^{tx} \\frac{\\lambda^x e^{-\\lambda}}{x!} \\\\\n&= e^{-\\lambda(1 - e^t)} \\sum_{x = 0}^\\infty \\underbrace{\\frac{(\\lambda e^t)^x e^{-\\lambda e^t}}{x!}}_{\\text{Poisson}(\\lambda e^t) \\text{ PMF}} \\\\\n&= \\exp\\left\\{-\\lambda(1 - e^t)\\right\\}\n\\end{align*}\n\\]\nThen, to find the first and second moments, differentiate and evaluate at \\(t = 0\\): \\[\n\\begin{align*}\n\\frac{d}{dt}m_X(t)\\Big\\rvert_{t = 0}\n&= \\lambda e^t \\exp\\left\\{-\\lambda(1 - e^t)\\right\\} \\Big\\rvert_{t = 0} = \\lambda \\\\\n\\frac{d^2}{d^2t}m_X(t)\\Big\\rvert_{t = 0}\n&= \\frac{d}{dt} \\left[\\lambda e^t \\exp\\left\\{-\\lambda(1 - e^t)\\right\\} \\right]\\Big\\rvert_{t = 0} \\\\\n&= \\left[\\lambda e^t \\exp\\left\\{-\\lambda(1 - e^t)\\right\\} + (\\lambda e^t)^2 \\exp\\left\\{-\\lambda(1 - e^t)\\right\\} \\right]\\Big\\rvert_{t = 0} \\\\\n&= \\lambda + \\lambda^2\n\\end{align*}\n\\] This matches the previous calculation for \\(\\mu_1, \\mu_2\\).\n\n\nMoment generating functions, when they exist, uniquely characterize probability distributions. If \\(X\\) and \\(Y\\) are two random variables whose moment generating functions exist, then \\(X \\stackrel{d}{=} Y\\) if and only if \\(m_X(t) = m_Y(t)\\) for all \\(t\\) in a neighborhood of zero. The proof is advanced, so we will simply state this as a fact.\nAs a consequence, an MGF can be both a way of describing a distribution and a useful tool, as the examples below illustrate.\n\n\n\n\n\n\nGaussian MGF\n\n\n\nIf \\(Z \\sim N(0, 1)\\) then the MGF of \\(Z\\) is: \\[\n\\begin{align*}\nm_Z (t)\n&= \\mathbb{E}e^{tZ} \\\\\n&= \\int_\\mathbb{R} e^{tz} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}} dz \\\\\n&= \\int_\\mathbb{R} \\frac{1}{\\sqrt{2\\pi}} e^{tz - \\frac{z^2}{2}} dz \\\\\n&= e^{\\frac{1}{2}t^2} \\underbrace{\\int_\\mathbb{R} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} (z - t)^2} dz}_{N(t, 1) \\text{ PDF}} \\\\\n&= e^{\\frac{1}{2}t^2}\n\\end{align*}\n\\] Now if \\(X \\sim N(\\mu, \\sigma^2)\\) then \\(X \\stackrel{d}{=} \\sigma Z + \\mu\\), so \\(X\\) has MGF: \\[\nm_X (t)\n= \\mathbb{E}e^{tX}\n= \\mathbb{E}e^{t(\\sigma Z + \\mu)}\n= \\mathbb{E}e^{(t\\sigma)Z}e^{t\\mu}\n= e^{t\\mu}m_Z(t\\sigma)\n= \\exp\\left\\{t\\mu + \\frac{1}{2}t^2\\sigma^2\\right\\}\n\\] Then the first two moments of each distribution are:\n\\[\n\\begin{align*}\n\\mathbb{E}Z &= m'_Z(0) = t e^{\\frac{1}{2}t^2}\\Big\\rvert_{t = 0} = 0 \\\\\n\\mathbb{E}Z^2 &= m''_Z(0) = e^{\\frac{1}{2}t^2} + t^2 e^{\\frac{1}{2}t^2}\\Big\\rvert_{t = 0} = 0 \\\\\n\\mathbb{E}X &= m'_X(0) = (\\mu + t\\sigma^2)e^{\\mu t + \\frac{1}{2}t^2\\sigma^2}\\Big\\rvert_{t = 0} = \\mu \\\\\n\\mathbb{E}X^2 &= m''_X(0) = \\sigma^2 m_X(t) + (\\mu + t\\sigma^2)^2 m_X(t)\\Big\\rvert_{t = 0} = \\mu^2 + \\sigma^2\n\\end{align*}\n\\] So then by the variance formula, one has: \\[\n\\begin{align*}\n\\text{var}Z &= \\mathbb{E}Z^2 - (\\mathbb{E}Z)^2 = 1 - 0^2 = 1 \\\\\n\\text{var}X &= \\mathbb{E}X^2 - (\\mathbb{E}X)^2 = \\mu^2 + \\sigma^2 - \\mu^2 = \\sigma^2 \\\\\n\\end{align*}\n\\]\n\n\nNotice that in the above example, it is easy to find the MGF of a linear function of a random variable with a known MGF. We can state this as a lemma.\nLemma. If the MGF of \\(X\\) exists and \\(Y = aX + b\\), then the MGF of \\(Y\\) is \\(m_Y (t) = e^{bt}m_X (at)\\).\n\n\n\n\n\n\nProof\n\n\n\n\\(m_Y (t) = \\mathbb{E}e^{tY} = \\mathbb{E}e^{t(aX + b)} = e^{tb}\\mathbb{E} e^{(ta)X} = e^{tb}m_X (at)\\)\n\n\nThe MGF occasionally comes in handy for other transformations, as the next example illustrates.\n\n\n\n\n\n\nLognormal moments\n\n\n\n\\(X\\) has a lognormal distribution if \\(\\log X \\sim N(\\mu, \\sigma^2)\\). The moments of \\(X\\) can actually be found from the MGF of \\(\\log X\\), since: \\[\nm_{\\log X} (t) = \\mathbb{E}e^{t\\log X} = \\mathbb{E}X^t\n\\] Since \\(\\log X\\) is Gaussian, \\(m_{\\log X} (t) = \\exp\\left\\{\\mu t + \\frac{1}{2}t^2\\sigma^2\\right\\}\\), so: \\[\n\\mathbb{E}X^t = \\exp\\left\\{\\mu t + \\frac{1}{2}t^2\\sigma^2\\right\\}\n\\] Interestingly, this expression holds for non-integer values of \\(t\\), since the MGF exists for every \\(t \\in \\mathbb{R}\\).\n\n\nOne of many interesting properties of the standard Gaussian distribution are that all its moments exist, and all odd moments are zero. Moreover, for any Gaussian distribution, the full moment sequence can be calculated explicitly. The next example explores these properties.\n\n\n\n\n\n\nGaussian moment sequence\n\n\n\nIf \\(Z \\sim N(0, 1)\\) so that \\(Z\\) has a standard Gaussian distribution, then \\(m_Z (t) = e^{\\frac{1}{2}t^2}\\).\nThe MGF is infinitely differentiable, and a Taylor expansion about zero gives: \\[\nm_Z (t) = m_Z(0) + m'_Z(0) \\frac{t}{1!} + m''_Z(0)\\frac{t^2}{2!} + \\cdots + m^{(k)}_Z (0) \\frac{t^k}{k!} + \\cdots\n\\] Notice, however, that the series expansion of the exponential function \\(e^x = \\sum_{n = 0}^\\infty \\frac{x^n}{n!}\\) (also a Taylor expansion about zero) gives that: \\[\n\\begin{align*}\ne^{\\frac{1}{2}t^2}\n&= 1 + \\frac{1}{1!}\\left(\\frac{t^2}{2}\\right)^1 + \\frac{1}{2!}\\left(\\frac{t^2}{2}\\right)^2 + \\cdots + \\frac{1}{k!}\\left(\\frac{t^2}{2}\\right)^k + \\cdots \\\\\n&= 1 + \\frac{t^2}{2\\cdot 1!} + \\frac{t^4}{2\\cdot 2!} + \\cdots + \\frac{t^{2k}}{2^k \\cdot k!} + \\cdots \\\\\n&= 1 + \\frac{t^2}{2!}\\cdot \\frac{2!}{2^1 1!} + \\frac{t^4}{4!}\\cdot \\frac{4!}{2^2 2!} + \\cdots + \\frac{t^{2k}}{(2k)!} \\cdot \\frac{(2k)!}{2^k k!} + \\cdots \\\\\n&= 1 + c_2 \\frac{t^2}{2!} + c_4\\frac{t^4}{4!} + \\cdots + c_{2k}\\frac{t^{2k}}{(2k)!} + \\cdots\n\\end{align*}\n\\] Above, \\(c_{2k} = \\frac{(2k)!}{2^k k!}\\) for \\(k = 1, 2, \\dots\\), and \\(c_{2k - 1} = 0\\). By equating the series, which entails that the coefficients match, one has that \\(c_k = m_Z^{(k)} (0)\\), and thus for \\(k = 1, 2, \\dots\\): \\[\n\\begin{align*}\n\\mathbb{E} Z^{2k} &= c_{2k} = \\frac{(2k)!}{2^k k!} \\\\\n\\mathbb{E} Z^{2k - 1} &= c_{2k - 1} = 0\n\\end{align*}\n\\]\nNow if \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(X \\stackrel{d}{=} \\sigma Z + \\mu\\), and by the binomimal theorem, one has: \\[\n\\mathbb{E}X^k\n= \\mathbb{E}\\left[(\\sigma Z + \\mu)^k\\right]\n= \\mathbb{E}\\left[ \\sum_{j = 0}^k {k \\choose j} (\\sigma Z)^j \\mu^{k - j}\\right]\n= \\sum_{j = 0}^k {k \\choose j} \\sigma^j \\mathbb{E}Z^j \\mu^{k - j}\n\\] Thus, the moment sequence for any Gaussian random variable can be obtained by direct calculation by first computing the moments of the standard Gaussian distribution via \\(c_{2k}\\), and then applying the formula above.\n\n\n\n\nThe Gamma distribution\nWe’ll cover one last common family of distributions closely connected with the gamma or factorial function. The gamma function is defined as the integral: \\[\n\\Gamma(\\alpha) = \\int_0^\\infty y^{\\alpha - 1} e^{-y} dy\n\\]\nLemma. Some key properties of the gamma function are:\n\n\\(\\Gamma(1) = 1\\)\n\\(\\Gamma\\left(\\frac{1}{2}\\right) = \\sqrt{pi}\\)\n\\(\\Gamma(\\alpha) = (\\alpha - 1)\\Gamma(\\alpha - 1)\\)\n\n\n\n\n\n\n\nProof\n\n\n\nFor (i), \\(\\Gamma(1) = \\int_0^\\infty e^{-y}dy = 1\\). For (ii), writing the definition and making the substituion \\(z^2 = y\\) yields the Gaussian integral: \\[\n\\Gamma\\left(\\frac{1}{2}\\right)\n= \\int_0^\\infty y^{-\\frac{1}{2}}e^{-y}dy\n= 2\\int_0^\\infty e^{-z^2}dz\n= \\int_{-\\infty}^\\infty e^{-z^2}dz = \\sqrt{pi}\n\\]\nLastly, (iii) is established via integration by parts: \\[\n\\begin{align*}\n\\Gamma(\\alpha)\n&= \\int_0^\\infty \\underbrace{y^{\\alpha - 1}}_{u}\\underbrace{e^{-y}}_{dv}dy \\\\\n&= \\left[-y^{\\alpha - 1} e^{-y}\\right]_0^\\infty + \\int_0^\\infty (\\alpha - 1) y^{\\alpha - 2} e^{-y}dy \\\\\n&= (\\alpha - 1) \\int_0^\\infty y^{(\\alpha - 1) - 1} e^{-y}dy \\\\\n&= (\\alpha - 1) \\Gamma(\\alpha - 1)\n\\end{align*}\n\\]\n\n\nConsider now the kernel \\(x^{\\alpha - 1}e^{-\\frac{x}{\\beta}}\\) for \\(\\alpha &gt; 0, \\beta &gt; 0\\): \\[\n\\int_0^\\infty x^{\\alpha - 1}e^{-\\frac{x}{\\beta}}\n= \\int_0^\\infty (z\\beta)^{\\alpha - 1}e^{-z}\\beta dz\n= \\beta^\\alpha \\Gamma(\\alpha)\n\\]\nNormalizing the kernel to integrate to one yields the gamma density. \\(X\\sim \\Gamma(\\alpha, \\beta)\\) if the PDF of \\(X\\) is: \\[\nf(x) = \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x^{\\alpha - 1}e^{-\\frac{x}{\\beta}}\n\\;,\\qquad\nx &gt; 0, \\alpha &gt; 0, \\beta &gt; 0\n\\]\nSince this is a nonnegative function that integrates to one over the support, it defines a valid probability distribution. The moments can be obtained by direct calculation.\n\n\n\n\n\n\nGamma moments\n\n\n\nThe moments of a gamma random variable can in fact be computed directly. If \\(X \\sim \\Gamma (\\alpha, \\beta)\\), then: \\[\n\\begin{align*}\n\\mathbb{E}X^k\n&= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int_0^\\infty x^k \\cdot x^{\\alpha - 1} e^{-\\frac{x}{\\beta}}dx \\\\\n&= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int_0^\\infty x^{(\\alpha + k) - 1} e^{-\\frac{x}{\\beta}}dx \\\\\n&= \\frac{\\Gamma(\\alpha + k)\\beta^{\\alpha + k}}{\\Gamma(\\alpha)\\beta^\\alpha}\\int_0^\\infty \\frac{1}{\\Gamma(\\alpha + k)\\beta^{\\alpha + k}} x^{(\\alpha + k) - 1} e^{-\\frac{x}{\\beta}}dx \\\\\n&= \\frac{\\Gamma(\\alpha + k)\\beta^{\\alpha + k}}{\\Gamma(\\alpha)\\beta^\\alpha}\n\\end{align*}\n\\] Nonetheless, the moment generating function of \\(X\\) is: \\[\n\\begin{align*}\nm_X (t)\n&= \\mathbb{E}e^{tX} \\\\\n&= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}\\int_0^\\infty e^{tx}x^{\\alpha - 1}e^{-x}dx \\\\\n&= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}\\int_0^\\infty x^{\\alpha - 1}e^{-x\\left(\\frac{1}{\\beta} - t\\right)}dx \\\\\n&= \\frac{1}{\\left(\\frac{1}{\\beta} - t\\right)^\\alpha \\beta^\\alpha}\\int_0^\\infty \\frac{\\left(\\frac{1}{\\beta} - t\\right)^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-x\\left(\\frac{1}{\\beta} - t\\right)}dx \\\\\n&= (1 - \\beta t)^{-\\alpha}\n\\;,\\quad t &lt; \\frac{1}{\\beta}\n\\end{align*}\n\\] The MGF is useful primarily for determining whether, e.g., a transformation has a gamma distribution. We’ll see examples later.\n\n\nSome special cases include:\n\nthe chi square distribution with parameter \\(\\nu &gt; 0\\) is a gamma distribution with parameters \\(\\alpha = \\frac{\\nu}{2}\\) and \\(\\beta = 2\\)\nthe exponential distribution with parameter \\(\\beta\\) is a gamma distribution with parameter \\(\\alpha = 1\\)\n\nThere is a special relationship between the standard Gaussian and the chi-square (and therefore gamma) distributions: if \\(Z\\sim N(0, 1)\\) then \\(Z^2 \\sim \\chi^2_1\\). This is shown by finding the CDF of \\(Z^2\\) and differentiating; the proof is left as an exercise."
  },
  {
    "objectID": "archive/f23/notes/week2-counting.html",
    "href": "archive/f23/notes/week2-counting.html",
    "title": "Counting rules",
    "section": "",
    "text": "Suppose \\(S = \\{s_1, \\dots, s_n\\}\\) and \\(\\mathcal{S} = 2^S\\). Let \\(\\{p_i\\}\\) be \\(n\\) numbers such that \\(0 \\leq p_i \\leq 1\\) and \\(\\sum_{i = 1}^n p_i = 1\\). Then the set function\n\\[\nP(E) = \\sum_{i: x_i \\in E} p_i \\quad,\\; E\\in \\mathcal{S}\n\\]\nis a probability measure on \\((S, \\mathcal{S})\\), i.e., \\((S, \\mathcal{S}, P)\\) is a probability space.\n\n\n\n\n\n\nProof\n\n\n\nSince by construction \\(\\mathcal{S}\\) is a \\(\\sigma\\)-algebra, it remains only to check that \\(P\\) satisfies the probability axioms.\n\nAxiom 1: \\(P(E) = \\sum_{i: x_i \\in E} p_i \\geq 0\\) since by hypothesis \\(p_i \\geq 0\\).\nAxiom 2: \\(P(S) = \\sum_{i: x_i \\in S} p_i = \\sum_{i = 1}^n p_i = 1\\).\nAxiom 3: let \\(\\{E_j\\}\\) be disjoint and define \\(I_j = \\{i: x_i \\in E_j\\}\\). Note that \\(\\left\\{i: x_i \\in \\bigcup_j E_j\\right\\} = \\bigcup_j I_j\\) and \\(I_j \\cap I_k = \\emptyset\\) for \\(j \\neq k\\). Then: \\[\nP\\left(\\bigcup_j E_j\\right) = \\sum_{\\bigcup_j I_j} p_i = \\sum_j \\sum_{I_j} p_i = \\sum_j P(E_j)\n\\]\n\n\n\nSo, probability measures on finite sample spaces are simply assignments of numbers in \\([0, 1]\\) to each outcome that sum to one.\nNow, if one has equally likely outcomes in a finite sample space, i.e., \\(p_i = p_j\\), then: \\[\nP(E) = \\sum_{i: x_i \\in E} p_i = \\sum_{i: x_i \\in E} \\frac{1}{|S|} = \\frac{|E|}{|S|}\n\\]\nThus, for finite probability spaces with equally likely outcomes, computing event probabilities is a matter of counting. While conceptually straightforward, it is often nontrivial to count the elements in a set. For example, how would you count the number of ways to draw a 3-of-a-kind in a 5-card poker hand? In other words, how many distinct combinations of 5 cards contain exactly three of matching rank and no other matches? For that matter, how many 5-card poker hands are possible? Once you know the answer, finding the probability of a 3-of-a-kind (or obtaining one by drawing 5 cards at random, anyway) is easy; counting the outcomes is the tricky part."
  },
  {
    "objectID": "archive/f23/notes/week2-counting.html#footnotes",
    "href": "archive/f23/notes/week2-counting.html#footnotes",
    "title": "Counting rules",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe notation \\([x_1, \\dots, x_n]\\) is an ad-hoc expression for the unordered collection consisting of \\(x_1, \\dots, x_n\\) possibly non-distinct elements. Some special notation is needed here because when the collection includes duplicates, it cannot be written as a set. For instance, \\(\\{0, 0, 1\\} = \\{0, 1\\}\\) but \\([0, 0, 1] \\neq [0, 1]\\).↩︎"
  },
  {
    "objectID": "archive/f23/notes/week10-covariance.html",
    "href": "archive/f23/notes/week10-covariance.html",
    "title": "Covariance and correlation",
    "section": "",
    "text": "Covariance and correlation are measures of dependence between two random variables based on their joint distribution. They quantify the tendency of values of the random variables to vary together, or to “co-vary”. They are signed measures, with the sign indicating whether they tend to vary in opposite directions (negative sign) or the same direction (positive sign).\n\nCovariance\nIf \\(X_1, X_2\\) are random variables then the covariance between them is defined as the expectation: \\[\n\\text{cov}(X_1, X_2) = \\mathbb{E}\\left[(X_1 - \\mathbb{E}X_1)(X_2 - \\mathbb{E}X_2)\\right]\n\\] The expectation is computed from the joint distribution of \\((X_1, X_2)\\), so for instance if the random vector is discrete: \\[\n\\text{cov}(X_1, X_2) = \\sum_{x_1}\\sum_{x_2} (x_1 - \\mathbb{E}X_1)(x_2 - \\mathbb{E}X_2)P(X_1 = x_1, X_2 = x_2)\n\\] And if the random vector is continuous: \\[\n\\text{cov}(X_1, X_2) = \\int\\int (x_1 - \\mathbb{E}X_1)(x_2 - \\mathbb{E}X_2)f(x_1, x_2) dx_1 dx_2\n\\] It is immediate that covariance is a symmetric operator, i.e., \\(\\text{cov}(X_1, X_2) = \\text{cov}(X_2, X_1)\\). Additionally, by expanding the product and applying linearity of expectation one obtains the covariance formula: \\[\n\\text{cov}(X_1, X_2) = \\mathbb{E}(X_1 X_2) - \\mathbb{E}X_1\\mathbb{E}X_2\n\\] This provides a convenient way to calculate covariances, much in the same way that the variance formula simplifies calculation of variances.\nLinearity of expectation also entails that covariance is “bi-linear”, meaning it is linear in each argument: \\[\n\\text{cov}(a X_1 + b, X_2) = a\\text{cov}(X_1, X_2) + \\text{cov}(b, X_2)\n\\] It is easy to show, however, that \\(\\text{cov}(b, X_2) = 0\\): \\[\n\\text{cov}(b, X)\n= \\mathbb{E}\\left[(b - \\mathbb{E}b)(X - \\mathbb{E}X)\\right]\n= \\mathbb{E}[\\underbrace{(b - b)}_{0}(X - \\mathbb{E}X)]\n= 0\n\\] Intuitively, this makes sense, since constants don’t vary at all. Lastly, notice that \\(\\text{cov}(X, X) = \\text{var}(X)\\).\n\n\n\n\n\n\nExercise\n\n\n\nUse bilinearity of covariance to show that:\n\n\\(\\text{var}(c) = 0\\) for any constant \\(c\\)\n\\(\\text{var}(aX + b) = a^2 \\text{var}X\\)\n\n\n\n\n\n\n\n\n\nExample: calculating a covariance\n\n\n\nLet \\((X_1, X_2)\\) be a continuous random vector distributed on the unit square according to the density: \\[\nf(x_1, x_2) = x_1 + x_2\n\\;,\\quad (x_1, x_2) \\in (0, 1)\\times (0, 1)\n\\]\nTo find the covariance, one needs the expectations \\(\\mathbb{E}X_1X_2\\), \\(\\mathbb{E}X_1\\), \\(\\mathbb{E}X_2\\). Marginally, \\(X_1\\) and \\(X_2\\) have the same distribution, so the calculation will be shown only for \\(X_1\\): \\[\n\\begin{align*}\nf_1(x_1) &= \\int_0^1 (x_1 + x_2)dx_2 = x_1 + \\frac{1}{2}\\;,\\quad x_1 \\in (0, 1) \\\\\n\\mathbb{E}X_1 &= \\int_0^1 x_1\\left(x_1 + \\frac{1}{2}\\right)dx_1 = \\frac{7}{12} \\\\\n\\mathbb{E}X_2 &= \\mathbb{E}X_1 = \\frac{7}{12}\n\\end{align*}\n\\] Then: \\[\n\\begin{align*}\n\\mathbb{E}X_1 X_2\n&= \\int_0^1\\int_0^1 x_1 x_2 (x_1 + x_2) dx_1 dx_2 \\\\\n&= \\int_0^1\\int_0^1 (x_1^2 x_2 + x_1 x_2^2) dx_1 dx_2 \\\\\n&= \\int_0^1\\int_0^1 x_1^2 x_2 dx_1 dx_2 + \\int_0^1\\int_0^1 x_1 x_2^2 dx_1 dx_2 \\\\\n&= 2\\int_0^1\\int_0^1 x^2y dx dy \\\\\n&= 2\\int_0^1 \\frac{1}{2}x^2 dx \\\\\n&= \\frac{1}{3}\n\\end{align*}\n\\]\nSo: \\[\n\\text{cov}(X_1, X_2) = \\mathbb{E}X_1X_2 - \\mathbb{E}X_1\\mathbb{E}X_2 = \\frac{1}{3} - \\left(\\frac{7}{12}\\right)^2 = -\\frac{1}{144}\n\\] Check your understanding\n\nWhat is \\(\\text{cov}(-X_1, X_2)\\)?\nWhat is \\(\\text{cov}(X_2, X_1)\\)?\nWhat is \\(\\text{cov}(3X_1 - 2, 5X_2 + 1)\\)?\n\n\n\n\n\nCorrelation\nObserve that shifting a random vector by a constant will not change the covariance, but scaling will. For example, continuing the example immediately above, by bilinearity one has that \\(\\text{cov}(10X_1, 10X_2) = -\\frac{100}{144}\\). While this is a substantially larger number, intuitively, the scale transformation shouldn’t alter the dependence between \\(X_1, X_2\\) — if \\(X_1, X_2\\) are only weakly dependent, then \\(10X_1, 10X_2\\) should remain weakly dependent. Correlation is a standardized covariance measure that is scale-invariant.\nThe correlation between \\(X_1, X_2\\) is the covariance scaled by the variances: \\[\n\\text{corr}(X_1, X_2) = \\frac{\\text{cov}(X_1, X_2)}{\\sqrt{\\text{var}(X_1)\\text{var}(X_2)}}\n\\] This measure is scale invariant since it is a symmetric operator and \\(\\text{var}(a X_1) = a^2\\text{var}(X_1)\\), so: \\[\n\\text{corr}(aX_1, X_2)\n= \\frac{a\\text{cov}(X_1, X_2)}{\\sqrt{a^2\\text{var}(X_1)\\text{var}(X_2)}}\n= \\frac{\\text{cov}(X_1, X_2)}{\\sqrt{\\text{var}(X_1)\\text{var}(X_2)}}\n= \\text{corr}(X_1, X_2)\n\\]\n\n\n\n\n\n\nExample: computing correlation\n\n\n\nContinuing the previous example, the marginal variances are obtained by the following calculation: \\[\n\\begin{align*}\n\\mathbb{E}X_1^2 = \\int_0^1 x_1^2\\left(x_1 + \\frac{1}{2}\\right)dx_1 &= \\frac{5}{12} \\\\\n\\text{var}(X_1) = \\mathbb{E}X_1^2 - \\left(\\mathbb{E}X_2\\right)^2 &= \\frac{11}{144}\n\\end{align*}\n\\]\nThen, the correlation is: \\[\n\\text{corr}(X_1, X_2) = \\frac{-\\frac{1}{144}}{\\sqrt{\\frac{11}{144}}\\sqrt{\\frac{11}{144}}} = -\\frac{1}{11}\n\\]\n\n\nIn addition to being scale-invariant, correlation is easier to interpret since it must be a number between 0 and 1.\nLemma. Let \\(X_1, X_2\\) be random variables with finite second moments. Then \\(-1 \\leq \\text{corr}(X_1, X_2) \\leq 1\\).\n\n\n\n\n\n\nProof\n\n\n\nDenote the correlation by \\(\\rho = \\text{corr}(X_1, X_2)\\), the means by \\(\\mu_1, \\mu_2\\), and the variances by \\(\\sigma_1^2, \\sigma_2^2\\). Note that \\(\\text{cov}(X_1, X_2) = \\sigma_1\\sigma_2\\rho\\).\nThen consider the expression \\(\\left[(X_1 - \\mu_1) + t(X_2 - \\mu_2)\\right]^2\\) as a polynomial in \\(t\\). Since the polynomial is nonnegative everywhere, by expanding the square one obtains: \\[\n0 \\leq \\mathbb{E}\\left\\{\\left[(X_1 - \\mu_1) + t(X_2 - \\mu_2)\\right]^2\\right\\} = (\\sigma_1^2)t^2 + (2\\sigma_1\\sigma_2\\rho) t + \\sigma_1^2\n\\] Thus, the polynomial can have at most one real-valued root (at zero), so the discriminant is negative. Therefore: \\[\n(2\\sigma_1\\sigma_2\\rho)^2 - 4\\sigma_1^2\\sigma_2^2 \\leq 0\n\\quad\\Longleftrightarrow\\quad\n\\rho^2 \\leq 1\n\\]\n\n\nThis result establishes that the largest absolute values of a correlation are \\(-1\\) and \\(1\\); the smallest is \\(0\\). Thus, (absolute) values nearer to 1 indicate stronger dependence, and (absolute) values nearer to zero indicate weaker dependence.\n\n\n\n\n\n\nExercise: contingency table\n\n\n\nConsider the random vector defined by the joint distribution given in the table below:\n\n\n\n\n\\(X_1 = 0\\)\n\\(X_2 = 1\\)\n\n\n\n\n\\(X_2 = 0\\)\n0.1\n0.5\n\n\n\\(X_2 = 1\\)\n0.3\n0.1\n\n\n\nFirst, consider whether you expect outcomes to be dependent, and if so, whether you expect a positive or negative covariance/correlation. Then compute the covariance and correlation.\n\n\nLastly, it is important to note that covariance and correlation do not capture every type of dependence, but rather only linear or approximately linear dependence. We will return to this later, but the classical counterexample is given below.\n\n\n\n\n\n\nPerfectly dependent but uncorrelated\n\n\n\nLet \\(U \\sim \\text{uniform}(-1, 1)\\), and define \\(X = U^2\\). Then \\(\\mathbb{E}U = 0\\), so: $$ (U, X) = (UX) = U^3 = _{-1}^1 u^3 du = 0\n$$ However, obviously \\(X, U\\) are dependent because \\(X\\) is a deterministic function of \\(U\\)."
  },
  {
    "objectID": "archive/f23/notes/week6-distributions.html",
    "href": "archive/f23/notes/week6-distributions.html",
    "title": "Common probability distributions",
    "section": "",
    "text": "Distributions based on Bernoulli trials\nSeveral distributions arise from considering a sequence of independent so-called “Bernoulli trials”: random experiments with a binary outcome. While the coin toss is the prototypical example, a wide range of situations can be described as Bernoulli trials: clinical outcomes, device function, manufacturing defects, and so on. For the purposes of probability calculations, outcomes are encoded as 1, called a ‘success’, and 0, called a ‘failure’.\nBernoulli distribution. A random variable \\(X\\) has a Bernoulli distribution with parameter \\(p\\), written \\(X \\sim \\text{Bernoulli}(p)\\) if it has the PMF: \\[\nP(X = x) = p^x (1 - p)^{1 - x}\n\\;,\\qquad x = 0, 1\n\\;,\\; p \\in (0, 1)\n\\] Note that this PMF simply assigns probability \\(p\\) to the outcome \\(X = 1\\) and probability \\(1 - p\\) to the outcome \\(X = 0\\).\nGeometric distribution. Imagine now a sequence of independent Bernoulli trials with success probability \\(p\\), and define \\(X\\) to be the number of trials before the first success. Then the event \\(X = k\\) is equivalent to observing \\(k\\) failures and 1 success, in just that order. By independence, the associated probability is: \\[\nP(X = k) = (1 - p)^k p\n\\;,\\qquad k = 0, 1, 2, \\dots\n\\] This is a valid PMF. Any random variable with this PMF is said to have a geometric distribution with parameter \\(p\\), written \\(X \\sim \\text{geometric}(p)\\). The CDF can be written in closed form as \\(F(x) = 1 - (1 - p)^{\\lfloor x \\rfloor + 1}\\), where \\(\\lfloor x\\rfloor\\) is the “floor” function: the largest integer smaller than or equal to \\(x\\). For an exercise, verify that the above is a PMF and derive the CDF.\nBinomial distribution. Let \\(X\\) now record the number of successes in \\(n\\) independent Bernoulli trials. The set of all possible outcomes for \\(n\\) trials is \\(\\{0, 1\\}^n\\), but since not all outcomes are equally likely unless \\(p = \\frac{1}{2}\\), the probability of each outcome \\(s \\in \\{0, 1\\}^n\\) in general can be obtained from independence as: \\[\nP(s) = p^{\\sum_{i = 1}^n s_i} (1 - p)^{n - \\sum_{i = 1}^n s_i}\n\\] Then, the probability of \\(k\\) successes is the sum of the probabilities of outcomes with \\(k\\) 1’s: \\[\nP(X = k) = \\sum_{s \\in S: \\sum_i s_i = k} P(s) = \\sum_{s \\in S: \\sum_i s_i = k} p^k (1 - p)^{n - k} = {n \\choose k} p^k (1 - p)^{n - k}\n\\;,\\qquad k = 0, 1, 2, \\dots, n\n\\] There are \\({n \\choose k}\\) terms in the sum since that is the number of ways to allocate the \\(k\\) successes to the \\(n\\) positions. A random variable with this PMF is said to have a binomial distribution with parameters \\(n, p\\), written \\(X \\sim \\text{binomial}(n, p)\\), or simply \\(X \\sim b(n, p)\\). There is no closed form CDF.\n\n\n\n\n\n\nSanity check\n\n\n\nThe binomial PMF is clearly non-negative for any \\(k\\) in the support set \\(\\{0, 1, \\dots, n\\}\\), so it suffices to check whether the PMF sums to one. For this we use the binomial theorem: \\[\n\\sum_{k = 0}^n P(X = k) = \\sum_{k = 0}^n {n \\choose k} p^k (1 - p)^{n - k} = (p + 1 - p)^n = 1\n\\]\n\n\nNegative binomial. If now \\(X\\) records the number of trials until \\(r\\) successes are observed, then, by analogous reasoning in the binomial case, the probability of \\(n\\) trials is the sum over all ways to allocate the successes, except for the last, among the trials, of the probability of \\(r\\) successes and \\(n - r\\) trials: \\[\nP(X = n) = {n - 1 \\choose r - 1}p^r (1 - p)^{n - r}\n\\;,\\qquad\nn = r, r + 1, r + 2, \\dots\n\\]\nThere is an alternate form of the negative binomial that arises from considering the number of failures, akin to the geometric distribution, rather than the number of trials. If \\(Y\\) records the number of failures before \\(r\\) successes, then \\(Y = X - r\\), so with \\(k = n - r\\), the PMF above becomes \\[\nP(Y = k) = P(X - r = k) = P(X = k + r) = {k + r - 1 \\choose k} p^r (1 - p)^k\n\\;,\\qquad\nk = 0, 1, 2, \\dots\n\\] To show that this is a valid PMF, use the second form and write: \\[\n\\begin{align*}\n{k + r - 1 \\choose k}\n&= \\frac{(r + k - 1)(r + k - 2) \\cdots (r + 1)(r)}{k!} \\\\\n&= (-1)^k \\frac{(-r)(-r - 1)\\cdots (-r - k + 2) (-r - k + 1)}{k!} \\\\\n&= (-1)^k {-r \\choose k}\n\\end{align*}\n\\] Technically, \\({-r\\choose k}\\) is a generalized binomial coefficient. While the above should make this notation plausible, we won’t give a full treatment here; however, binomial series with generalized coefficients are convergent under certain conditions, and the limit exhibits the same form as the more familiar binomial theorem. We’ll simply apply the result. Using the limit of the binomial series, one obtains: \\[\n\\sum_{k = 0}^\\infty {k + r - 1\\choose k} (1 - p)^k = \\sum_{k = 0}^\\infty {-r \\choose k} (p - 1)^k = (1 + p - 1)^{-r} = p^{-r}\n\\] From which it follows that: \\[\n\\sum_{k = 0}^\\infty P(Y = k) = p^r \\sum_{k = 0}^\\infty {k + r - 1\\choose k} (1 - p)^k = p^r p^{-r} = 1\n\\]\n\n\n\n\n\n\nExample: blood types\n\n\n\nAbout 36% of people in the US have blood type A positive. Consider a blood drive in which donors participate independently of blood type and are representative of the general population. If \\(X\\) records whether an arbitrary donor is of blood type \\(A^+\\), a reasonable model is \\(X \\sim \\text{Bernoulli}(p = 0.36)\\).\nThe following questions can be answered using distributions based on Bernoulli trials.\n\nWhat is the probability that the first 5 donors are not \\(A^+\\)?\nWhat is the probability that more than 5 donors have blood drawn before an \\(A^+\\) donor has blood drawn?\nWhat is the probability that of the first 20 donors, 10 are \\(A^+\\)?\nWhat is the probability that it takes 30 donors to obtain 10 \\(A^+\\) samples?\n\nThe answers are as follows:\n\nHere, consider \\(X\\) to be the number of donors before the first \\(A^+\\) donor; then \\(X \\sim \\text{geometric}(p = 0.36)\\). So, \\(P(X = 5) = (1 - p)^5 p = 0.64^5 \\cdot 0.36\\approx 0.0387\\).\nLet \\(X\\) remain as in (i). Then \\(P(X &gt; 5) = 1 - P(X \\leq 5) = (1 - p)^{5 + 1} = 0.64^6 \\approx 0.0687\\).\nNow let \\(X\\) record the number of \\(A^+\\) donors out of the first 20. Then \\(X \\sim b(n = 20, p = 0.36)\\), so \\(P(X = 10) = {20\\choose 10} \\cdot 0.36^{10} \\cdot 0.64^{10} \\approx 0.0779\\)\nNow let \\(X\\) record the number of donors until 10 \\(A^+\\) samples are obtained. Then \\(X \\sim nb(r = 10, p = 0.36)\\) where the first parametrization is used. Then \\(P(X = 30) = {30-1 \\choose 10-1} \\cdot 0.36^{10} \\cdot 0.64^{20} \\approx 0.0487\\).\n\n\n\nMultinomial distribution. The multinomial generalizes the binomial to trials with \\(k &gt; 2\\) outcomes. If \\(p_1, \\dots, p_k\\) denote the probabilities of each of \\(k\\) outcomes for a single trial, and \\(X_1, \\dots, X_k\\) count the number of each outcome observed in \\(n\\) independent trials, then: \\[\nP(X_1 = x_1, \\dots, X_k = x_k) = \\frac{n!}{x_1! \\cdots x_{k - 1}!} p_1^{x_1} \\cdots p_{k - 1}^{x_{k - 1}}\n\\;,\\qquad \\sum_i x_i = n\\;,\\; \\sum_i p_i = 1\n\\] Note that only \\(k - 1\\) terms appear in the above expression since \\(X_k = n - \\sum_{i = 1}^{k - 1} X_i\\).\n\n\n\n\n\n\nExample: blood types (cont’d)\n\n\n\nThe percentages of the US population with each of the 8 blood types is given below.\n\n\n\nType\nFrequency\n\n\n\n\n\\(O^+\\)\n34.7%\n\n\n\\(O^-\\)\n6.6%\n\n\n\\(A^+\\)\n35.7%\n\n\n\\(A^-\\)\n6.3%\n\n\n\\(B^+\\)\n8.5%\n\n\n\\(B^-\\)\n1.5%\n\n\n\\(AB^+\\)\n3.4%\n\n\n\\(AB^-\\)\n0.6%\n\n\n\nWhat is the probability of observing 4 \\(O^+\\), 4 \\(A^+\\), 1 \\(B^+\\), and 1 \\(AB^+\\) donors among 10 total donors? This can be found using the multinomial PMF as: \\[\n\\frac{10!}{4!0!4!0!1!0!1!} \\cdot 0.347^4 \\cdot 0.066^0 \\cdot 0.357^4 \\cdot 0.063^0 \\cdot 0.085^1 \\cdot 0.015^0 \\cdot 0.034^1 \\approx 0.00579\n\\]\n\n\nThe above PMFs convey the distribution of probabilities across outcomes, but what are “typical” values that one is likely to observe for these random variables? There are several so-called measures of center, including: the mode or value with largest mass/density; the median or ‘middle’ value with equal mass/density above and below; and the mean or average of values in the support weighted by density/mass.\nThe mean is known, formally, as the expected value or simply the ‘expectation’ of a random variable, and defined to be: \\[\n\\mathbb{E}X = \\begin{cases}\n  \\sum_{x \\in \\mathbb{R}} x P(X = x), &X \\text{ is discrete} \\\\\n  \\int_\\mathbb{R} x f(x) dx, &X \\text{ is continuous}\n\\end{cases}\n\\] The expectation exists when \\(X\\) is absolutely summable/integrable, i.e., when \\(\\sum_{x \\in \\mathbb{R}} |x|P(X = x) &lt; \\infty\\) in the discrete case or \\(\\int_\\mathbb{R} |x| f(x) dx &lt; \\infty\\) in the continuous case.\n\n\n\n\n\n\nExercise: expectations\n\n\n\nShow that:\n\nIf \\(X \\sim \\text{Bernoulli}(p)\\) then \\(\\mathbb{E}X = p\\)\nIf \\(X \\sim \\text{binomial}(n, p)\\) then \\(\\mathbb{E}X = np\\)\nIf \\(X \\sim \\text{geometric}(p)\\) then \\(\\mathbb{E}X = \\frac{1 - p}{p}\\)\n\n\n\nPoisson distribution. Consider a binomial probability \\({n \\choose x}p^x (1 - p)^{n - x}\\). If the expectation \\(np\\) is held constant at \\(\\lambda\\) while \\(n \\rightarrow \\infty\\), the probability tends to: \\[\n\\lim_{n \\rightarrow\\infty} {n \\choose x}p^x (1 - p)^{n - x}\n= \\lim_{n \\rightarrow \\infty} \\frac{n!}{(n - x)!n^x} \\cdot \\frac{\\lambda^x}{x!} \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^n \\cdot \\left(1 - \\frac{\\lambda}{n}\\right)^{-x}\n= \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\] This is a PMF for \\(x = 0, 1, 2, \\dots\\) and \\(\\lambda &gt; 0\\), since it is obviously nonnegative and: \\[\n\\sum_{x = 0}^\\infty \\frac{\\lambda^x e^{-\\lambda}}{x!} = e^{-\\lambda} \\sum_{x = 0}^\\infty \\frac{\\lambda^x}{x!} = e^{-\\lambda}e^\\lambda = 1\n\\] If a random variable \\(X\\) has this PMF, then \\(X \\sim \\text{Poisson}(\\lambda)\\). Think of this distribution as pertaining to the outcome of infinitely many Bernoulli trials with a finite expected total number of successes. For a Poisson random variable, \\(\\mathbb{E}X = \\lambda\\): \\[\n\\mathbb{E}X\n= \\sum_{x = 0}^\\infty x \\frac{\\lambda^x e^{-\\lambda}}{x!}\n= \\sum_{x = 1}^\\infty \\frac{\\lambda^x e^{-\\lambda}}{(x - 1)!}\n= \\lambda\\sum_{x = 1}^\\infty \\frac{\\lambda^{x - 1} e^{-\\lambda}}{(x - 1)!}\n= \\lambda\\sum_{x = 0}^\\infty \\frac{\\lambda^{x} e^{-\\lambda}}{x!}\n= \\lambda\n\\]\n\n\n\n\n\n\nExercise: web traffic\n\n\n\nThe Poisson distribution is often used to model count data. Suppose you’re recording the number of visits to your website each day for a year, and the average number of daily visits is 32.7, so you decide to model the random variable \\(X\\), which records the number of daily visits, as Poisson with parameter \\(\\lambda = 32.7\\).\n\nFind the following probabilities according to your model: \\(P(X = 0), P(X \\leq 20), P(X &gt; 40), P(X &gt; 100)\\).\nIf you assume visits on each day are independent, how many days would you expect to observe no visits in a year? Under 20 visits? Over 40 visits? Over 100 visits?\nIf your year of data shows 30 days with over 100 visits, do you think the Poisson is a good model?\n\nWe’ll work through the solution in class.\n\n\n\n\nBasic continuous distributions\nHere we’ll look at a few elementary continuous distributions.\nUniform distribution. The continuous uniform distribution corresponds to drawing a real number at random from an interval \\((a, b)\\): the density is constant on the specified interval and zero elsewhere. \\(X \\sim \\text{uniform}(a, b)\\) if \\(X\\) has the PDF: \\[\nf(x) = \\frac{1}{b - a}\n\\;,\\qquad x \\in (a, b)\n\\;,\\; -\\infty &lt; a &lt; b &lt; \\infty\n\\] Including one or both endpoints in the interval does not change the probabilities of any events; even though this will produce different densities, the CDF will be the same. The CDF in either case is: \\[\nF(x) = \\int_{-\\infty}^x \\frac{1}{b - a}dz = \\begin{cases}\n0 &,\\; x \\leq a \\\\\n\\frac{x - a}{b - a} &,\\; x \\in (a, b) \\\\\n1 &,\\; x \\geq b\n\\end{cases}\n\\]\nAnother way of looking at the matter of endpoints is that since \\(P(X = x) = 0\\) for every \\(x \\in \\mathbb{R}\\), countably many points may be ‘removed’ without fundamentally alterning the probability distribution. At any rate, for some problems, it makes more sense contextually to specify a uniform distribution on a closed interval, so you may occasionally see \\(X \\sim \\text{uniform}[a, b]\\), though this is less common.\nThe expectation of a uniform random variable is the midpoint of the interval: \\[\n\\int_\\mathbb{R} x f(x) dx = \\frac{1}{b - a}\\int_a^b x dx = \\frac{1}{b-a}\\left[\\frac{x^2}{2}\\right]_a^b = \\frac{b^2 - a^2}{2(b - a)} = \\frac{(b + a)(b - a)}{2(b - a)} = \\frac{b + a}{2}\n\\]\nExponential distribution. The exponential distribution is given by the PDF: \\[\nf(x) = \\alpha e^{-\\alpha x}\n\\;,\\qquad\nx &gt; 0\n\\;,\\;\n\\alpha &gt; 0\n\\]\nThe CDF is \\(F(x) = 1 - e^{-\\alpha x}\\), and the mean is: \\[\n\\begin{align*}\n\\mathbb{E}X\n&= \\int_\\mathbb{R} x f(x) dx \\\\\n&= \\int_0^\\infty x \\alpha e^{-\\alpha x}dx \\\\\n&= \\alpha \\int_0^\\infty \\left(-\\frac{d}{d\\alpha} e^{-\\alpha x}\\right)dx \\\\\n&= \\alpha \\frac{d}{d\\alpha}\\left[\\frac{e^{-\\alpha x}}{\\alpha} \\right]_0^\\infty \\\\\n&= \\alpha \\frac{d}{d\\alpha}\\left(0 - \\frac{1}{\\alpha}\\right) \\\\\n&= \\alpha \\cdot \\frac{1}{\\alpha^2} \\\\\n&= \\frac{1}{\\alpha}\n\\end{align*}\n\\]\nThe exponential distribution is often used to model waiting times and failure times.\n\n\nThe Gaussian distribution\nThe Gaussian or normal distribution is of central importance in statistical inference, and arises in relation to averages. Here we’ll develop the density in the standard case (no parameters) and then introduce the center and scale parameters through a simple linear transformation. We’ll start with an important calculus result.\nTheorem (Gaussian integral). \\(\\int_\\mathbb{R} e^{-z^2}dz = \\sqrt\\pi\\).\n\n\n\n\n\n\nProof\n\n\n\nLet \\(I\\) denote the integral of interest. Then: \\[\nI^2 = \\left(\\int_\\mathbb{R} e^{-z^2}dz\\right)\\left(\\int_\\mathbb{R} e^{-w^2}dw\\right) = \\int_\\mathbb{R}\\int_\\mathbb{R} e^{-(z^2 + w^2)}dzdw\n\\] Now converting to polar coordinates — that is, applying the transformation \\(z = r\\cos\\theta\\) and \\(w = r\\sin\\theta\\) and using the result from calculus that \\(dzdw = rdrd\\theta\\) — one obtains: \\[\nI^2 = \\int_0^{2\\pi}\\int_0^\\infty e^{-r^2}rdrd\\theta = \\int_0^{2\\pi} \\left(\\left[-\\frac{1}{2}e^{-r^2}\\right]^\\infty_0\\right)d\\theta = \\int_0^{2\\pi} \\frac{1}{2}d\\theta = \\pi\n\\] Taking square roots yields \\(I = \\sqrt\\pi\\) as required.\n\n\nGaussian distribution. The standard Gaussian or normal distribution is given by the PDF \\(\\varphi (x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\\) for \\(x \\in \\mathbb{R}\\). There is no closed form for the CDF, but we write the CDF as \\(\\Phi(x) = \\int_{-\\infty}^x \\varphi(z)dz\\). If a random variable \\(Z\\) has this PDF/CDF, write \\(Z \\sim N(0, 1)\\).\nAs a remark, special notation is used for the PDF and CDF of the standard normal/Gaussian because it is used so frequently. \\(\\varphi\\) and \\(\\Phi\\) are typical. \\(\\varphi\\) is a valid PDF since it is evidently non-negative and: \\[\n\\begin{align*}\n\\int_\\mathbb{R} \\varphi(x)dx\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_\\mathbb{R} e^{-\\frac{x^2}{2}}dx \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_\\mathbb{R} e^{-z^2}\\sqrt{2}dz \\qquad \\left(z^2 = \\frac{x^2}{2} \\Rightarrow dz = \\frac{dx}{\\sqrt{2}}\\right) \\\\\n&= \\sqrt{\\frac{2}{2\\pi}} \\int_\\mathbb{R} e^{-z^2}dz \\\\\n&= \\sqrt{\\frac{2}{2\\pi}} \\cdot\\sqrt{\\pi} \\\\\n&= 1\n\\end{align*}\n\\]\nIf \\(Z \\sim N(0, 1)\\) then the expectation of \\(Z\\) is: \\[\n\\begin{align*}\n\\mathbb{E}Z &= \\int_\\mathbb{R} x\\varphi(x)dx \\\\\n&= \\int_{-\\infty}^0 x\\varphi(x)dx + \\int_0^\\infty x\\varphi(x)dx \\\\\n&= -\\int_{-\\infty}^0 -x\\varphi(-x)dx + \\int_0^\\infty x\\varphi(x)dx \\\\\n&= -\\int_0^\\infty x\\varphi(x)dx + \\int_0^\\infty x\\varphi(x)dx \\\\\n&= 0\n\\end{align*}\n\\]\nThis argument leverages the fact, immediate from the functional form of \\(\\varphi\\), that \\(\\varphi(x) = \\varphi(-x)\\), i.e., \\(\\varphi\\) is an even function.\nNow let \\(Z \\sim N(0, 1)\\) and define \\(X = \\sigma Z + \\mu\\) for arbitrary numbers \\(\\sigma &gt; 0\\) and \\(\\mu \\in \\mathbb{R}\\). \\(\\sigma\\) is referred to as a scale parameter and \\(\\mu\\) is referred to as a location parameter. The CDF of \\(X\\) is: \\[\nF_X (x) = P(X \\leq x) = P(\\sigma Z + \\mu \\leq x) = P\\left(Z \\leq \\frac{x - \\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\n\\] It follows that the PDF is: \\[\nf_X (x) = \\frac{d}{dx} \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right) = \\frac{1}{\\sigma}\\varphi\\left(\\frac{x - \\mu}{\\sigma}\\right) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\left(x - \\mu\\right)^2\\right\\}\n\\] We write \\(X \\sim N(\\mu, \\sigma^2)\\) to indicate that \\(X\\) has a Gaussian distribution with parameters \\(\\mu\\) and \\(\\sigma\\). Furthermore, \\(X\\) has expectation: \\[\n\\begin{align*}\n\\mathbb{E}X\n&= \\int_\\mathbb{R} x\\cdot\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\left(x - \\mu\\right)^2\\right\\}dx \\\\\n&= \\int_\\mathbb{R} (\\sigma z + \\mu)\\cdot\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{1}{2}z^2\\right\\}dz \\qquad \\left(z = \\frac{x - \\mu}{\\sigma} \\Rightarrow dz = \\frac{1}{\\sigma}dx\\right) \\\\\n&= \\sigma\\int_\\mathbb{R} z \\cdot\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{1}{2}z^2\\right\\}dz + \\mu\\int_\\mathbb{R} \\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{1}{2}z^2\\right\\}dz \\\\\n&= \\sigma\\mathbb{E}Z + \\mu \\int_\\mathbb{R}\\varphi(z)dz \\\\\n&= \\mu\n\\end{align*}\n\\]\nNotice that \\(\\mathbb{E}X = \\mathbb{E}\\left(\\sigma Z + \\mu\\right) = \\sigma\\mathbb{E}Z + \\mu\\).\n\n\nMore expectations\nTheorem. If \\(X\\) is a random variable and \\(g\\) is a function, then the expectation of \\(Y = g(X)\\), provided it exists, is: \\[\n\\mathbb{E}X = \\begin{cases}\n  \\sum_{x \\in \\mathbb{R}} g(x) P(X = x) &,\\;X \\text{ is discrete}\\\\\n  \\int_\\mathbb{R} g(x)f(x)dx &,\\;X \\text{ is continuous}\\\\\n\\end{cases}\n\\]\nThe proof requires some advanced techniques, so we’ll skip it. Hogg & Craig provide a sketch in the discrete case and otherwise defer to references. Some texts simply state this as a definition.\nCorollaries. If \\(X\\) is a random variable, \\(a, b, c\\) are constants, and \\(g_1, g_2\\) are functions whose expectations exist, then the following statements are true.\n\n\\(\\mathbb{E}\\left(ag_1(X) + bg_2(X) + c\\right) = a\\mathbb{E}g_1(X) + b\\mathbb{E}g_2(X) + c\\).\nIf \\(g_1(x) \\geq 0\\) for all \\(x\\) in the support of \\(X\\), then \\(\\mathbb{E}g_1(X) \\geq 0\\).\nIf \\(g_1(x) \\geq g_2(x)\\) for all \\(x\\) in the support of \\(X\\), then \\(\\mathbb{E}g_1(X) \\geq \\mathbb{E}g_2(X)\\).\nIf \\(a \\leq g_1(x) \\leq b\\) then for all \\(x\\) in the support of \\(X\\), \\(a \\leq \\mathbb{E}g_1(X) \\leq b\\).\n\n\n\n\n\n\n\nProof\n\n\n\nNotice that (ii) and (iv) are special cases of (iii), so it suffices to prove (i) and (iv). Essentially, these properties follow directly from properties of integration/summation.\nFor (i), in the discrete case:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left(ag_1(X) + bg_2(X) + c\\right)\n&= \\sum_x \\left(ag_1(x) + bg_2(x) + c\\right) P(X = x) \\\\\n&= a\\sum_x g_1(x)P(X = x) + b\\sum_x g_2(x)P(X = x) + c\\sum_x P(X = x) \\\\\n&= a\\mathbb{E}g_1(X) + b\\mathbb{E}g_2(X) + c\n\\end{align*}\n\\]\nIn the continuous case, the argument is essentially the same:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left(ag_1(X) + bg_2(X) + c\\right)\n&= \\int_\\mathbb{R} \\left(ag_1(x) + bg_2(x) + c\\right) f(x)dx \\\\\n&= a\\int_\\mathbb{R} g_1(x)f(x)dx + b\\int_\\mathbb{R} g_2(x)f(x)dx + c\\int_\\mathbb{R}f(x)dx \\\\\n&= a\\mathbb{E}g_1(X) + b\\mathbb{E}g_2(X) + c\n\\end{align*}\n\\]\nFor (iii), let \\(g_2(x) \\leq g_1(x)\\) for every \\(x \\in \\mathbb{R}\\). Then, in the discrete case:\n\\[\n\\mathbb{E}g_2(X) = \\sum_x g_2(x) P(X = x) \\leq \\sum_x g_1(x) P(X = x) = \\mathbb{E}g_1(X)\n\\]\nIn the continuous case:\n\\[\n\\mathbb{E}g_2(X) = \\int_\\mathbb{R} g_2(x) f(x) dx \\leq \\int_\\mathbb{R} g_1(x) f(x) dx = \\mathbb{E}g_1(X)\n\\]\nTo obtain (ii), set \\(g_2(x) \\equiv 0\\). Then if \\(0 \\leq g_1(x)\\) for every \\(x\\in\\mathbb{R}\\), one has \\(g_2 (x) \\leq g_1(x)\\) and so \\(\\mathbb{E}g_2(X) \\leq \\mathbb{E}g_1(X)\\). But \\(\\mathbb{E}g_2(X) = 0\\), so \\(0 \\leq \\mathbb{E}g_1(X)\\).\nTo obtain (iv), set \\(g_2(x) \\equiv a\\) and consider a function \\(g_3(x) \\equiv b\\). Then the expectations exist and \\(\\mathbb{E}g_2(X) = a\\) and \\(\\mathbb{E}g_3(X) = b\\). apply (iii) to obtain that if \\(g_2(x) \\leq g_1(x) \\leq g_3(x)\\) for all \\(x \\in \\mathbb{R}\\), the expectations are similarly ordered and thus \\(a \\leq \\mathbb{E}g_1(X) \\leq b\\).\n\n\nThese corollaries can often ease calculations. For example, it is immediate that for any random variable whose expectation exists, \\(\\mathbb{E}X^2 \\geq 0\\). Similarly, scaling and shifting a random variable scales and shifts the mean: \\(\\mathbb{E}(aX + b) = a\\mathbb{E}X + b\\).\nThe variance of a random variable is defined as the expectation: \\[\n\\text{var}X = \\mathbb{E}\\left(X - \\mathbb{E}X\\right)^2\n\\]\n\n\n\n\n\n\nExample: Bernoulli variance\n\n\n\nIf \\(X \\sim \\text{Bernoulli}(p)\\) then, noting that from above \\(\\mathbb{E}X = p\\), the variance of \\(X\\) is: \\[\n\\begin{align*}\n\\text{var}X\n&= \\mathbb{E}(X - \\mathbb{E}X)^2 \\\\\n&= \\mathbb{E}(X - p)^2 \\\\\n&= (1 - p)^2 P(X = 1) + (0 - p)^2 P(X = 0) \\\\\n&= (1 - p)^2 p + p^2 (1 - p) \\\\\n&= (1 - p)\\left((1 - p)p + p^2\\right) \\\\\n&= (1 - p)\\left(p - p^2 + p^2\\right) \\\\\n&= (1 - p)p\n\\end{align*}\n\\]\n\n\nWhile one could calculate the variance directly, as in the example above, this is often a cumbersome calculation in more complex cases. Instead, by the corollary, and noting that \\(\\mathbb{E}g(X)\\) is a constant, one can obtain the variance formula as: \\[\n\\text{var}X = \\mathbb{E}\\left(X^2 - 2X\\mathbb{E}X + \\mathbb{E}X\\mathbb{E}X\\right) = \\mathbb{E}X^2 - 2\\mathbb{E}X\\mathbb{E}X + \\mathbb{E}X\\mathbb{E}X = \\mathbb{E}X^2 - \\left(\\mathbb{E}X\\right)^2\n\\] Thus, to calculate the variance of a random variable, one simply needs to know \\(\\mathbb{E}X\\) and \\(\\mathbb{E}X^2\\).\n\n\n\n\n\n\nExample: Poisson variance\n\n\n\nIf \\(X \\sim \\text{Poisson}(\\lambda)\\), then from before \\(\\mathbb{E}X = \\lambda\\), and: \\[\n\\begin{align*}\n\\mathbb{E}X^2\n&= \\sum_{x = 0}^\\infty x^2 \\frac{\\lambda^x e^{-\\lambda}}{x!} \\\\\n&= \\lambda \\sum_{x = 0}^\\infty x \\frac{\\lambda^{x-1} e^{-\\lambda}}{(x - 1)!} \\\\\n&= \\lambda \\sum_{x = 0}^\\infty (x + 1) \\frac{\\lambda^{x} e^{-\\lambda}}{x!} \\\\\n&= \\lambda \\left[\\sum_{x = 0}^\\infty x \\frac{\\lambda^{x} e^{-\\lambda}}{x!} + 1 \\cdot \\frac{\\lambda^{x} e^{-\\lambda}}{x!} \\right] \\\\\n&= \\lambda \\left[\\sum_{x = 0}^\\infty x \\frac{\\lambda^{x} e^{-\\lambda}}{x!} + \\sum_{x = 0}^\\infty 1 \\cdot \\frac{\\lambda^{x} e^{-\\lambda}}{x!} \\right] \\\\\n&= \\lambda(\\lambda + 1)\n\\end{align*}\n\\]\nThen, by the variance formula: \\[\n\\text{var}X = \\mathbb{E}X^2 - \\left(\\mathbb{E}X\\right)^2 = \\lambda(\\lambda + 1) - \\lambda^2 = \\lambda^2 + \\lambda - \\lambda^2 = \\lambda\n\\]"
  },
  {
    "objectID": "archive/f23/notes/week9-joint.html",
    "href": "archive/f23/notes/week9-joint.html",
    "title": "Joint distributions",
    "section": "",
    "text": "This week we extend the concept of probability distributions to multiple variables and introduce multivariate probability distributions. A multivariate distribution can be thought of in either of two ways, as either:\n\nthe ‘joint distribution’ of two or more random variables considered together;\nthe distribution of a random vector.\n\nAll of the same concepts for univariate distributions — distribution functions, transformations, and expectations — extend easily to the multivariate setting.\n\nHouse hunting\nThe need for multivariate distributions can be motivated by a simple example: consider shopping for a home or apartment. We might record the number of bedrooms and bathrooms for every home as the vector: \\[\n\\mathbb{x} = (x_1, x_2) = (\\#\\text{ bedrooms}, \\#\\text{ bathrooms})\n\\]\nNow imagine selecting a home at random from current listings in your area; then \\(\\mathbb{X} = (X_1, X_2)\\) will be a random vector for which the ordered pairs of possible values \\((x_1, x_2)\\) have some distribution that reflects the frequency of combinations of bedrooms and bathrooms across current listings. Write the probability of selecting a home with \\(x_1\\) bedrooms and \\(x_2\\) bathrooms as a conjunction of events, that is, as: \\[\nP(X_1 = x_1, X_2 = x_2) = P(\\{X_1 = x_1\\} \\cap \\{X_2 = x_2\\})\n\\]\nSuppose the joint distribution of \\((X_1, X_2)\\), that is, the frequencies of bed/bath pairs among listings, is given by the table below.\n\n\n\n\n\n\n\n\n\n\n\n\\(x_1 = 0\\)\n\\(x_1 = 1\\)\n\\(x_1 = 2\\)\n\\(x_1 = 3\\)\n\n\n\n\n\\(x_2 = 1\\)\n0.1\n0.1\n0.2\n0\n\n\n\\(x_2 = 1.5\\)\n0\n0.1\n0.2\n0\n\n\n\\(x_2 = 2\\)\n0\n0\n0\n0.3\n\n\n\\(x_2 = 2.5\\)\n0\n0\n0\n0\n\n\n\nThe table indicates, for instance, that \\(P(X_1 = 2, X_2 = 1.5) = 0.2\\), meaning the probability that a randomly selected listing has 2 bedrooms and 1.5 bathrooms is 0.2.\nThe marginal probability that a randomly selected home has 1.5 bathrooms (regardless of the number of bedrooms) can be obtained by summing the probabilities in the corresponding row: \\[\nP(X_2 = 1.5) = \\sum_{x_1} P(X_1 = x_1, X_2, = 1.5) = 0 + 0.1 + 0.2 + 0 = 0.3\n\\]\nNotice that these probabilities are not necessarily information you could obtain if you knew the frequencies of values of \\(x_1\\) and of \\(x_2\\) separately. For instance, computing the marginal probabilities indicates that the most common number of bathrooms is 1.5 and the most common number of bedrooms is 2, but that doesn’t entail that the most frequent pair is 2 bed and 1.5 bath. Rather, 3 bed, 2 bath homes are most common. This is possible because the variables are measured together on each home rather than, say, on separate collections of homes.\nThe joint distribution therefore takes account of how variables interact across the outcomes of a random process. The example illustrates that when multiple variables are measured together, a joint distribution is needed to fully capture the probabilistic behavior of the variables.\n\n\nRandom vectors\nFormally, \\(X = (X_1, X_2)\\) is a random vector if for some probability space \\((S, \\mathcal{S}, P)\\) \\[\nX = (X_1, X_2): S \\longrightarrow \\mathbb{R}^2\n\\] and preimages of the Borel sets in \\(\\mathbb{R}^2\\) — sets that can be formed from countable collections of rectangles — have well-defined probabilities in the underlying space.\nAs with random variables, random vectors induce a probability measure \\[\nP_X (B) = P\\left(X^{-1}(B)\\right)\n\\]\nThe induced measure \\(P_X\\) is both the joint distribution of the random variables \\(X_1, X_2\\) and the distribution of the random vector \\(X\\).\nIn the house hunting example, we might formalize things as follows. Suppose the sample space is a collection of \\(N\\) listings \\(S = \\{s_1, \\dots, s_N\\}\\), and since the thought experiment involved selecting a listing at random, \\(P(s_i) = \\frac{1}{N}\\) for each \\(i\\). Then the measure induced by \\(X\\) would be computed as the probability of selecting a house with the specified number of bedrooms and bathrooms, resulting, for instance, in: \\[\nP_X ((1, 1.5)) = \\frac{\\#\\text{ 1br, 1.5ba homes}}{N}\n\\]\nThere is really no fundamental difference between joint distributions and univariate distributions — the former are simply distributions of vector-valued functions rather than univariate functions.\nThe definition above extends directly to vectors in \\(\\mathbb{R}^n\\) without modification. We will focus for now mostly on bivariate distributions, but where possible, concepts will be extended to collections of arbitrarily many random variables.\n\n\nCharacterizing multivariate distributions\nLet \\(X:S\\rightarrow\\mathbb{R}^n\\) be a random vector comprising \\(n\\) random variables \\(X_1, \\dots, X_n\\). The joint cumulative distribution function is defined as: \\[\nF(x_1, \\dots, x_n) = P_X \\left((-\\infty, x_1] \\times \\cdots \\times (-\\infty, x_n]\\right) = P(X_1 \\leq x_1, \\dots, X_n \\leq x_n)\n\\]\nAs with random variables, the joint CDF uniquely characterizes distributions, and is the basis for distinguishing discrete and continuous distributions.\nThe random vector \\(X\\) is discrete if its CDF \\(F\\) takes countably many values, and is continuous if \\(F\\) is continuous.\nIn the discrete case, the joint PMF is: \\[\nP(X_1 = x_1, \\dots, X_n = x_n) = P_X \\left(\\{x_1, \\dots, x_n\\}\\right)\n\\] In the continuous case, the joint PDF is the function \\(f\\) satisfying: \\[\nF(x_1, \\dots, x_n) = \\int_{-\\infty}^{x_1} \\cdots \\int_{-\\infty}^{x_n} f(x_1, \\dots, x_n)dx_n \\cdots dx_1\n\\] Typically, one has: \\[\nf(x_1, \\dots, x_n) = \\frac{\\partial^n}{\\partial x_1 \\cdots \\partial x_n} F(x_1, \\dots, x_n)\n\\] Joint PMFs/PDFs also uniquely characterize the distribution of \\(X\\). In fact, although the CDF is introduced here in the multivariate case in order to define discrete and continuous random vectors, it is rarely used in practice to compute probabilities, expectations, and the like. More often, distributions of random vectors are characterized by specifying the joint PMF/PDF.\nProbabilities associated with the random vector are given in relation to the joint PMF/PDF by: \\[\nP_X (B) = P(X \\in B) =\n\\begin{cases}\n\\sum_{x \\in B} P(X_1 = x_1, \\dots, X_n = x_n) \\\\\n\\int\\cdots\\int_B f(x_1, \\dots, x_n)dx_1\\cdots dx_n\n\\end{cases}\n\\]\nAn arbitrary function \\(f\\) is a joint PMF/PDF just in case it is nonnegative everywhere and sums/integrates to one.\n\n\n\n\n\n\nExample: calculating probabilities using a joint PDF\n\n\n\nLet \\((X_1, X_2)\\) have a uniform joint distribution on the unit circle:\n\\[\nf(x_1, x_2) = \\frac{1}{\\pi}\\;,\\quad x_1^2 + x_2^2 \\leq 1\n\\]\nIt is easy to check that this is a valid PDF since it is nonnegative everywhere and the area of the unit circle is \\(\\pi\\), so \\(f\\) clearly integrates to one over the support. To verify analytically, note that for fixed \\(x_1\\), one has \\(-\\sqrt{1 - x_1^2} \\leq x_2 \\leq \\sqrt{1 - x_1^2}\\), and across all values of \\(x_2\\), one has \\(-1\\leq x_1 \\leq 1\\), so:\n\\[\n\\int_{-1}^1 \\int_{-\\sqrt{1 - x_1^2}}^\\sqrt{1 - x_1^2} \\frac{1}{\\pi}dx_2 dx_1\n\\]\nIn this example it is a little easier to compute probabilities via areas, since for any region \\(B\\in\\mathbb{R}^2\\), the probability of \\(B\\) is simply the area of its intersection with the unit circle, divided by \\(\\pi\\). That is, denoting the support of the random vector by \\(S = \\{(x_1, x_2)\\in\\mathbb{R}2: x_1^2 + x_2^2 \\leq 1\\}\\), one has:\n\\[\nP_X(B) = \\frac{1}{\\pi}\\times\\text{area}(B \\cap S)\n\\]\nSo for instance, if the event of interest is that the random vector \\(X\\) lies in the positive quadrant, the intersection of the unit circle with the positive quadrant comprises a quarter of the area of the unit circle, so the probability is \\(\\frac{1}{4}\\).\nMore formally, if \\(B = \\{(x_1, x_2)\\in\\mathbb{R}^2: x_1 \\geq 0, x_2 \\geq 0\\}\\), then \\(\\text{area}(B\\cap S) = \\frac{\\pi}{4}\\), so:\n\\[\nP_X(B) = \\frac{1}{\\pi}\\times\\frac{\\pi}{4} = \\frac{1}{4}\n\\]To compute the probability analytically using the PDF, we need to determine the integration bounds. Fixing \\(x_1\\), one has that on \\(B\\cap S\\) the values of \\(x_2\\) are given by \\(0 \\leq x_2 \\leq \\sqrt{1 - x_1^2}\\), and across all values of \\(x_2\\) on \\(B \\cap S\\), the values of \\(x_1\\) are given by \\(0 \\leq x_1 \\leq 1\\). Polar coordinates simplify the integration:\n\\[\n\\begin{align*}\nP(X_1 \\geq 0, X_2 \\geq 0)\n&= \\int\\int_{B\\cap S} \\frac{1}\\pi dx_2 dx_1 \\\\\n&= \\int_0^1\\int_0^\\sqrt{1 - x_1^2} \\frac{1}{\\pi}dx_2 dx_1\\\\\n&= \\int_0^\\frac{\\pi}{2}\\int_0^1 \\frac{r}{\\pi}drd\\theta\\\\\n&= \\int_0^\\frac{\\pi}{2} \\frac{1}{2\\pi}d\\theta\\\\\n&= \\frac{1}{4}\n\\end{align*}\n\\]\nOften the trickiest part of computing probabilities from joint PDFs is determining appropriate integration bounds. It helps considerably to sketch the support set and region of interest; we’ll review this technique in class.\nCheck your understanding\nFind \\(P(X_1 \\geq X_2)\\) both informally using areas, and analytically using integration.\n\n\nThe CDFs of individual components of a random vector can be obtained by integration or summation. Note that the event \\(\\{X_j \\leq x_j\\}\\) is equivalent to \\(\\{X_j \\leq x_j\\}\\cap\\left[\\bigcap_{i \\neq j}\\{-\\infty &lt; X_i &lt; \\infty\\}\\right]\\). So: \\[\nP(X_j \\leq x_j) = \\lim_{x_{\\neg j}\\rightarrow\\infty} F(x_1, \\dots, x_n)\n\\] For instance, in the bivariate case, if \\(X = (X_1, X_2)\\) has CDF \\(F\\), then the CDF of \\(X_2\\) alone is: \\[\nP(X_2 \\leq x) = \\lim_{x_1 \\rightarrow \\infty} F(x_1, x)\n\\] This CDF can also be obtained from the PMF/PDF as: \\[\nP(X_2 \\leq x)\n=\\begin{cases}\n  \\int_{-\\infty}^{x} \\underbrace{\\left[\\int_{-\\infty}^\\infty f(x_1, x_2)dx_1\\right]}_{\\text{PDF of } X_2} dx_2 \\\\\n  \\sum_{x_2 \\leq x} \\underbrace{\\left[\\sum_{x_1} P(X_1 = x_1, X_2 = x_2)\\right]}_{\\text{PMF of } X_2}\n\\end{cases}\n\\]\nThe expressions in square brackets must be the PDF/PMF of \\(X_2\\), since distribution functions are unique. Thus, the marginal distributions of individual vector components are given, in the continuous case, by ‘integrating out’ the other components: \\[\n\\begin{align*}\nf_1(x_1) &= \\int_\\mathbb{R} f(x_1, x_2) dx_2 \\\\\nf_2(x_2) &= \\int_\\mathbb{R} f(x_1, x_2) dx_1 \\\\\n\\end{align*}\n\\]\nIn the discrete case, the marginal distributions are obtained by summing out the other components: \\[\n\\begin{align*}\nP(X_1 = x_1) &= \\sum_{x_2} P(X_1 = x_1, X_2 = x_2) \\\\\nP(X_2 = x_2) &= \\sum_{x_1} P(X_1 = x_1, X_2 = x_2) \\\\\n\\end{align*}\n\\]\n\n\n\n\n\n\nExample: finding marginal distributions\n\n\n\nIf the random vector \\(X = (X_1, X_2)\\) has a uniform joint distribution on the unit circle (continuing the previous example), then the marginal distribution of \\(X_1\\) is given by:\n\\[\n\\begin{align*}\nf_1 (x_1)\n&= \\int_{-\\sqrt{1 - x_2^2}}^\\sqrt{1 - x_2^2} \\frac{1}{\\pi}dx_1\n= \\frac{2}{\\pi}\\sqrt{1 - x_1^2}\n\\;,\\quad x_1 \\in (0, 1)\n\\end{align*}\n\\]\nThe bounds of integration are found by reasoning that for fixed \\(x_2\\), the possible values of \\(x_1\\) are given by \\(-\\sqrt{1 - x_2^2} \\leq x_1 \\leq \\sqrt{1 - x_2^2}\\). The marginal support of \\(X_1\\) is \\(S_1 = (0, 1)\\).\nIt is perhaps somewhat surprising that \\(X_1\\) is not marginally uniform, given that the vector \\((X_1, X_2)\\) has a uniform distribution. One way to understand this fact is that if all points on the unit circle occur with equal frequency, then not all values of the \\(X_1\\) coordinate will occur with the same frequency; in particular, larger values of \\(X_1\\) are less likely since the corresponding regions in the circle comprise fewer points.\nCheck your understanding\nVerify that the marginal density above is in fact a valid PDF (hint: use the transformation \\(x = \\sin\\theta\\) to compute the integral).\n\n\n\n\nExpectations\nThe expectation of a random vector \\(X\\) is defined as the vector of marginal expectations, assuming they exist: \\[\n\\mathbb{E}X = \\left[\\begin{array}{c}\n\\mathbb{E}X_1 \\\\\n\\vdots\\\\\n\\mathbb{E}X_n\n\\end{array}\n\\right]\n\\]\nHowever, the expected value of a function \\(g(x_1, \\dots, x_n)\\) is defined, assuming the sums/integrals exist, as: \\[\n\\mathbb{E}\\left[g(X_1, \\dots, X_n)\\right]\n= \\begin{cases}\n\\int\\cdots\\int_{\\mathbb{R}^n} g(x_1, \\dots, x_n)f(x_1, \\dots, x_n)dx_1\\cdots dx_n \\\\\n\\sum_{x_1}\\cdots\\sum_{x_n} g(x_1, \\dots, x_n) P(X_1 = x_1, \\dots, X_n = x_n)\n\\end{cases}\n\\]\n\n\n\n\n\n\nExample: house hunting\n\n\n\nConsider again the house hunting example where \\(X_1\\) denotes the number of bedrooms and \\(X_2\\) denotes the number of bathrooms, and for a randomly selected listing the vector \\((X_1, X_2)\\) has joint distribution:\n\n\n\n\n\\(x_1 = 0\\)\n\\(x_1 = 1\\)\n\\(x_1 = 2\\)\n\\(x_1 = 3\\)\n\n\n\n\n\\(x_2 = 1\\)\n0.1\n0.1\n0.2\n0\n\n\n\\(x_2 = 1.5\\)\n0\n0.1\n0.2\n0\n\n\n\\(x_2 = 2\\)\n0\n0\n0\n0.3\n\n\n\\(x_2 = 2.5\\)\n0\n0\n0\n0\n\n\n\nSuppose you want to know the expected ratio of bedrooms to bathrooms. The expectation is:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[\\frac{X_1}{X_2}\\right]\n&= \\sum_{(x_1, x_2)} \\frac{x_1}{x_2}P(X_1 = x_1, X_2 = x_2) \\\\\n&= \\frac{0}{1}\\cdot 0.1 + \\frac{1}{1}\\cdot 0.1 + \\frac{2}{1}\\cdot 0.2 + \\frac{1}{1.5}\\cdot 0.1 + \\frac{2}{1.5}\\cdot 0.2 + \\frac{3}{2}\\cdot 0.3 \\\\\n&= 1.283\n\\end{align*}\n\\]\nSo on average, a randomly selected home will have 1.283 bedrooms to every bathroom.\n\n\nBased on this definition, it is easy to show that expectation is a linear operator. In the bivariate case: \\[\n\\mathbb{E}\\left[aX_1 + bX_2 + c\\right] = a\\mathbb{E}X_1 + b\\mathbb{E}X_2 + c\n\\] Slightly more generally: \\[\n\\mathbb{E}\\left[a_0 + \\sum_{i = 1}^n a_i X_i\\right] = a_0 + \\sum_{i = 1}^n a_i \\mathbb{E}X_i\n\\]The proofs are obtained from direct application of the definition of expectation given immediately above, and are left as exercises.\n\n\n\n\n\n\nExample: flu season\n\n\n\nSuppose the random vector \\(X = (X_1, X_2)\\) denotes the number of influenza A and influenza B cases per week in a given region, and suppose the joint distribution is given by the PMF:\n\\[\nP(X_1 = x_1, X_2 = x_2) = \\frac{\\mu_1^{x_1}\\mu_2^{x_2}\\exp\\{-(\\mu_1 + \\mu_2)\\}}{x_1! x_2!}\n\\quad\n\\begin{cases}\nx_1 = 0, 1, 2, \\dots \\\\\nx_2 = 0, 1, 2, \\dots \\\\\n\\mu_1 &gt; 0 \\\\\n\\mu_2 &gt; 0\n\\end{cases}\n\\]\nThe marginal distributions are given by:\n\\[\n\\begin{align*}\nP(X_1 = x_1)\n&= \\sum_{x_2 = 0}^\\infty P(X_1 = x_1, X_2 = x_2) \\\\\n&= \\frac{\\mu_1^{x_1}e^{-\\mu_1}}{x_1!} \\sum_{x_2 = 0}^\\infty \\frac{\\mu_2^{x_2} e^{-\\mu_2}}{x_2!} \\\\\n&= \\frac{\\mu_1^{x_1}e^{-\\mu_1}}{x_1!}\n\\;,\\quad x_1 = 0, 1, 2, \\dots \\\\\nP(X_2 = x_2)\n&= \\sum_{x_1 = 0}^\\infty P(X_1 = x_1, X_2 = x_2) \\\\\n&= \\frac{\\mu_2^{x_2}e^{-\\mu_2}}{x_2!} \\sum_{x_1 = 0}^\\infty \\frac{\\mu_1^{x_1} e^{-\\mu_1}}{x_1!} \\\\\n&= \\frac{\\mu_2^{x_2}e^{-\\mu_2}}{x_2!}\n\\;,\\quad x_2 = 0, 1, 2, \\dots\n\\end{align*}\n\\]\nIn other words, \\(X_1 \\sim \\text{Poisson}(\\mu_1)\\) and \\(X_2 \\sim\\text{Poisson}(\\mu_2)\\). Therefore \\(\\mathbb{E}X_1 = \\mu_1\\) and \\(\\mathbb{E}X_2 = \\mu_2\\), so by linearity of expectation the expected total number of flu cases is:\n\\[\n\\mathbb{E}\\left[X_1 + X_2\\right] = \\mathbb{E}X_1 + \\mathbb{E}X_2 = \\mu_1 + \\mu_2\n\\]\nWhat if we want to know not just the expected total number of flu cases, but its probability distribution? The strategy here is to make a one-to-one transformation in which one of the transformed variables is the sum, and then compute the marginal distribution of that transformed variable. To that end, define:\n\\[\nY_1 = X_1 + X_2\n\\quad\\text{and}\\quad\nY_2 = X_2\n\\]\nThis transformation is one-to-one, and the support set is given by \\(S_Y = \\{(y_1, y_2): y_1 = 0, 1, 2, \\dots; y_2 = 0, 1, 2, \\dots, y_1\\}\\). The inverse transformation is given by:\n\\[\nX_1 = Y_1 - Y_2\n\\quad\\text{and}\\quad\nX_2 = Y_2\n\\]\nSo the joint distribution of \\(Y = (Y_1, Y_2)\\) is:\n\\[\nP(Y_1 = y_1, Y_2 = y_2)\n= P(X_1 = y_1 - y_2, X_2 = y_2)\n= \\frac{\\mu_1^{y_1 - y_2}\\mu_2^{y_2}\\exp\\{-(\\mu_1 + \\mu_2)\\}}{(y_1 - y_2)! y_2!}\n\\]And therefore the marginal distribution of \\(Y_1\\) is obtained by summing out \\(Y_2\\). Note that \\(y_2 \\leq y_1\\), so the sum should be computed up to \\(y_1\\).\n\\[\n\\begin{align*}\nP(Y_1 = y_1)\n&= \\sum_{y_2 = 0}^{y_1} P(Y_1 = y_1, Y_2 = y_2) \\\\\n&= \\sum_{y_2 = 0}^{y_1} \\frac{\\mu_1^{y_1 - y_2}\\mu_2^{y_2}\\exp\\{-(\\mu_1 + \\mu_2)\\}}{(y_1 - y_2)! y_2!} \\\\\n&= \\frac{\\exp\\{-(\\mu_1 + \\mu_2)\\}}{y_1!}\\sum_{y_2 = 0}^{y_1} {y_1 \\choose y_2} \\mu_1^{y_1 - y_2} \\mu_2^{y_2} \\\\\n&= \\frac{(\\mu_1 + \\mu_2)^{y_1}\\exp\\{-(\\mu_1 + \\mu_2)\\}}{y_1!}\n\\end{align*}\n\\]\nSo \\(Y_1 = X_1 + X_2 \\sim\\text{Poisson}(\\mu_1 + \\mu_2)\\).\n\n\nThe above example illustrates a bivariate transformation. This will be our next topic."
  },
  {
    "objectID": "archive/f23/notes/week3-conditional.html",
    "href": "archive/f23/notes/week3-conditional.html",
    "title": "Conditional probability",
    "section": "",
    "text": "Let \\((S, \\mathcal{S}, P)\\) be a probability space and let \\(A \\in \\mathcal{S}\\) be an event with \\(P(A) &gt; 0\\). The conditional probability of any event \\(E\\in\\mathcal{S}\\) given \\(A\\) is defined as: \\[\nP(E\\;|A) = \\frac{P(E\\cap A)}{P(A)}\n\\]\nThis is interpreted as the chance of \\(E\\) provided that \\(A\\) has occurred. Importantly, \\(E|A\\) is not an event; rather, \\(P(\\cdot\\;| A)\\) is a new probability measure. To see this, check the axioms:\n\n(A1) Since \\(P\\) is a probability measure, \\(P(E\\cap A) \\geq 0\\), so it follows \\(P(E\\;|A) = \\frac{P(E\\cap A)}{P(A)} \\geq 0\\).\n(A2) \\(P(S\\;|A) = \\frac{P(S\\cap A)}{P(A)} = \\frac{P(A)}{P(A)} = 1\\)\n(A3) If \\(\\{E_i\\}\\) is a disjoint collection, then \\(\\{E_i \\cap A\\}\\) is also a disjoint collection, so by countable additivity of \\(P\\), one has: \\[\nP\\left(\\bigcup_i E_i \\;\\big|\\; A\\right) = \\frac{P\\left(\\left[\\bigcup_i E_i\\right]\\cap A\\right)}{P(A)} = \\frac{P\\left(\\bigcup_i (E_i\\cap A)\\right)}{P(A)} = \\sum_i \\frac{P(E_i\\cap A)}{P(A)} = \\sum_i P(E_i\\;|A)\n\\]\n\nOne can view \\(P(\\cdot\\;|A)\\) as a probability measure on \\((S, \\mathcal{S})\\), or as a probability measure on \\(\\left(A, \\mathcal{S}^A\\right)\\) where \\(\\mathcal{S}^A = \\{E\\cap A: E \\in \\mathcal{S}\\}\\). Some prefer the latter view, since it aligns with the interpretation that by conditioning on \\(A\\) one is redefining the sample space.\n\nBasic properties\nAn immediate consequence of the definition is that: \\[\nP(E\\cap A) = P(E\\;| A) P(A)\n\\]\nIn fact, this multiplication rule for conditional probabilities can be generalized to an arbitrary finite collection of events: \\[\nP\\left(\\bigcap_{i = 1}^n E_i\\right) = P(E_1) \\times P(E_2\\;|E_1) \\times P(E_3\\;|E_1 \\cap E_2) \\times\\cdots\\times P(E_n\\;| E_1 \\cap \\cdots \\cap E_{n - 1})\n\\]\nOr, written more compactly: \\[\nP\\left(\\bigcap_{i = 1}^n E_i\\right) = P(E_1) \\times \\prod_{i = 2}^n P\\left(E_i \\;\\Bigg| \\bigcap_{j = 1}^{i - 1} E_j \\right)\n\\]\n\n\n\n\n\n\nProof\n\n\n\nApply the definition of conditional probability to the terms in the product on the right hand side to see that: \\[\n\\begin{align*}\nP(E_1) \\times \\prod_{i = 2}^n P\\left(E_i \\;\\Bigg| \\bigcap_{j = 1}^{i - 1} E_j \\right)\n&= P(E_1) \\times \\prod_{j = 2}^n \\left[\\frac{P\\left(\\bigcap_{j = 1}^i E_j\\right)}{P\\left(\\bigcap_{j = 1}^{i - 1} E_j \\right)}\\right] \\\\\n&= P(E_1) \\times \\frac{P\\left(\\bigcap_{j = 1}^2 E_j\\right)}{P\\left( E_1 \\right)}\n  \\times \\frac{P\\left(\\bigcap_{j = 1}^3 E_j\\right)}{P\\left(\\bigcap_{j = 1}^{2} E_j \\right)}\n  \\times\\cdots\n  \\times \\frac{P\\left(\\bigcap_{j = 1}^n E_j\\right)}{P\\left(\\bigcap_{j = 1}^{n - 1} E_j \\right)}\n\\end{align*}\n\\] Then notice that all terms cancel, leaving only \\(P\\left(\\bigcap_{j = 1}^n E_j\\right)\\), and establishing the result.\n\n\nThe multiplication rule provides a convenient way to compute certain probabilities, as in some problems it’s easier to find a conditional probability than an unconditional one.\n\n\n\n\n\n\nExample: (more) poker hands\n\n\n\nConsider drawing 5 cards at random. What’s the probability that all 5 are diamonds?\nThis could be found by computing the total number of ways to draw 5 diamonds out of the total number of ways to draw 5 cards: \\[\n\\frac{{13 \\choose 5}}{{52 \\choose 5}}\n= \\frac{13!}{5!8!}\\times\\frac{5!47!}{52!}\n\\] Or, one can avoid evaluating the factorials and instead notice that the conditional probability of drawing a diamond given having already drawn \\(k\\) diamonds is \\(\\frac{13 - k}{52 - k}\\) — the number of diamonds left as a proportion of the number of cards left — so: \\[\nP\\left(\\text{5 diamonds}\\right) = \\frac{13}{52}\\times\\frac{12}{51}\\times\\frac{11}{50}\\times\\frac{10}{49}\\times\\frac{9}{48} \\approx 0.000495\n\\]\nFor a quick exercise, check the result by simplifying the factorials in the first solution.\nTo see why this is an application of the multiplication rule for conditional probabilities more formally, let \\(E_i = \\{\\text{draw a diamond on the $k$th draw}\\}\\). Then: \\[\n\\begin{align*}\nP\\left(\\text{5 diamonds}\\right)\n&= P(E_1 \\cap E_2 \\cap E_3 \\cap E_4 \\cap E_5) \\\\\n&= P(E_1)\n  \\times P(E_2\\;| E_1)\n  \\times P(E_3\\;| E_1 \\cap E_2) \\\\\n  &\\qquad\\times P(E_4 \\;| E_1 \\cap E_2 \\cap E_3)\n  \\times P(E_5\\;| E_1 \\cap E_2 \\cap E_3 \\cap E_4)\n\\end{align*}\n\\]\n\n\nAnother useful property is the law of total probability: if \\(\\{A_i\\}\\) is a partition of the sample space \\(S\\), then for any event \\(E \\in \\mathcal{S}\\) one has: \\[\nP(E) = \\sum_i P(E\\;| A_i) P(A_i)\n\\]\n\n\n\n\n\n\nProof\n\n\n\nNote that \\(E = E \\cap S = E\\cap \\left[\\bigcup_i A_i\\right] = \\bigcup_i (E \\cap A_i)\\). Since \\(\\{A_i\\}\\) is disjoint, so is \\(\\{E \\cap A_i\\}\\). Then by countable additivity: \\[\nP(E) = P\\left[\\bigcup_i (E \\cap A_i)\\right] = \\sum_i P(E \\cap A_i)\n\\] And by the multiplication rule for conditional probabilities \\(P(E\\cap A_i) = P(E\\;| A_i) P(A_i)\\) so: \\[\nP(E) = \\sum_i P(E \\cap A_i) = \\sum_i P(E\\;| A_i) P(A_i)\n\\]\n\n\n\n\n\n\n\n\nExample: CVD rates\n\n\n\nIt’s estimated that 8% of men and 0.5% of women have color vision deficiency (CVD). Supposing that exactly these proportions appear in a group of 400 women and 200 men, what’s the probability that a randomly selected individual has CVD?\nFrom the problem set-up:\n\n\\(P(\\text{CVD}\\;| M) = 0.08\\) and \\(P(\\text{CVD}\\;| F) = 0.005\\)\n\\(P(M) = \\frac{1}{3}\\) and \\(P(F) = \\frac{2}{3}\\)\n\nNote that these are the probabilities of selecting a person with the specified attributes from this group, assuming all individuals are equally likely to be selected. This is consistent with how we’ve defined probabilities for finite sample spaces. In this case, the sample space is the collection of 600 individuals, and each individual is assigned selection probability \\(\\frac{1}{600}\\). Thus, the meaning of the probability here comes from sampling from this group of people at random; importantly, these are not statements about the chance of having CVD, or being of one sex or the other, or the like.\nThe law of total probability yields: \\[\n\\begin{align*}\nP(\\text{CVD})\n&= P(\\text{CVD}\\;| M) P(M) + P(\\text{CVD}\\;| F) P(F) \\\\\n&= 0.08 \\cdot \\frac{1}{3} + 0.005 \\cdot\\frac{2}{3} \\\\\n&= 0.03\n\\end{align*}\n\\]\n\n\n\n\nIndependence\nIf two events are independent, then the occurrence of one event doesn’t affect the probability of the other. For example, obtaining heads in a coin toss doesn’t affect the chances of obtaining a heads in a subsequent toss.\nBy contrast, if two events are dependent, then the occurrence of one changes the probability of the other. For example, the likelihood of a car accident is higher in heavy rain.\nFor independent events, conditioning on one event does not change the probability of the other. Thus, we say that any two events \\(E\\) and \\(A\\) are independent just in case: \\[\nP(E \\cap A) = P(E)P(A)\n\\]\nIf so, we write \\(E\\perp A\\).\nWhile it might be more intuitive to define independence according to the criterion \\(P(E\\;| A) = P(E)\\), recall that \\(P(E\\;| A)\\) is undefined if \\(P(A) = 0\\). By using instead the (almost equivalent) criterion \\(P(E \\cap A) = P(E)P(A)\\), the concept is still well-defined for events with probability zero.\nNote two facts:\n\nindependent events are not disjoint unless at least one event has probability zero\nevents with probability zero are independent of all other events, including themselves.\n\n\n\n\n\n\n\nProof\n\n\n\nFor the first fact, observe that for any disjoint events \\(A, B\\), one has \\(P(A \\cap B) = P(\\emptyset) = 0\\), so \\(P(A \\cap B) = P(A)P(B)\\) cannot hold unless either \\(P(A) = 0\\) or \\(P(B) = 0\\).\nFor the second fact, if \\(P(A) = 0\\), then for any event \\(B\\), by monotonicity of the probability measure \\(B \\cap A \\subseteq A\\) implies \\(P(B\\cap A) \\leq P(A) = 0\\), so \\(P(B \\cap A) = 0\\). Therefore \\(P(B\\cap A) = P(A)P(B)\\).\n\n\n\n\n\n\n\n\nExample: dice rolls\n\n\n\nConsider rolling two six-sided dice. The sample space for this experiment is \\(S = \\{1, 2, 3, 4, 5, 6\\}^2\\), i.e., all ordered pairs of integers between 1 and 6 corresponding to the physical possibilities for the two dice. If all outcomes are equally likely then \\(P((i, j)) = \\frac{1}{|S|} = \\frac{1}{36}\\) for all \\(i, j \\in \\{1, \\dots, 6\\}\\).\nIn this scenario the value of the first die is independent of the value of the second die. While this is intuitively obvious, to see this probabilistically, let \\(A_x = \\{(i, j) \\in S: i = x\\}\\) denote the event that the first die is an \\(i\\), and let \\(B_y = \\{(i, j)\\in S: j = y\\}\\) denote the event that the second die is a \\(j\\). Now \\(|A_x| = |B_y| = 6\\) for each \\(x\\) and each \\(y\\), so: \\[\nP(A_x) = P(B_x) = \\frac{6}{36} = \\frac{1}{6}\n\\] Then note that for any \\(x, y\\), \\(A_x \\cap B_y = \\{(x, y)\\}\\), so: \\[\nP(A_x \\cap B_y) = \\frac{1}{36} = \\frac{1}{6}\\times\\frac{1}{6} = P(A_x)P(B_y)\n\\]\nThere are many probability measures on this space that are not equally likely but for which \\(A_x, B_y\\) remain independent. For example, the table below shows probabilities assigned to each of the 36 possible rolls that are not equally likely, but it is easy to verify that the product of the probabilities in the row/column headers yields the probability in the corresponding cell.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P(B_1) = \\frac{1}{3}\\)\n\\(P(B_2) = \\frac{1}{6}\\)\n\\(P(B_3) = 0\\)\n\\(P(B_4) = \\frac{1}{6}\\)\n\\(P(B_5) = \\frac{1}{6}\\)\n\\(P(B_6) = \\frac{1}{6}\\)\n\n\n\n\n\\(P(A_1) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_2) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_3) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_4) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_5) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_6) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\nNow consider the following probability measure. Note it is still a valid probability measure because the entries in the table sum to one.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P(B_1) = \\frac{11}{36}\\)\n\\(P(B_2) = \\frac{7}{36}\\)\n\\(P(B_3) = 0\\)\n\\(P(B_4) = \\frac{1}{6}\\)\n\\(P(B_5) = \\frac{1}{6}\\)\n\\(P(B_6) = \\frac{1}{6}\\)\n\n\n\n\n\\(P(A_1) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_2) = \\frac{1}{6}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_3) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_4) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_5) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_6) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\nCheck your understanding:\n\nAre \\(A_x, B_y\\) independent under this probability measure? How do you check?\nWrite the a table of the probabilities \\(P(A_x | B_1)\\).\n\n\n\nA collection of events \\(\\{E_i\\}\\) is pairwise independent if for every pair of distinct events \\(E_i, E_j\\) one has: \\[\nP(E_i \\cap E_j) = P(E_i)P(E_j)\n\\]\nA collection of events \\(\\{E_i\\}\\) is mutually independent if for every \\(k\\) and every subcollection of \\(k\\) distinct events \\(E_{i_1}, \\dots, E_{i_k}\\): \\[\nP\\left(\\bigcap_{j = 1}^k E_{i_j}\\right) = \\prod_{j = 1}^k P(E_{i_j})\n\\]\n\n\n\n\n\n\nExample\n\n\n\nConsider tossing two coins, so \\(S = \\{HH, HT, TH, TT\\}\\), and assume all outcomes are equally likely. Define the events:\n\\[\n\\begin{align*}\nE_1 &= \\{HH, HT\\} \\quad(\\text{heads on first toss}) \\\\\nE_2 &= \\{HH, TH\\} \\quad(\\text{heads on second toss}) \\\\\nE_3 &= \\{HH, TT\\} \\quad(\\text{tosses match})\n\\end{align*}\n\\]\nNow, \\(P(E_i) = \\frac{1}{2}\\) for each \\(E_i\\), since each has two outcomes, so \\(P(E_i)P(E_j) = \\frac{1}{4}\\). Moreover, \\(P(E_i \\cap E_j) = P(\\{HH\\}) = \\frac{1}{4}\\) for each pair of events \\(i \\neq j\\), since \\(HH\\) is the only shared outcome between any two events. However: \\[\nP(E_1 \\cap E_2 \\cap E_3) = P(\\{HH\\}) = \\frac{1}{4} \\neq \\frac{1}{8} = P(E_1)P(E_2)P(E_3)\n\\] So the events are pairwise independent, but not mutually independent.\n\n\n\n\nBayes’ theorem\nIn many circumstances one may be able to compute \\(P(A\\;| B)\\) but wish to know \\(P(B\\;| A)\\). For example, you might have a good estimate for a diagnostic test of the probability that the test is positive given that a condition, illness, or pathogen is present, but what would really be much more useful to patients is the probability that one has the condition, illness, or pathogen given that they obtained a positive test result. The following theorem provides a means of accomplishing this.\nBayes’ theorem. If \\(\\{E_i\\}\\) is a partition of \\(S\\) and \\(A\\) is an event with nonzero probability, then:\n\\[\nP(E_i\\;|A) = \\frac{P(A\\;| E_i) P(E_i)}{\\sum_i P(A\\;| E_i) P(E_i)}\n\\]\n\n\n\n\n\n\nProof\n\n\n\nThe result follows from the multiplication rule and the law of total probability. Since \\(\\{E_i\\}\\) is a partition, \\(P(A) = \\sum_i P(A\\;| E_i) P(E_i)\\); further, \\(P(E_i \\cap A) = P(A\\;| E_i) P(E_i)\\) Then: \\[\nP(E_i\\;|A) = \\frac{P(E_i \\cap A)}{P(A)} = \\frac{P(A\\;| E_i) P(E_i)}{\\sum_i P(A\\;| E_i) P(E_i)}\n\\]\n\n\nThe probability \\(P(E_i \\;| A)\\) is referred to as the posterior probability and the probability \\(P(E_i)\\) is referred to as the prior probability. The theorem provides a means of obtaining the posterior from the priors (assuming the conditional probabilities \\(P(A\\;| E_i)\\) can be computed) and is sometimes interpreted as a rule for ‘updating’ the probabilities \\(P(E_i)\\).\nAs a special case, note that for any event \\(E\\), \\(\\{E, E^C\\}\\) partition the sample space. Therefore: \\[\nP(E\\;| A) = \\frac{P(A\\;| E) P(E)}{P(A\\;| E) P(E) + P(A\\;| E^C) P(E^C)}\n\\]\n\n\n\n\n\n\nExample: diagnostic test\n\n\n\nConsider a diagnostic test and let \\(A\\) denote the event one has the condition of interest, \\(T_+\\) denote the event one receives a positive test result, and \\(T_-\\) denote the event one receives a negative test result. Assume that laboratory controls indicate that:\n\\[\n\\begin{align*}\nP(T_+ \\;| A) &= 0.99 \\\\\nP(T_- \\;| A) &= 0.01 \\\\\nP\\left(T_+ \\;| A^C\\right) &= 0.05 \\\\\nP\\left(T_- \\;| A^C\\right) &= 0.95\n\\end{align*}\n\\] As an aside, \\(P(T_+\\;| A)\\) is the true positive rate, also known as the sensitivity of the test; \\(P(T_- \\;| A^C)\\) is the true negative rate, also known as the specificity. Note that \\(P(T_+ \\;| A) + P(T_- \\;| A^C) \\neq 1\\), despite the fact that \\(T_+ = (T_-)^C\\); this is because conditioning on different events produces different probability measures.\nSuppose first that the condition has a prevalence of \\(0.5\\%\\) in the general population, so assume \\(P(A) = 0.005\\). Then the probability that one has the condition given a positive test result is: \\[\n\\begin{align*}\nP(A \\;| T_+)\n&= \\frac{P(T_+ \\;| A) P(A)}{P(T_+ \\;| A) P(A) + P\\left(T_+ \\;| A^C\\right) P\\left(A^C\\right)} \\\\\n&= \\frac{(0.99)(0.005)}{(0.99)(0.005) + (0.05)(0.995)}\\\\\n&\\approx 0.0905\n\\end{align*}\n\\] Further, the probability that one has the condition given a negative test result is: \\[\n\\begin{align*}\nP(A \\;| T_-)\n&= \\frac{P(T_- \\;| A) P(A)}{P(T_- \\;| A) P(A) + P\\left(T_- \\;| A^C\\right) P\\left(A^C\\right)}  \\\\\n&= \\frac{(0.01)(0.005)}{(0.01)(0.005) + (0.95)(0.995)} \\\\\n&\\approx 0.0000529\n\\end{align*}\n\\]\nThis usually comes as a surprise, given that the test appears to be fairly accurate, but somewhat prone to false negatives. What happens if the condition is more prevalent, say, present in \\(10\\%\\) of the population? In this scenario:\n\\[\n\\begin{align*}\nP(A \\;| T_+)\n&= \\frac{(0.99)(0.1)}{(0.99)(0.1) + (0.05)(0.9)}\n\\approx 0.688 \\\\\nP(A \\;| T_-)\n&= \\frac{(0.05)(0.1)}{(0.05)(0.1) + (0.95)(0.9)}\n\\approx 0.0012\n\\end{align*}\n\\]\nTry a few other scenarios on your own. First make a guess about how the posterior probability will change, and then check the calculation.\n\nWhat happens if the false positive rate \\(P\\left(T_+\\;| A^C\\right)\\) is higher/lower, keeping everything else fixed?\nWhat happens if the false negative rate \\(P\\left(T_- \\;| A)\\right)\\) is higher/lower, keeping everything else fixed?\nWhat happens if the specificity increases to \\(99.5\\%\\) but the false negative rate increases to \\(10\\%\\)?\nWhat happens if the sensitivity decreases to \\(91\\%\\) but the false positive rate drops to \\(0.1\\%\\)?\n\n\n\nThe odds of an event \\(E\\) usually refers to the ratio \\(\\frac{P(E)}{P\\left(E^C\\right)}\\), but to be more precise, this is really the odds of \\(E\\) relative to its complement \\(E^C\\). More generally, for any two events \\(E_1, E_2\\): \\[\n\\text{odds}\\left(E_1, E_2\\right) = \\frac{P(E_1)}{P(E_2)}\n\\] The value of the odds gives the factor by which \\(E_1\\) is more likely than \\(E_2\\), and:\n\nif \\(\\text{odds}\\left(E_1, E_2\\right) = 1\\), the events are equally likely;\nif \\(\\text{odds}\\left(E_1, E_2\\right) &lt; 1\\), \\(E_2\\) is more likely than \\(E_1\\);\nif \\(\\text{odds}\\left(E_1, E_2\\right) &gt; 1\\), \\(E_1\\) is more likely than \\(E_2\\).\n\nThe conditional odds of one event relative to another is defined identically but with a conditional probability measure: \\[\n\\text{odds}\\left(E_1, E_2 \\;| A\\right) = \\frac{P(E_1 \\;| A)}{P(E_2\\;| A)}\n\\] One way to understand the diagnostic test example is that, although the probability of having the condition given a positive result is low, conditioning on the test result increases the posterior odds substantially.\n\n\n\n\n\n\nExample: diagnostic test (cont’d)\n\n\n\nThe prior odds of having the condition are: \\[\n\\text{odds}\\left(A, A^C\\right) = \\frac{P(A)}{P\\left(A^C\\right)} = \\frac{0.005}{0.995} = 0.0005\n\\] But the posterior odds after conditioning on \\(T_+\\) are: \\[\n\\text{odds}\\left(A, A^C \\;| T_+\\right) = \\frac{P(A\\;| T_+)}{P\\left(A^C \\;| T_+\\right)} = \\frac{0.33}{0.67} \\approx 0.493\n\\] So the odds increase by a factor of \\(\\frac{0.493}{0.0005} \\approx 985\\) by conditioning on a positive test result.\n\n\nThere is an odds form of Bayes’ theorem, which states that under the same conditions as the original result, the posterior odds are the prior odds multiplied by a ‘Bayes factor’: \\[\n\\text{odds}\\left(E_i, E_j\\;| A\\right) = \\text{odds}(E_i, E_j) \\cdot \\frac{P(A\\;| E_i)}{P(A\\;| E_j)}\n\\] The result follows from the definition of conditional odds and application of Bayes’ theorem to the conditional probabilities \\(P(E_i\\;| A)\\), and the proof is left as an exercise.\n\n\n\n\n\n\nExample: the Monty Hall problem\n\n\n\nThis is a classic probability problem named after the original host of the game show Let’s Make a Deal. The scenario is this: on a game show, you’re presented with three doors; behind one door is a car, and behind the other two are goats. If you pick the door concealing a car, you win the car (although no one ever explains whether, if you pick otherwise, you win a goat). You choose a door; then the host opens one of the remaining doors, revealing a goat, and asks you if you want to stay with your original guess, or switch. Are you more likely to win the car if you stay, or if you switch?\nWe’ll work out the solution in class."
  },
  {
    "objectID": "archive/f23/syllabus.html",
    "href": "archive/f23/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Course information\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 01/3585] 4:10pm – 5:00pm MTWR Construction Innovations Center Room C201\n[Section 02/3430] 5:10pm – 6:00pm MTWR Construction Innovations Center Room C201\n\nOffice hours: 1:00pm – 3:00pm WR 25-236 and by appointment\nProbability is the mathematics of random events. It is an active field of study and provides the theoretical foundation for the discipline of statistics. This course aims to formalize familiar probability concepts covered in prior coursework (predominantly STAT 305), including probability rules, random variables, common distributions, and expected value. We will build on prior experience and intuition and develop tools to support deeper inquiry and subsequent study of mathematical statistics in STAT 426 and STAT 427. Students will also be introduced to potentially new concepts that play central roles in statistics such as joint distributions, transformations, and conditional expectation. Lectures will provide an exposition of definitions, properties, theorems, and examples, and offer a venue for discussion; students will practice applying this material to solve probability problems on homework assignments and demonstrate fluency with key concepts on in-class exams.\n\n\nCatalog Description\nRigorous development of probability theory. Probability axioms, combinatorial methods, conditional and marginal probability, independence, random variables, univariate and multivariate probability distributions, conditional distributions, transformations, order statistics, expectation and variance. Use of statistical simulation throughout the course. 4 lectures. Prerequisite: MATH 241; MATH 248 or CSC 348; and STAT 305. Recommended: STAT 301.\n\n\nTextbook and references\nThere is no required textbook for the course. However, the overall course organization will closely follow chapters 1 through 4 of DeGroot & Schervish, and exposition of this material will follow Hogg & Craig, which is slightly more formal in style. These books are listed below.\n\n(recommended) DeGroot & Schervish, Probability and Statistics, 4th edition, Addison-Wesley.\n(reference) Hogg, McKean, & Craig, Introduction to Mathematical Statistics, 8th edition, Pearson.\n\nDesk copies of DeGroot & Schervish are available in StatLab (25-107B).\n\n\nLearning outcomes\nThe course aims to enable students to:\n\nuse the axiomatic construction of probability to derive properties of probability measures and conditional probability measures, and apply definitions and properties to solve probability problems \nconstruct probability models for discrete and continuous random variables, develop familiarity with common probability distributions, and use distribution functions to derive properties such as expectations and variances \ndetermine joint distributions for collections of random variables, and use joint distribution functions to (a) derive properties such as covariance and correlation, (b) to determine conditional and marginal distributions, and (c) derive distributions of transformations and functions of one or more random variables \n\n\n\nAssessments\nAttainment of learning outcomes will be measured by evaluation of submitted homework assignments and exams. Letter grades will be assigned based on a weighted average of scores, with the relative weighting approximately as indicated in parentheses below.\n\nHomeworks (50%). Homework assignments will be given approximately weekly (except for exam weeks and holidays). The goal of homework assignments is to provide opportunities for students to reinforce key concepts, definitions, and results through practice and to develop and apply problem-solving abilities. These will be distributed on Thursday each week in which they are assigned and due in class the following Thursday; in general, each assignment will consist of questions pertaining to material discussed in class during the preceding week. Students are encouraged to collaborate on homework assignments, but are expected to prepare submissions individually and indicate in writing on their submission any classmates they collaborated with (see collaboration policy below). Submissions should reflect students’ best effort and will be evaluated based on completeness, organization, and correctness of selected answers. Each student’s lowest homework score will be omitted in grade calculations; otherwise, all homework scores will be weighted equally.\nMidterms (30%). Two midterm exams will be given during the quarter at approximately four-week intervals as shown on the tentative schedule below. The goal of the midterm exams is to assess students’ ability to use key concepts, definitions, and results in furtherance of course learning outcomes. Midterms will be given in class on Thursdays during indicated weeks and consist of 2-3 questions that must be answered individually during the allotted time without consulting anyone except the instructor. Students will be allowed one sheet of notes that they may consult while taking each exam. Answers will be evaluated based on completeness and correctness. While both midterm exams will count towards final grades, whichever exam receives the higher score will be weighted more heavily in grade calculations.\nFinal (20%). A final exam will be given in the usual classroom at the time scheduled by the registrar. The goal of the final exam is to assess students’ integration of the course material in furtherance of course learning oucomes. For Section 01, the final will be held on Wednesday, December 13, 4:10pm — 7:00pm; for Section 02, the final will be held on Friday, December 15, 4:10pm — 7:00pm. The final will consist of 4-6 questions that must be answered individually during the allotted time without consulting anyone except the instructor. Students will be allowed one sheet of notes that they may consult while taking the final. Its scope will be cumulative but emphasis will be on later material. Answers will be evaluated based on completeness and correctness.\n\nEvery effort will be made to return evaluated work within one week of submission. Final exams, per University policy, will be retained by the instructor. Scores will be recorded in Canvas, and students will be responsible for checking their scores to ensure accurate data entry upon receipt of evaluated work.\nVery roughly, letter grades will be assigned as follows. A: 90-100. B: 75-90. C: 60-75. D: 50-60. However, please note that letter grades represent qualitative assessments of attainment of course objectives, and as such these ranges may be adjusted at instructor discretion to ensure apporpriate correspondence with letter grade definitions based on course assessments. Please also note that failure to adhere to course policies — particularly collaboration, academic integrity, and attendance policies — may result in a lower letter grade than would otherwise be assigned.\n\n\nTentative schedule\nSubject to change at instructor discretion.\n\n\n\n\n\n\n\n\n\nWeek\nLecture Topic\nSuggested reading\nAssignments\n\n\n\n\n0 (9/21)\nCourse introduction\n\n\n\n\n1 (9/25)\nProbability axioms and properties\n1.4–1.6\nHW1\n\n\n2 (10/2)\nCounting methods\n1.7–1.10\nHW2\n\n\n3 (10/9)\nConditional probability and independence\n2.1–2.2\nHW3\n\n\n4 (10/16)\nBayes’ theorem\n2.3–2.4\nMidterm 1\n\n\n5 (10/23)\nDiscrete and continuous random variables\n3.1–3.3\nHW4\n\n\n6 (10/30)\nJoint and conditional distributions\n3.4–3.7\nHW5\n\n\n7 (11/6)\nTransformations and order statistics\n3.8–3.9\nHW6\n\n\n8 (11/13)\nExpectation\n4.1–4.2\nMidterm 2\n\n\nFall break (11/20)\n\n\n\n\n\n9 (11/27)\nVariance and covariance\n4.3, 4.6\nHW7\n\n\n10 (12/4)\nConditional expectation\n4.7\n\n\n\nFinals (12/11)\n\n\nFinal\n\n\n\n\n\nCourse policies\n\nCollaboration\nCollaboration within the class (including across class sections) is allowed (and encouraged!) on homework assignments, subject to certain conditions outlined in the paragraph below. Collaborations should not include individuals outside of the class. Students collaborating with a group are expected to prepare their own solutions, in their own words and writing, and should to indicate their collaborators in writing on their submission. This is limited to peers that were consulted closely in completing the assignment; brief or passing interactions are not considered collaborations.\nA collaboration is a shared effort. Students that choose to work together on homework assignments are expected to make material contributions towards producing one or more shared answers or solutions. Material contributions might include participation in discussions, critique of a proposed solution, or presentation of a problem approach. In the absence of such contributions, submitting solutions prepared in a group is not appropriate. The best way to adhere to this policy is to attempt problems individually before consulting others.\n\n\nAttendance\nRegular attendance is essential for success in the course and required per University policy. Each student may miss two class meetings without notice but additional absences should be excusable and students should notify the instructor. Frequent unexcused absences may negatively impact course grades.\n\n\nCommunication and email\nStudents are encouraged to use face-to-face means of communication (office hours, class meetings, appointments) when possible. Email may be used on a secondary basis or when a written record of communication is needed. Every effort is made to respond to email within 48 weekday hours — so a message sent Thursday or Friday may not receive a reply until Monday or Tuesday. Time-sensitive messages should be identified as such in the subject line. Students should wait one week before sending a reminder.\n\n\nTime commitment\nSTAT425 is a four-credit course, which corresponds to a minimum time commitment of 12 hours per week, including lectures, reading, homework assignments, and study time. Some variability in workload by week should be expected, and most students will need to budget a few extra hours each week. However, students can expect to be able to meet course expectations with a time commitment less than 16 hours per week, and are encouraged to notify the instructor if they are regularly exceeding this amount.\n\n\nAssignment scores and final grades\nEvery effort will be made to provide consistent and fair evaluation of student work. Students should notify the instructor of any errors and/or discrepancies in evaluation promptly and on an assignment-by-assignment basis (i.e., not at the end of the quarter) to guarantee consideration for correction. Final grades will only be changed in the case of clerical errors. Attempting to negotiate scores or final grades is not appropriate. Per University policy, faculty have final responsibility for grading criteria and grading judgment and have the right to alter student assessment or other parts of the syllabus during the term. If any student feels their grade is unfairly assigned at the end of the course, they have the right to appeal it according to the procedure outlined here.\n\n\nLate work\nEach student may turn in two homework assignments up to one week late without penalty at any time during the quarter and without notice. Subsequently, assignments turned in up to one week late will be evaluated for 75% credit unless an extension is granted in advance. No late work will be accepted beyond one week after the original due date. This policy applies to homework assignments only, and not exams.\n\n\nAccommodations\nIt is University policy to provide, on a flexible and individualized basis, reasonable accommodations to students who have disabilities that may affect their ability to participate in course activities or to meet course requirements. Accommodation requests should be made through the Disability Resource Center (DRC).\n\n\nConduct and Academic Integrity\nStudents are expected to be aware of and adhere to University policy regarding academic integrity and conduct. Detailed information on these policies, and potential repercussions of policy violations, can be found via the Office of Student Rights & Responsibilities (OSRR).\n\n\nCopyright and distribution of course materials\nAll course materials, including handouts, homework assignments, study guides, course notes, exams, and solutions are subject to copyright; students are not permitted to share or distribute any course materials without the explicit written consent of the instructor. This includes, in particular, uploading materials or prepared solutions to online services and sharing materials or prepared solutions with students who may take the course in a future term. Transgressions of this policy compromise the effectiveness of course assessments and do a disservice to future students."
  },
  {
    "objectID": "archive/f23/exams/midterm2.html",
    "href": "archive/f23/exams/midterm2.html",
    "title": "Midterm 2",
    "section": "",
    "text": "Instructions: read each problem carefully and provide solutions in the space below the prompt or on the reverse side of the page. You should provide as much justification and detail in calculation as needed to clearly display your thought process, but need not show or justify every step. If you use results from class in your solution, you can simply write, ``by a theorem/problem/example from class“. None of the problems require you to perform numerical calculations.If you are unable to obtain a complete solution, a solution sketch may receive partial credit if it reflects a clear understanding of the problem and a well-reasoned approach. Please feel free to ask any clarifying questions about the problems as they arise. Good luck!\n\nLet \\(X\\) have the distribution characterized by the CDF below. Determine whether \\(X\\) is discrete or continuous, and then write the PMF/PDF and compute its expectation \\(\\mathbb{E}X\\).\n\n\nset.seed(113023)\nplot.ecdf(rpois(10, 1), main = 'CDF', ylab = 'F(x)')\n\n\n\n\n\n\n\n\n\nLet \\(X\\) have MGF \\(m_X(t) = \\frac{1}{1 - t^2}, -1&lt;t&lt;1\\), and define \\(Y = \\mu X + \\sigma\\), where \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma &gt; 0\\). Find the MGF of \\(Y\\), and use it to determine its mean \\(\\mathbb{E}Y\\) and variance \\(\\text{var}Y\\).\nLet \\(Y\\) be distributed according to the CDF: \\[F_k(y) = \\begin{cases} 0 &, y \\leq 0 \\\\ \\frac{1}{2}y^{\\frac{1}{k}} &, 0 &lt; y &lt; 2^k \\\\ 1 &, y \\geq 2^k \\end{cases}\\] Find the PDF of \\(Y\\) (be sure to indicate the support set) and use it to calculate the mean and variance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nPDF/PMF\nSupport\nParameters\nMean\nVariance\nMGF\n\n\n\n\nDiscrete uniform\n\\(\\frac{1}{n}\\)\n\\(\\{a_1, \\dots, a_n\\}\\)\nnone\n\\(\\bar{a}\\)\n\\(\\frac{1}{n}\\sum_{i = 1}^n (a_i - \\bar{a})^2\\)\n\\(\\frac{1}{n}\\sum_{i = 1}^n e^{ta_i}\\)\n\n\nBernoulli\n\\(p^x (1 - p)^{1 - x}\\)\n\\(\\{0, 1\\}\\)\n\\(p \\in (0, 1)\\)\n\\(p\\)\n\\(p(1 - p)\\)\n\\(1 - p + pe^t\\)\n\n\nGeometric\n\\((1 - p)^x p\\)\n\\(\\mathbb{N}\\)\n\\(p \\in (0, 1)\\)\n\\(\\frac{1 - p}{p}\\)\n\\(\\frac{1 - p}{p^2}\\)\n\\(\\frac{p}{1 - (1 - p)e^t}\\)\n\\(t &lt; -\\log(1 - p)\\)\n\n\nBinomial\n\\({n \\choose p} p^x (1 - p)^{n - x}\\)\n\\(\\{0, 1, \\dots, n\\}\\)\n\\(n \\in \\mathbb{Z}^+\\)\n\\(p \\in (0, 1)\\)\n\\(np\\)\n\\(np(1 - p)\\)\n\\(\\left(1 - p + pe^t\\right)^n\\)\n\n\nNegative binomial\n\\({r + x - 1 \\choose x} p^r (1 - p)^x\\)\n\\(\\mathbb{N}\\)\n\\(r \\in \\mathbb{Z}^+\\)\n\\(p \\in (0, 1)\\)\n\\(\\frac{r(1 - p)}{p}\\)\n\\(\\frac{r(1 - p)}{p^2}\\)\n\\(\\left(\\frac{p}{1 - (1 - p)e^t}\\right)^r\\)\n\\(t &lt; -\\log(1 - p)\\)\n\n\nPoisson\n\\(\\frac{\\lambda^xe^{-\\lambda}}{x!}\\)\n\\(\\mathbb{N}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\\(\\exp\\left\\{-\\lambda(1 - e^t)\\right\\}\\)\n\n\nContinuous uniform\n\\(\\frac{1}{b - a}\\)\n\\((a, b)\\)\n\\(-\\infty &lt; a &lt; b &lt; \\infty\\)\n\\(\\frac{b + a}{2}\\)\n\\(\\frac{(b - a)^2}{12}\\)\n\\(\\begin{cases}\\frac{e^{ta} - e^{tb}}{t(b - a)} &,\\;t\\neq 0 \\\\ 1 &,\\; t = 0 \\end{cases}\\)\n\n\nGaussian\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2}\\)\n\\(\\mathbb{R}\\)\n\\(\\mu \\in \\mathbb{R}, \\sigma &gt; 0\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\exp\\left\\{\\mu t + \\frac{1}{2}t^2\\sigma^2\\right\\}\\)\n\n\nGamma\n\\(\\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}}\\)\n\\(x &gt; 0\\)\n\\(\\alpha &gt; 0, \\beta &gt; 0\\)\n\\(\\alpha\\beta\\)\n\\(\\alpha\\beta^2\\)\n\\(\\left(1 - \\beta t\\right)^{-\\alpha}\\)"
  },
  {
    "objectID": "archive/f23/exams/midterm2-practice.html",
    "href": "archive/f23/exams/midterm2-practice.html",
    "title": "Midterm 2",
    "section": "",
    "text": "Instructions: read each problem carefully and provide solutions in the space below the prompt or on the reverse side of the page. You should provide as much justification and detail in calculation as needed to clearly display your thought process, but need not show or justify every step. If you use results from class in your solution, you can simply write, ``by a theorem/problem/example from class“. None of the problems require you to perform numerical calculations.If you are unable to obtain a complete solution, a solution sketch may receive partial credit if it reflects a clear understanding of the problem and a well-reasoned approach.Please feel free to ask any clarifying questions about the problems as they arise. Good luck!\n\nLet \\(f(x) = c\\left[1 - (x - 1)^2\\right]\\). Find the value of \\(c\\) and support set for which \\(f\\) is a PDF.\nIntuitively, a log transformation should reduce the skewness of a distribution. The skewness of a random variable is defined as the third standardized moment, i.e., the expectation \\(\\mathbb{E}\\left[\\left(\\frac{X - \\mu}{\\sigma}\\right)^3\\right] = \\frac{\\mu_3 - 3\\mu_1\\sigma^2 - \\mu_1^3}{\\sigma^3}\\), where \\(\\mu_k\\) denotes the \\(k\\)th moment and \\(\\sigma\\) denotes the square root of the variance, i.e., \\(\\sigma = \\sqrt{\\mu_2 - \\mu_1^2}\\). Let \\(\\log(X) \\sim N(0, 1)\\); show that the skewness of \\(X\\) is larger than the skewness of \\(\\log(X)\\).\nLet \\(X \\sim \\text{exponential}(\\beta)\\) and \\(c &gt; 0\\) and define \\[\nY = \\begin{cases}\n   1 &,\\; X &gt; c \\\\\n   0 &,\\; X \\leq c\n\\end{cases}\n\\] Find the distribution of \\(Y\\) and its mean and variance.\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nPDF/PMF\nSupport\nParameters\nMean\nVariance\nMGF\n\n\n\n\nDiscrete uniform\n\\(\\frac{1}{n}\\)\n\\(\\{a_1, \\dots, a_n\\}\\)\nnone\n\\(\\bar{a}\\)\n\\(\\frac{1}{n}\\sum_{i = 1}^n (a_i - \\bar{a})^2\\)\n\\(\\frac{1}{n}\\sum_{i = 1}^n e^{ta_i}\\)\n\n\nBernoulli\n\\(p^x (1 - p)^{1 - x}\\)\n\\(\\{0, 1\\}\\)\n\\(p \\in (0, 1)\\)\n\\(p\\)\n\\(p(1 - p)\\)\n\\(1 - p + pe^t\\)\n\n\nGeometric\n\\((1 - p)^x p\\)\n\\(\\mathbb{N}\\)\n\\(p \\in (0, 1)\\)\n\\(\\frac{1 - p}{p}\\)\n\\(\\frac{1 - p}{p^2}\\)\n\\(\\frac{p}{1 - (1 - p)e^t}\\)\n\\(t &lt; -\\log(1 - p)\\)\n\n\nBinomial\n\\({n \\choose p} p^x (1 - p)^{n - x}\\)\n\\(\\{0, 1, \\dots, n\\}\\)\n\\(n \\in \\mathbb{Z}^+\\)\n\\(p \\in (0, 1)\\)\n\\(np\\)\n\\(np(1 - p)\\)\n\\(\\left(1 - p + pe^t\\right)^n\\)\n\n\nNegative binomial\n\\({r + x - 1 \\choose x} p^r (1 - p)^x\\)\n\\(\\mathbb{N}\\)\n\\(r \\in \\mathbb{Z}^+\\)\n\\(p \\in (0, 1)\\)\n\\(\\frac{r(1 - p)}{p}\\)\n\\(\\frac{r(1 - p)}{p^2}\\)\n\\(\\left(\\frac{p}{1 - (1 - p)e^t}\\right)^r\\)\n\\(t &lt; -\\log(1 - p)\\)\n\n\nPoisson\n\\(\\frac{\\lambda^xe^{-\\lambda}}{x!}\\)\n\\(\\mathbb{N}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\\(\\exp\\left\\{-\\lambda(1 - e^t)\\right\\}\\)\n\n\nContinuous uniform\n\\(\\frac{1}{b - a}\\)\n\\((a, b)\\)\n\\(-\\infty &lt; a &lt; b &lt; \\infty\\)\n\\(\\frac{b + a}{2}\\)\n\\(\\frac{(b - a)^2}{12}\\)\n\\(\\begin{cases}\\frac{e^{ta} - e^{tb}}{t(b - a)} &,\\;t\\neq 0 \\\\ 1 &,\\; t = 0 \\end{cases}\\)\n\n\nGaussian\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2}\\)\n\\(\\mathbb{R}\\)\n\\(\\mu \\in \\mathbb{R}, \\sigma &gt; 0\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\exp\\left\\{\\mu t + \\frac{1}{2}t^2\\sigma^2\\right\\}\\)\n\n\nGamma\n\\(\\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} x^{\\alpha - 1} e^{-\\frac{x}{\\beta}}\\)\n\\(x &gt; 0\\)\n\\(\\alpha &gt; 0, \\beta &gt; 0\\)\n\\(\\alpha\\beta\\)\n\\(\\alpha\\beta^2\\)\n\\(\\left(1 - \\beta t\\right)^{-\\alpha}\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Probability is the mathematics of random events. In the context of modern statistics, probability theory provides a framework for studying the properties of samples, estimators, and inferences (the subjects of STAT 426 and STAT 427). This course will cover the axiomatic formulation of probability and formalize familiar concepts, including basic probability rules, random variables, distributions, and expectations. The course will also introduce concepts that play central roles in statistics, such as joint distributions, transformations, and conditional expectations.\nRead more in the [course syllabus].\nInstructor: Trevor Ruiz (he/him/his) [email] [website]\nLearning assistant: Edy Reynolds [email]\nClass meetings:\nOffice hours (OH) and learning assistant hours (LAH):\nFor office hours, drop ins are welcome (25-236) but availability is not guaranteed without an appointment. For LA hours, availability is drop-in only on a first come, first served basis."
  },
  {
    "objectID": "index.html#week-1-92324",
    "href": "index.html#week-1-92324",
    "title": "Probability Theory",
    "section": "Week 1 (9/23/24)",
    "text": "Week 1 (9/23/24)\nSet theory; probability axioms\nMonday: sets and set operations [notes]\n\n[reading] sections 1.1, 1.2, 1.2.1\n[assignment] exercises 1.2.1, 1.2.2, 1.2.6 [solutions]\n[optional] exercises 1.2.5, 1.2.7\n\nWednesday: probability axioms and properties [notes]\n\n[reading] section 1.3\n[assignment] exercises 1.3.6, 1.3.7, 1.3.21, 1.3.22 [solutions]\n[optional] exercises 1.3.3, 1.3.5"
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Course Syllabus",
    "section": "Tentative schedule",
    "text": "Tentative schedule\nSubject to change at instructor discretion.\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings\nAssignments\n\n\n\n\n1 (9/23)\nSet theory, probability axioms\n1.2, 1.2.1, 1.2.2, 1.3\n\n\n\n2 (9/30)\nCounting methods\n1.3.1, 1.3.2\n\n\n\n3 (10/7)\nConditional probability, independence\n1.4, 1.4.1\nQuiz 1 (M)\n\n\n4 (10/14)\nRandom variables\n1.5, 1.6, 1.7\n\n\n\n5 (10/21)\nExpectation, transformations\n1.8, 1.9, 1.6.1, 1.7.2\nQuiz 2 (M)\n\n\n6 (10/28)\nRandom vectors, joint distributions\n2.1, 2.2, 2.3, 2.4\n\n\n\n7 (11/4)\nRandom vectors, joint distributions\n2.1, 2.2, 2.3, 2.4\nQuiz 3 (M)\n\n\n8 (11/12) Veteran’s day observed 11/11\nMultivariate transformations, order statistics\n2.6, 2.8\n\n\n\n9 (11/18)\nCommon distributions\n3.1, 3.2, 3.3\nQuiz 4 (M)\n\n\nFall break (11/25)\n\n\n\n\n\n10 (12/2)\nThe normal distribution\n3.4, 3.5\nQuiz 5 (M)\n\n\nFinals\n\n\nCommon final Saturday 12/7, time TBD"
  },
  {
    "objectID": "index.html#week-2-93024",
    "href": "index.html#week-2-93024",
    "title": "Probability Theory",
    "section": "Week 2 (9/30/24)",
    "text": "Week 2 (9/30/24)\nDiscrete probability spaces; counting methods\nMonday: counting rules [notes]\n\n[reading] section 1.3.1\n[assignment] exercises 1.3.10, 1.3.18, 1.3.19 \n[optional] exercises 1.3.14, 1.3.15, 1.3.16\n\nWednesday: the matching problem [notes]\n\n[reading] section 1.3.2\n[assignment] exercises 1.3.11, 1.3.14, 1.4.2, 1.4.6\n[optional] exercises 1.4.4, 1.4.5"
  },
  {
    "objectID": "index.html#week-3-10724",
    "href": "index.html#week-3-10724",
    "title": "Probability Theory",
    "section": "Week 3 (10/7/24)",
    "text": "Week 3 (10/7/24)\nMonday: Quiz 1\n\nno reading or assignment\n\nWednesday: conditional probability\n\n[reading] section 1.4"
  }
]
---
title: "Common probability distributions"
author: "STAT425, Fall 2023"
author-title: "Course notes"
date: today
published-title: "Updated"
---

### Distributions based on Bernoulli trials

Several distributions arise from considering a sequence of independent so-called "[Bernoulli trials]{style="color:blue"}": random experiments with a binary outcome. While the coin toss is the prototypical example, a wide range of situations can be described as Bernoulli trials: clinical outcomes, device function, manufacturing defects, and so on. For the purposes of probability calculations, outcomes are encoded as 1, called a 'success', and 0, called a 'failure'.

**Bernoulli distribution**. A random variable $X$ has a Bernoulli distribution with parameter $p$, written $X \sim \text{Bernoulli}(p)$ if it has the PMF: $$
P(X = x) = p^x (1 - p)^{1 - x}
\;,\qquad x = 0, 1
\;,\; p \in (0, 1)
$$ Note that this PMF simply assigns probability $p$ to the outcome $X = 1$ and probability $1 - p$ to the outcome $X = 0$.

**Geometric distribution**. Imagine now a sequence of independent Bernoulli trials with success probability $p$, and define $X$ to be the number of trials before the first success. Then the event $X = k$ is equivalent to observing $k$ failures and 1 success, in just that order. By independence, the associated probability is: $$
P(X = k) = (1 - p)^k p
\;,\qquad k = 0, 1, 2, \dots 
$$ This is a valid PMF. Any random variable with this PMF is said to have a geometric distribution with parameter $p$, written $X \sim \text{geometric}(p)$. The CDF can be written in closed form as $F(x) = 1 - (1 - p)^{\lfloor x \rfloor + 1}$, where $\lfloor x\rfloor$ is the "floor" function: the largest integer smaller than or equal to $x$. For an exercise, verify that the above is a PMF and derive the CDF.

**Binomial distribution**. Let $X$ now record the number of successes in $n$ independent Bernoulli trials. The set of all possible outcomes for $n$ trials is $\{0, 1\}^n$, but since not all outcomes are equally likely unless $p = \frac{1}{2}$, the probability of each outcome $s \in \{0, 1\}^n$ in general can be obtained from independence as: $$
P(s) = p^{\sum_{i = 1}^n s_i} (1 - p)^{n - \sum_{i = 1}^n s_i}
$$ Then, the probability of $k$ successes is the sum of the probabilities of outcomes with $k$ 1's: $$
P(X = k) = \sum_{s \in S: \sum_i s_i = k} P(s) = \sum_{s \in S: \sum_i s_i = k} p^k (1 - p)^{n - k} = {n \choose k} p^k (1 - p)^{n - k} 
\;,\qquad k = 0, 1, 2, \dots, n
$$ There are ${n \choose k}$ terms in the sum since that is the number of ways to allocate the $k$ successes to the $n$ positions. A random variable with this PMF is said to have a binomial distribution with parameters $n, p$, written $X \sim \text{binomial}(n, p)$, or simply $X \sim b(n, p)$. There is no closed form CDF.

::: callout-tip
## Sanity check

The binomial PMF is clearly non-negative for any $k$ in the support set $\{0, 1, \dots, n\}$, so it suffices to check whether the PMF sums to one. For this we use the binomial theorem: $$
\sum_{k = 0}^n P(X = k) = \sum_{k = 0}^n {n \choose k} p^k (1 - p)^{n - k} = (p + 1 - p)^n = 1
$$
:::

**Negative binomial**. If now $X$ records the number of trials until $r$ successes are observed, then, by analogous reasoning in the binomial case, the probability of $n$ trials is the sum over all ways to allocate the successes, except for the last, among the trials, of the probability of $r$ successes and $n - r$ trials: $$
P(X = n) = {n - 1 \choose r - 1}p^r (1 - p)^{n - r}
\;,\qquad
n = r, r + 1, r + 2, \dots
$$

There is an alternate form of the negative binomial that arises from considering the number of failures, akin to the geometric distribution, rather than the number of trials. If $Y$ records the number of failures before $r$ successes, then $Y = X - r$, so with $k = n - r$, the PMF above becomes $$
P(Y = k) = P(X - r = k) = P(X = k + r) = {k + r - 1 \choose k} p^r (1 - p)^k
\;,\qquad
k = 0, 1, 2, \dots
$$ To show that this is a valid PMF, use the second form and write: $$
\begin{align*}
{k + r - 1 \choose k} 
&= \frac{(r + k - 1)(r + k - 2) \cdots (r + 1)(r)}{k!} \\
&= (-1)^k \frac{(-r)(-r - 1)\cdots (-r - k + 2) (-r - k + 1)}{k!} \\
&= (-1)^k {-r \choose k}
\end{align*}
$$ Then, using the limit of the binomial series, one obtains: $$
\sum_{k = 0}^\infty {k + r - 1\choose k} (1 - p)^k = \sum_{k = 0}^\infty {-r \choose k} (p - 1)^k = (1 + p - 1)^{-r} = p^{-r}
$$ From which it follows that: $$
\sum_{k = 0}^\infty P(Y = k) = p^r \sum_{k = 0}^\infty {k + r - 1\choose k} (1 - p)^k = p^r p^{-r} = 1
$$

::: callout-note
## Example: blood types

About 36% of people in the US have blood type A positive. Consider a blood drive in which donors participate independently of blood type and are representative of the general population. If $X$ records whether an arbitrary donor is of blood type $A^+$, a reasonable model is $X \sim \text{Bernoulli}(p = 0.36)$.

The following questions can be answered using distributions based on Bernoulli trials.

i.  What is the probability that the first 5 donors are *not* $A^+$?
ii. What is the probability that more than 5 donors have blood drawn before an $A^+$ donor has blood drawn?
iii. What is the probability that of the first 20 donors, 10 are $A^+$?
iv. What is the probability that it takes 30 donors to obtain 10 $A^+$ samples?

The answers are as follows:

i.  Here, consider $X$ to be the number of donors before the first $A^+$ donor; then $X \sim \text{geometric}(p = 0.36)$. So, $P(X = 5) = (1 - p)^5 p = 0.64^5 \cdot 0.36\approx 0.0387$.

ii. Let $X$ remain as in (i). Then $P(X > 5) = 1 - P(X \leq 5) = (1 - p)^{5 + 1} = 0.64^6 \approx 0.0687$.

iii. Now let $X$ record the number of $A^+$ donors out of the first 20. Then $X \sim b(n = 20, p = 0.36)$, so $P(X = 10) = {20\choose 10} \cdot 0.36^{10} \cdot 0.64^{10} \approx 0.0779$

iv. Now let $X$ record the number of donors until 10 $A^+$ samples are obtained. Then $X \sim nb(r = 10, p = 0.36)$ where the first parametrization is used. Then $P(X = 30) = {30-1 \choose 10-1} \cdot 0.36^{10} \cdot 0.64^{20} \approx 0.0487$.
:::

**Multinomial distribution**. The multinomial generalizes the binomial to trials with $k > 2$ outcomes. If $p_1, \dots, p_k$ denote the probabilities of each of $k$ outcomes for a single trial, and $X_1, \dots, X_k$ count the number of each outcome observed in $n$ independent trials, then: $$
P(X_1 = x_1, \dots, X_k = x_k) = \frac{n!}{x_1! \cdots x_{k - 1}!} p_1^{x_1} \cdots p_{k - 1}^{x_{k - 1}}
\;,\qquad \sum_i x_i = n\;,\; \sum_i p_i = 1
$$ Note that only $k - 1$ terms appear in the above expression since $X_k = n - \sum_{i = 1}^{k - 1} X_i$.

::: callout-tip
## Example: blood types (cont'd)

The percentages of the US population with each of the 8 blood types is given below.

| Type   | Frequency |
|--------|-----------|
| $O^+$  | 34.7%     |
| $O^-$  | 6.6%      |
| $A^+$  | 35.7%     |
| $A^-$  | 6.3%      |
| $B^+$  | 8.5%      |
| $B^-$  | 1.5%      |
| $AB^+$ | 3.4%      |
| $AB^-$ | 0.6%      |

What is the probability of observing 4 $O^+$, 4 $A^+$, 1 $B^+$, and 1 $AB^+$ donors among 10 total donors? This can be found using the multinomial PMF as: $$
\frac{10!}{4!0!4!0!1!0!1!} \cdot 0.347^4 \cdot 0.066^0 \cdot 0.357^4 \cdot 0.063^0 \cdot 0.085^1 \cdot 0.015^0 \cdot 0.034^1 \approx 0.00579
$$
:::

The above PMFs convey the distribution of probabilities across outcomes, but what are "typical" values that one is likely to observe for these random variables? There are several so-called measures of center, including: the [mode]{style="color:blue"} or value with largest mass/density; the [median]{style="color:blue"} or 'middle' value with equal mass/density above and below; and the [mean]{style="color:blue"} or average of values in the support weighted by density/mass.

The mean is known, formally, as the [expected value]{style="color:blue"} or simply the 'expectation' of a random variable, and defined to be: $$
\mathbb{E}X = \begin{cases}
  \sum_{x \in \mathbb{R}} x P(X = x), &X \text{ is discrete} \\
  \int_\mathbb{R} x f(x) dx, &X \text{ is continuous}
\end{cases}
$$ The expectation exists when $X$ is absolutely summable/integrable, *i.e.*, when $\sum_{x \in \mathbb{R}} |x|P(X = x) < \infty$ in the discrete case or $\int_\mathbb{R} |x| f(x) dx < \infty$ in the continuous case.

::: callout-note
## Exercise

Show that:

i.  If $X \sim \text{Bernoulli}(p)$ then $\mathbb{E}X = p$
ii. If $X \sim \text{binomial}(n, p)$ then $\mathbb{E}X = np$
iii. If $X \sim \text{geometric}(p)$ then $\mathbb{E}X = \frac{1 - p}{p}$
:::

**Poisson distribution**. Consider a binomial probability ${n \choose x}p^x (1 - p)^{n - x}$. If the expectation $np$ is held constant at $\lambda$ while $n \rightarrow \infty$, the probability tends to: $$
\lim_{n \rightarrow\infty} {n \choose x}p^x (1 - p)^{n - x} 
= \lim_{n \rightarrow \infty} \frac{n!}{(n - x)!n^x} \cdot \frac{\lambda^x}{x!} \cdot \left(1 - \frac{\lambda}{n}\right)^n \cdot \left(1 - \frac{\lambda}{n}\right)^{-x} 
= \frac{\lambda^x e^{-\lambda}}{x!}
$$ This is a PMF for $x = 0, 1, 2, \dots$ and $\lambda > 0$, since it is obviously nonnegative and: $$
\sum_{x = 0}^\infty \frac{\lambda^x e^{-\lambda}}{x!} = e^{-\lambda} \sum_{x = 0}^\infty \frac{\lambda^x}{x!} = e^{-\lambda}e^\lambda = 1
$$ If a random variable $X$ has this PMF, then $X \sim \text{Poisson}(\lambda)$. Think of this distribution as pertaining to the outcome of infinitely many Bernoulli trials with a finite expected total number of successes. For a Poisson random variable, $\mathbb{E}X = \lambda$: $$
\mathbb{E}X 
= \sum_{x = 0}^\infty x \frac{\lambda^x e^{-\lambda}}{x!}
= \sum_{x = 1}^\infty \frac{\lambda^x e^{-\lambda}}{(x - 1)!}
= \lambda\sum_{x = 1}^\infty \frac{\lambda^{x - 1} e^{-\lambda}}{(x - 1)!}
= \lambda\sum_{x = 0}^\infty \frac{\lambda^{x} e^{-\lambda}}{x!}
= \lambda
$$

::: callout-note
## Exercise: web traffic

The Poisson distribution is often used to model count data. Suppose you're recording the number of visits to your website each day for a year, and the average number of daily visits is 32.7, so you decide to model the random variable $X$, which records the number of daily visits, as Poisson with parameter $\lambda = 32.7$.

i.  Find the following probabilities according to your model: $P(X = 0), P(X \leq 20), P(X > 40), P(X > 100)$.
ii. If you assume visits on each day are independent, how many days would you expect to observe no visits in a year? Under 20 visits? Over 40 visits? Over 100 visits?
iii. If your year of data shows 30 days with over 100 visits, do you think the Poisson is a good model?

We'll work through the solution in class.
:::

---
title: "Conditional probability"
author: "STAT425, Fall 2023"
author-title: "Course notes"
date: today
published-title: "Updated"
---

Let $(S, \mathcal{S}, P)$ be a probability space and let $A \in \mathcal{S}$ be an event with $P(A) > 0$. The [conditional probability]{style="color:blue"} of any event $E\in\mathcal{S}$ [given $A$]{style="color:blue"} is defined as: $$
P(E\;|A) = \frac{P(E\cap A)}{P(A)}
$$

This is interpreted as the chance of $E$ provided that $A$ has occurred. Importantly, $E|A$ is not an event; rather, $P(\cdot\;| A)$ is a new probability measure. To see this, check the axioms:

-   (A1) Since $P$ is a probability measure, $P(E\cap A) \geq 0$, so it follows $P(E\;|A) = \frac{P(E\cap A)}{P(A)} \geq 0$.
-   (A2) $P(S\;|A) = \frac{P(S\cap A)}{P(A)} = \frac{P(A)}{P(A)} = 1$
-   (A3) If $\{E_i\}$ is a disjoint collection, then $\{E_i \cap A\}$ is also a disjoint collection, so by countable additivity of $P$, one has: $$
    P\left(\bigcup_i E_i \;\big|\; A\right) = \frac{P\left(\left[\bigcup_i E_i\right]\cap A\right)}{P(A)} = \frac{P\left(\bigcup_i (E_i\cap A)\right)}{P(A)} = \sum_i \frac{P(E_i\cap A)}{P(A)} = \sum_i P(E_i\;|A)
    $$

One can view $P(\cdot\;|A)$ as a probability measure on $(S, \mathcal{S})$, or as a probability measure on $\left(A, \mathcal{S}^A\right)$ where $\mathcal{S}^A = \{E\cap A: E \in \mathcal{S}\}$. Some prefer the latter view, since it aligns with the interpretation that by conditioning on $A$ one is redefining the sample space.

### Basic properties

An immediate consequence of the definition is that: $$
P(E\cap A) = P(E\;| A) P(A)
$$

In fact, this [multiplication rule for conditional probabilities]{style="color:blue"} can be generalized to an arbitrary finite collection of events: $$
P\left(\bigcap_{i = 1}^n E_i\right) = P(E_1) \times P(E_2\;|E_1) \times P(E_3\;|E_1 \cap E_2) \times\cdots\times P(E_n\;| E_1 \cap \cdots \cap E_{n - 1})
$$

Or, written more compactly: $$
P\left(\bigcap_{i = 1}^n E_i\right) = P(E_1) \times \prod_{i = 2}^n P\left(E_i \;\Bigg| \bigcap_{j = 1}^{i - 1} E_j \right)
$$

::: callout-tip
## Proof

Apply the definition of conditional probability to the terms in the product on the right hand side to see that: $$
\begin{align*}
P(E_1) \times \prod_{i = 2}^n P\left(E_i \;\Bigg| \bigcap_{j = 1}^{i - 1} E_j \right) 
&= P(E_1) \times \prod_{j = 2}^n \left[\frac{P\left(\bigcap_{j = 1}^i E_j\right)}{P\left(\bigcap_{j = 1}^{i - 1} E_j \right)}\right] \\
&= P(E_1) \times \frac{P\left(\bigcap_{j = 1}^2 E_j\right)}{P\left( E_1 \right)} 
  \times \frac{P\left(\bigcap_{j = 1}^3 E_j\right)}{P\left(\bigcap_{j = 1}^{2} E_j \right)}
  \times\cdots
  \times \frac{P\left(\bigcap_{j = 1}^n E_j\right)}{P\left(\bigcap_{j = 1}^{n - 1} E_j \right)}
\end{align*}
$$ Then notice that all terms cancel, leaving only $P\left(\bigcap_{j = 1}^n E_j\right)$, and establishing the result.
:::

The multiplication rule provides a convenient way to compute certain probabilities, as in some problems it's easier to find a conditional probability than an unconditional one.

::: callout-note
## Example: (more) poker hands

Consider drawing 5 cards at random. What's the probability that all 5 are diamonds?

This could be found by computing the total number of ways to draw 5 diamonds out of the total number of ways to draw 5 cards: $$
\frac{{13 \choose 5}}{{52 \choose 5}}
= \frac{13!}{5!8!}\times\frac{5!47!}{52!}
$$ Or, one can avoid evaluating the factorials and instead notice that the conditional probability of drawing a diamond given having already drawn $k$ diamonds is $\frac{13 - k}{52 - k}$ --- the number of diamonds left as a proportion of the number of cards left --- so: $$
P\left(\text{5 diamonds}\right) = \frac{13}{52}\times\frac{12}{51}\times\frac{11}{50}\times\frac{10}{49}\times\frac{9}{48} \approx 0.000495
$$

For a quick exercise, check the result by simplifying the factorials in the first solution.

To see why this is an application of the multiplication rule for conditional probabilities more formally, let $E_i = \{\text{draw a diamond on the $k$th draw}\}$. Then: $$
\begin{align*}
P\left(\text{5 diamonds}\right) 
&= P(E_1 \cap E_2 \cap E_3 \cap E_4 \cap E_5) \\
&= P(E_1)
  \times P(E_2\;| E_1) 
  \times P(E_3\;| E_1 \cap E_2) \\
  &\qquad\times P(E_4 \;| E_1 \cap E_2 \cap E_3)
  \times P(E_5\;| E_1 \cap E_2 \cap E_3 \cap E_4)
\end{align*}
$$
:::

Another useful property is the [law of total probability]{style="color:blue"}: if $\{A_i\}$ is a partition of the sample space $S$, then for any event $E \in \mathcal{S}$ one has: $$
P(E) = \sum_i P(E\;| A_i) P(A_i) 
$$

::: callout-tip
## Proof

Note that $E = E \cap S = E\cap \left[\bigcup_i A_i\right] = \bigcup_i (E \cap A_i)$. Since $\{A_i\}$ is disjoint, so is $\{E \cap A_i\}$. Then by countable additivity: $$
P(E) = P\left[\bigcup_i (E \cap A_i)\right] = \sum_i P(E \cap A_i)
$$ And by the multiplication rule for conditional probabilities $P(E\cap A_i) = P(E\;| A_i) P(A_i)$ so: $$
P(E) = \sum_i P(E \cap A_i) = \sum_i P(E\;| A_i) P(A_i)
$$
:::

::: callout-note
## Example: CVD rates

It's estimated that 8% of men and 0.5% of women have color vision deficiency (CVD). Supposing that exactly these proportions appear in a group of 400 women and 200 men, what's the probability that a randomly selected individual has CVD?

From the problem set-up:

-   $P(\text{CVD}\;| M) = 0.08$ and $P(\text{CVD}\;| F) = 0.005$
-   $P(M) = \frac{1}{3}$ and $P(F) = \frac{2}{3}$

Note that these are the probabilities of selecting a person with the specified attributes from this group, assuming all individuals are equally likely to be selected. This is consistent with how we've defined probabilities for finite sample spaces. The meaning of the probability here comes from sampling from this group of people at random; these are not statements about the chance of having CVD, or being of one sex or the other, or the like.

The law of total probability yields: $$
\begin{align*}
P(\text{CVD}) 
&= P(\text{CVD}\;| M) P(M) + P(\text{CVD}\;| F) P(F) \\
&= 0.08 \cdot \frac{1}{3} + 0.005 \cdot\frac{2}{3} \\
&= 0.03
\end{align*}
$$
:::

### Independence

If two events are independent, then the occurrence of one event doesn't affect the probability of the other. For example, obtaining heads in a coin toss doesn't affect the chances of obtaining a heads in a subsequent toss.

For [independent events]{style="color:blue"}, conditioning on one event does not change the probability of the other: if $E$ and $A$ are independent then $P(E\;|A) = P(E)$. Equivalently:
$$
P(A \cap B) = P(A)P(B)
$$

### Bayes' theorem

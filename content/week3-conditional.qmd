---
title: "Conditional probability"
author: "STAT425, Fall 2023"
author-title: "Course notes"
date: today
published-title: "Updated"
---

Let $(S, \mathcal{S}, P)$ be a probability space and let $A \in \mathcal{S}$ be an event with $P(A) > 0$. The [conditional probability]{style="color:blue"} of any event $E\in\mathcal{S}$ [given $A$]{style="color:blue"} is defined as: $$
P(E\;|A) = \frac{P(E\cap A)}{P(A)}
$$

This is interpreted as the chance of $E$ provided that $A$ has occurred. Importantly, $E|A$ is not an event; rather, $P(\cdot\;| A)$ is a new probability measure. To see this, check the axioms:

-   (A1) Since $P$ is a probability measure, $P(E\cap A) \geq 0$, so it follows $P(E\;|A) = \frac{P(E\cap A)}{P(A)} \geq 0$.
-   (A2) $P(S\;|A) = \frac{P(S\cap A)}{P(A)} = \frac{P(A)}{P(A)} = 1$
-   (A3) If $\{E_i\}$ is a disjoint collection, then $\{E_i \cap A\}$ is also a disjoint collection, so by countable additivity of $P$, one has: $$
    P\left(\bigcup_i E_i \;\big|\; A\right) = \frac{P\left(\left[\bigcup_i E_i\right]\cap A\right)}{P(A)} = \frac{P\left(\bigcup_i (E_i\cap A)\right)}{P(A)} = \sum_i \frac{P(E_i\cap A)}{P(A)} = \sum_i P(E_i\;|A)
    $$

One can view $P(\cdot\;|A)$ as a probability measure on $(S, \mathcal{S})$, or as a probability measure on $\left(A, \mathcal{S}^A\right)$ where $\mathcal{S}^A = \{E\cap A: E \in \mathcal{S}\}$. Some prefer the latter view, since it aligns with the interpretation that by conditioning on $A$ one is redefining the sample space.

### Basic properties

An immediate consequence of the definition is that: $$
P(E\cap A) = P(E\;| A) P(A)
$$

In fact, this [multiplication rule for conditional probabilities]{style="color:blue"} can be generalized to an arbitrary finite collection of events: $$
P\left(\bigcap_{i = 1}^n E_i\right) = P(E_1) \times P(E_2\;|E_1) \times P(E_3\;|E_1 \cap E_2) \times\cdots\times P(E_n\;| E_1 \cap \cdots \cap E_{n - 1})
$$

Or, written more compactly: $$
P\left(\bigcap_{i = 1}^n E_i\right) = P(E_1) \times \prod_{i = 2}^n P\left(E_i \;\Bigg| \bigcap_{j = 1}^{i - 1} E_j \right)
$$

::: callout-tip
## Proof

Apply the definition of conditional probability to the terms in the product on the right hand side to see that: $$
\begin{align*}
P(E_1) \times \prod_{i = 2}^n P\left(E_i \;\Bigg| \bigcap_{j = 1}^{i - 1} E_j \right) 
&= P(E_1) \times \prod_{j = 2}^n \left[\frac{P\left(\bigcap_{j = 1}^i E_j\right)}{P\left(\bigcap_{j = 1}^{i - 1} E_j \right)}\right] \\
&= P(E_1) \times \frac{P\left(\bigcap_{j = 1}^2 E_j\right)}{P\left( E_1 \right)} 
  \times \frac{P\left(\bigcap_{j = 1}^3 E_j\right)}{P\left(\bigcap_{j = 1}^{2} E_j \right)}
  \times\cdots
  \times \frac{P\left(\bigcap_{j = 1}^n E_j\right)}{P\left(\bigcap_{j = 1}^{n - 1} E_j \right)}
\end{align*}
$$ Then notice that all terms cancel, leaving only $P\left(\bigcap_{j = 1}^n E_j\right)$, and establishing the result.
:::

The multiplication rule provides a convenient way to compute certain probabilities, as in some problems it's easier to find a conditional probability than an unconditional one.

::: callout-note
## Example: (more) poker hands

Consider drawing 5 cards at random. What's the probability that all 5 are diamonds?

This could be found by computing the total number of ways to draw 5 diamonds out of the total number of ways to draw 5 cards: $$
\frac{{13 \choose 5}}{{52 \choose 5}}
= \frac{13!}{5!8!}\times\frac{5!47!}{52!}
$$ Or, one can avoid evaluating the factorials and instead notice that the conditional probability of drawing a diamond given having already drawn $k$ diamonds is $\frac{13 - k}{52 - k}$ --- the number of diamonds left as a proportion of the number of cards left --- so: $$
P\left(\text{5 diamonds}\right) = \frac{13}{52}\times\frac{12}{51}\times\frac{11}{50}\times\frac{10}{49}\times\frac{9}{48} \approx 0.000495
$$

For a quick exercise, check the result by simplifying the factorials in the first solution.

To see why this is an application of the multiplication rule for conditional probabilities more formally, let $E_i = \{\text{draw a diamond on the $k$th draw}\}$. Then: $$
\begin{align*}
P\left(\text{5 diamonds}\right) 
&= P(E_1 \cap E_2 \cap E_3 \cap E_4 \cap E_5) \\
&= P(E_1)
  \times P(E_2\;| E_1) 
  \times P(E_3\;| E_1 \cap E_2) \\
  &\qquad\times P(E_4 \;| E_1 \cap E_2 \cap E_3)
  \times P(E_5\;| E_1 \cap E_2 \cap E_3 \cap E_4)
\end{align*}
$$
:::

Another useful property is the [law of total probability]{style="color:blue"}: if $\{A_i\}$ is a partition of the sample space $S$, then for any event $E \in \mathcal{S}$ one has: $$
P(E) = \sum_i P(E\;| A_i) P(A_i) 
$$

::: callout-tip
## Proof

Note that $E = E \cap S = E\cap \left[\bigcup_i A_i\right] = \bigcup_i (E \cap A_i)$. Since $\{A_i\}$ is disjoint, so is $\{E \cap A_i\}$. Then by countable additivity: $$
P(E) = P\left[\bigcup_i (E \cap A_i)\right] = \sum_i P(E \cap A_i)
$$ And by the multiplication rule for conditional probabilities $P(E\cap A_i) = P(E\;| A_i) P(A_i)$ so: $$
P(E) = \sum_i P(E \cap A_i) = \sum_i P(E\;| A_i) P(A_i)
$$
:::

::: callout-note
## Example: CVD rates

It's estimated that 8% of men and 0.5% of women have color vision deficiency (CVD). Supposing that exactly these proportions appear in a group of 400 women and 200 men, what's the probability that a randomly selected individual has CVD?

From the problem set-up:

-   $P(\text{CVD}\;| M) = 0.08$ and $P(\text{CVD}\;| F) = 0.005$
-   $P(M) = \frac{1}{3}$ and $P(F) = \frac{2}{3}$

Note that these are the probabilities of selecting a person with the specified attributes from this group, assuming all individuals are equally likely to be selected. This is consistent with how we've defined probabilities for finite sample spaces. The meaning of the probability here comes from sampling from this group of people at random; these are not statements about the chance of having CVD, or being of one sex or the other, or the like.

The law of total probability yields: $$
\begin{align*}
P(\text{CVD}) 
&= P(\text{CVD}\;| M) P(M) + P(\text{CVD}\;| F) P(F) \\
&= 0.08 \cdot \frac{1}{3} + 0.005 \cdot\frac{2}{3} \\
&= 0.03
\end{align*}
$$
:::

### Independence

If two events are independent, then the occurrence of one event doesn't affect the probability of the other. For example, obtaining heads in a coin toss doesn't affect the chances of obtaining a heads in a subsequent toss.

By contrast, if two events are dependent, then the occurrence of one changes the probability of the other. For example, the likelihood of a car accident is higher in heavy rain.

For [independent events]{style="color:blue"}, conditioning on one event does not change the probability of the other: if $E$ and $A$ are independent then $P(E\;|A) = P(E)$. Equivalently: $$
P(A \cap B) = P(A)P(B)
$$

::: callout-note
## Example: dice rolls

Consider rolling two six-sided dice. The sample space for this experiment is $S = \{1, 2, 3, 4, 5, 6\}^2$, *i.e.*, all ordered pairs of integers between 1 and 6 corresponding to the physical possibilities for the two dice. If all outcomes are equally likely then $P((i, j)) = \frac{1}{|S|} = \frac{1}{36}$ for all $i, j \in \{1, \dots, 6\}$.

In this scenario the value of the first die is independent of the value of the second die. While this is intuitively obvious, to see this probabilistically, let $A_x = \{(i, j) \in S: i = x\}$ denote the event that the first die is an $i$, and let $B_y = \{(i, j)\in S: j = y\}$ denote the event that the second die is a $j$. Now $|A_x| = |B_y| = 6$ for each $x$ and each $y$, so: $$
P(A_x) = P(B_x) = \frac{6}{36} = \frac{1}{6} 
$$ Then note that for any $x, y$, $A_x \cap B_y = \{(x, y)\}$, so: $$
P(A_x \cap B_y) = \frac{1}{36} = \frac{1}{6}\times\frac{1}{6} = P(A_x)P(B_y)
$$

There are many probability measures on this space that are *not* equally likely but for which $A_x, B_y$ remain independent. For example, the table below shows probabilities assigned to each of the 36 possible rolls that are not equally likely, but it is easy to verify that the product of the probabilities in the row/column headers yields the probability in the corresponding cell.

|                        | $P(B_1) = \frac{1}{3}$ | $P(B_2) = \frac{1}{6}$ | $P(B_3) = 0$   | $P(B_4) = \frac{1}{6}$ | $P(B_5) = \frac{1}{6}$ | $P(B_6) = \frac{1}{6}$ |
|------------------------|------------------------|------------------------|----------------|------------------------|------------------------|------------------------|
| $P(A_1) = \frac{1}{6}$ | $\frac{2}{36}$         | $\frac{1}{36}$         | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |
| $P(A_2) = \frac{1}{6}$ | $\frac{2}{36}$         | $\frac{1}{36}$         | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |
| $P(A_3) = \frac{1}{6}$ | $\frac{2}{36}$         | $\frac{1}{36}$         | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |
| $P(A_4) = \frac{1}{6}$ | $\frac{2}{36}$         | $\frac{1}{36}$         | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |
| $P(A_5) = \frac{1}{6}$ | $\frac{2}{36}$         | $\frac{1}{36}$         | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |
| $P(A_6) = \frac{1}{6}$ | $\frac{2}{36}$         | $\frac{1}{36}$         | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |

Now consider the following probability measure. Note it is still a valid probability measure because the entries in the table sum to one.

|                        | $P(B_1) = \frac{11}{36}$ | $P(B_2) = \frac{7}{36}$ | $P(B_3) = 0$   | $P(B_4) = \frac{1}{6}$ | $P(B_5) = \frac{1}{6}$ | $P(B_6) = \frac{1}{6}$ |
|------------------------|--------------------------|-------------------------|----------------|------------------------|------------------------|------------------------|
| $P(A_1) = \frac{1}{6}$ | $\frac{2}{36}$           | $\frac{1}{36}$          | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |
| $P(A_2) = \frac{1}{6}$ | $\frac{1}{36}$           | $\frac{2}{36}$          | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |
| $P(A_3) = \frac{1}{6}$ | $\frac{2}{36}$           | $\frac{1}{36}$          | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |
| $P(A_4) = \frac{1}{6}$ | $\frac{2}{36}$           | $\frac{1}{36}$          | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |
| $P(A_5) = \frac{1}{6}$ | $\frac{2}{36}$           | $\frac{1}{36}$          | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |
| $P(A_6) = \frac{1}{6}$ | $\frac{2}{36}$           | $\frac{1}{36}$          | $\frac{0}{36}$ | $\frac{1}{36}$         | $\frac{1}{36}$         | $\frac{1}{36}$         |

Check your understanding:

1.  Are $A_x, B_y$ independent under this probability measure? How do you check?
2.  Write the a table of the probabilities $P(A_x | B_1)$.
:::

Note that independent events are *not* disjoint unless at least one event has probability zero. For any disjoint events $A, B$, one has $P(A \cap B) = P(\emptyset) = 0$, they cannot be independent unless either $P(A) = 0$ or $P(B) = 0$.

A collection of events $\{E_i\}$ is [pairwise independent]{style="color:blue"} if for every pair of distinct events $E_i, E_j$ one has: $$
P(E_i \cap E_j) = P(E_i)P(E_j)
$$

A collection of events $\{E_i\}$ is [mutually independent]{style="color:blue"} if for every $k$ and every subcollection of $k$ distinct events $E_{i_1}, \dots, E_{i_k}$: $$
P\left(\bigcap_{j = 1}^k E_{i_j}\right) = \prod_{j = 1}^k P(E_{i_j})
$$

::: callout-note
## Example

Consider tossing two coins, so $S = \{HH, HT, TH, TT\}$, and assume all outcomes are equally likely. Define the events:

$$
\begin{align*}
E_1 &= \{HH, HT\} \quad(\text{heads on first toss}) \\
E_2 &= \{HH, TH\} \quad(\text{heads on second toss}) \\
E_3 &= \{HH, TT\} \quad(\text{tosses match})
\end{align*}
$$

Now, $P(E_i) = \frac{1}{2}$ for each $E_i$, since each has two outcomes, so $P(E_i)P(E_j) = \frac{1}{4}$. Moreover, $P(E_i \cap E_j) = P(\{HH\}) = \frac{1}{4}$ for each pair of events $i \neq j$, since $HH$ is the only shared outcome between any two events. However: $$
P(E_1 \cap E_2 \cap E_3) = P(\{HH\}) = \frac{1}{4} \neq \frac{1}{8} = P(E_1)P(E_2)P(E_3)
$$ So the events are pairwise independent, but not mutually independent.
:::

### Bayes' theorem

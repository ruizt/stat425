<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="STAT425, Fall 2023">
<meta name="dcterms.date" content="2023-11-13">

<title>STAT425 - Common probability distributions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H3YKZX4J88"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-H3YKZX4J88', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">STAT425</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html" rel="" target="">
 <span class="menu-text">Course syllabus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../content.html" rel="" target="">
 <span class="menu-text">Materials</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#distributions-based-on-bernoulli-trials" id="toc-distributions-based-on-bernoulli-trials" class="nav-link active" data-scroll-target="#distributions-based-on-bernoulli-trials">Distributions based on Bernoulli trials</a></li>
  <li><a href="#basic-continuous-distributions" id="toc-basic-continuous-distributions" class="nav-link" data-scroll-target="#basic-continuous-distributions">Basic continuous distributions</a></li>
  <li><a href="#the-gaussian-distribution" id="toc-the-gaussian-distribution" class="nav-link" data-scroll-target="#the-gaussian-distribution">The Gaussian distribution</a></li>
  <li><a href="#more-expectations" id="toc-more-expectations" class="nav-link" data-scroll-target="#more-expectations">More expectations</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Common probability distributions</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Course notes</div>
    <div class="quarto-title-meta-contents">
             <p>STAT425, Fall 2023 </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Updated</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 13, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="distributions-based-on-bernoulli-trials" class="level3">
<h3 class="anchored" data-anchor-id="distributions-based-on-bernoulli-trials">Distributions based on Bernoulli trials</h3>
<p>Several distributions arise from considering a sequence of independent so-called “<span style="color:blue">Bernoulli trials</span>”: random experiments with a binary outcome. While the coin toss is the prototypical example, a wide range of situations can be described as Bernoulli trials: clinical outcomes, device function, manufacturing defects, and so on. For the purposes of probability calculations, outcomes are encoded as 1, called a ‘success’, and 0, called a ‘failure’.</p>
<p><strong>Bernoulli distribution</strong>. A random variable <span class="math inline">\(X\)</span> has a Bernoulli distribution with parameter <span class="math inline">\(p\)</span>, written <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span> if it has the PMF: <span class="math display">\[
P(X = x) = p^x (1 - p)^{1 - x}
\;,\qquad x = 0, 1
\;,\; p \in (0, 1)
\]</span> Note that this PMF simply assigns probability <span class="math inline">\(p\)</span> to the outcome <span class="math inline">\(X = 1\)</span> and probability <span class="math inline">\(1 - p\)</span> to the outcome <span class="math inline">\(X = 0\)</span>.</p>
<p><strong>Geometric distribution</strong>. Imagine now a sequence of independent Bernoulli trials with success probability <span class="math inline">\(p\)</span>, and define <span class="math inline">\(X\)</span> to be the number of trials before the first success. Then the event <span class="math inline">\(X = k\)</span> is equivalent to observing <span class="math inline">\(k\)</span> failures and 1 success, in just that order. By independence, the associated probability is: <span class="math display">\[
P(X = k) = (1 - p)^k p
\;,\qquad k = 0, 1, 2, \dots
\]</span> This is a valid PMF. Any random variable with this PMF is said to have a geometric distribution with parameter <span class="math inline">\(p\)</span>, written <span class="math inline">\(X \sim \text{geometric}(p)\)</span>. The CDF can be written in closed form as <span class="math inline">\(F(x) = 1 - (1 - p)^{\lfloor x \rfloor + 1}\)</span>, where <span class="math inline">\(\lfloor x\rfloor\)</span> is the “floor” function: the largest integer smaller than or equal to <span class="math inline">\(x\)</span>. For an exercise, verify that the above is a PMF and derive the CDF.</p>
<p><strong>Binomial distribution</strong>. Let <span class="math inline">\(X\)</span> now record the number of successes in <span class="math inline">\(n\)</span> independent Bernoulli trials. The set of all possible outcomes for <span class="math inline">\(n\)</span> trials is <span class="math inline">\(\{0, 1\}^n\)</span>, but since not all outcomes are equally likely unless <span class="math inline">\(p = \frac{1}{2}\)</span>, the probability of each outcome <span class="math inline">\(s \in \{0, 1\}^n\)</span> in general can be obtained from independence as: <span class="math display">\[
P(s) = p^{\sum_{i = 1}^n s_i} (1 - p)^{n - \sum_{i = 1}^n s_i}
\]</span> Then, the probability of <span class="math inline">\(k\)</span> successes is the sum of the probabilities of outcomes with <span class="math inline">\(k\)</span> 1’s: <span class="math display">\[
P(X = k) = \sum_{s \in S: \sum_i s_i = k} P(s) = \sum_{s \in S: \sum_i s_i = k} p^k (1 - p)^{n - k} = {n \choose k} p^k (1 - p)^{n - k}
\;,\qquad k = 0, 1, 2, \dots, n
\]</span> There are <span class="math inline">\({n \choose k}\)</span> terms in the sum since that is the number of ways to allocate the <span class="math inline">\(k\)</span> successes to the <span class="math inline">\(n\)</span> positions. A random variable with this PMF is said to have a binomial distribution with parameters <span class="math inline">\(n, p\)</span>, written <span class="math inline">\(X \sim \text{binomial}(n, p)\)</span>, or simply <span class="math inline">\(X \sim b(n, p)\)</span>. There is no closed form CDF.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Sanity check
</div>
</div>
<div class="callout-body-container callout-body">
<p>The binomial PMF is clearly non-negative for any <span class="math inline">\(k\)</span> in the support set <span class="math inline">\(\{0, 1, \dots, n\}\)</span>, so it suffices to check whether the PMF sums to one. For this we use the binomial theorem: <span class="math display">\[
\sum_{k = 0}^n P(X = k) = \sum_{k = 0}^n {n \choose k} p^k (1 - p)^{n - k} = (p + 1 - p)^n = 1
\]</span></p>
</div>
</div>
<p><strong>Negative binomial</strong>. If now <span class="math inline">\(X\)</span> records the number of trials until <span class="math inline">\(r\)</span> successes are observed, then, by analogous reasoning in the binomial case, the probability of <span class="math inline">\(n\)</span> trials is the sum over all ways to allocate the successes, except for the last, among the trials, of the probability of <span class="math inline">\(r\)</span> successes and <span class="math inline">\(n - r\)</span> trials: <span class="math display">\[
P(X = n) = {n - 1 \choose r - 1}p^r (1 - p)^{n - r}
\;,\qquad
n = r, r + 1, r + 2, \dots
\]</span></p>
<p>There is an alternate form of the negative binomial that arises from considering the number of failures, akin to the geometric distribution, rather than the number of trials. If <span class="math inline">\(Y\)</span> records the number of failures before <span class="math inline">\(r\)</span> successes, then <span class="math inline">\(Y = X - r\)</span>, so with <span class="math inline">\(k = n - r\)</span>, the PMF above becomes <span class="math display">\[
P(Y = k) = P(X - r = k) = P(X = k + r) = {k + r - 1 \choose k} p^r (1 - p)^k
\;,\qquad
k = 0, 1, 2, \dots
\]</span> To show that this is a valid PMF, use the second form and write: <span class="math display">\[
\begin{align*}
{k + r - 1 \choose k}
&amp;= \frac{(r + k - 1)(r + k - 2) \cdots (r + 1)(r)}{k!} \\
&amp;= (-1)^k \frac{(-r)(-r - 1)\cdots (-r - k + 2) (-r - k + 1)}{k!} \\
&amp;= (-1)^k {-r \choose k}
\end{align*}
\]</span> Technically, <span class="math inline">\({-r\choose k}\)</span> is a generalized binomial coefficient. While the above should make this notation plausible, we won’t give a full treatment here; however, binomial series with generalized coefficients are convergent under certain conditions, and the limit exhibits the same form as the more familiar binomial theorem. We’ll simply apply the result. Using the limit of the binomial series, one obtains: <span class="math display">\[
\sum_{k = 0}^\infty {k + r - 1\choose k} (1 - p)^k = \sum_{k = 0}^\infty {-r \choose k} (p - 1)^k = (1 + p - 1)^{-r} = p^{-r}
\]</span> From which it follows that: <span class="math display">\[
\sum_{k = 0}^\infty P(Y = k) = p^r \sum_{k = 0}^\infty {k + r - 1\choose k} (1 - p)^k = p^r p^{-r} = 1
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: blood types
</div>
</div>
<div class="callout-body-container callout-body">
<p>About 36% of people in the US have blood type A positive. Consider a blood drive in which donors participate independently of blood type and are representative of the general population. If <span class="math inline">\(X\)</span> records whether an arbitrary donor is of blood type <span class="math inline">\(A^+\)</span>, a reasonable model is <span class="math inline">\(X \sim \text{Bernoulli}(p = 0.36)\)</span>.</p>
<p>The following questions can be answered using distributions based on Bernoulli trials.</p>
<ol type="i">
<li>What is the probability that the first 5 donors are <em>not</em> <span class="math inline">\(A^+\)</span>?</li>
<li>What is the probability that more than 5 donors have blood drawn before an <span class="math inline">\(A^+\)</span> donor has blood drawn?</li>
<li>What is the probability that of the first 20 donors, 10 are <span class="math inline">\(A^+\)</span>?</li>
<li>What is the probability that it takes 30 donors to obtain 10 <span class="math inline">\(A^+\)</span> samples?</li>
</ol>
<p>The answers are as follows:</p>
<ol type="i">
<li><p>Here, consider <span class="math inline">\(X\)</span> to be the number of donors before the first <span class="math inline">\(A^+\)</span> donor; then <span class="math inline">\(X \sim \text{geometric}(p = 0.36)\)</span>. So, <span class="math inline">\(P(X = 5) = (1 - p)^5 p = 0.64^5 \cdot 0.36\approx 0.0387\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X\)</span> remain as in (i). Then <span class="math inline">\(P(X &gt; 5) = 1 - P(X \leq 5) = (1 - p)^{5 + 1} = 0.64^6 \approx 0.0687\)</span>.</p></li>
<li><p>Now let <span class="math inline">\(X\)</span> record the number of <span class="math inline">\(A^+\)</span> donors out of the first 20. Then <span class="math inline">\(X \sim b(n = 20, p = 0.36)\)</span>, so <span class="math inline">\(P(X = 10) = {20\choose 10} \cdot 0.36^{10} \cdot 0.64^{10} \approx 0.0779\)</span></p></li>
<li><p>Now let <span class="math inline">\(X\)</span> record the number of donors until 10 <span class="math inline">\(A^+\)</span> samples are obtained. Then <span class="math inline">\(X \sim nb(r = 10, p = 0.36)\)</span> where the first parametrization is used. Then <span class="math inline">\(P(X = 30) = {30-1 \choose 10-1} \cdot 0.36^{10} \cdot 0.64^{20} \approx 0.0487\)</span>.</p></li>
</ol>
</div>
</div>
<p><strong>Multinomial distribution</strong>. The multinomial generalizes the binomial to trials with <span class="math inline">\(k &gt; 2\)</span> outcomes. If <span class="math inline">\(p_1, \dots, p_k\)</span> denote the probabilities of each of <span class="math inline">\(k\)</span> outcomes for a single trial, and <span class="math inline">\(X_1, \dots, X_k\)</span> count the number of each outcome observed in <span class="math inline">\(n\)</span> independent trials, then: <span class="math display">\[
P(X_1 = x_1, \dots, X_k = x_k) = \frac{n!}{x_1! \cdots x_{k - 1}!} p_1^{x_1} \cdots p_{k - 1}^{x_{k - 1}}
\;,\qquad \sum_i x_i = n\;,\; \sum_i p_i = 1
\]</span> Note that only <span class="math inline">\(k - 1\)</span> terms appear in the above expression since <span class="math inline">\(X_k = n - \sum_{i = 1}^{k - 1} X_i\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: blood types (cont’d)
</div>
</div>
<div class="callout-body-container callout-body">
<p>The percentages of the US population with each of the 8 blood types is given below.</p>
<table class="table">
<thead>
<tr class="header">
<th>Type</th>
<th>Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(O^+\)</span></td>
<td>34.7%</td>
</tr>
<tr class="even">
<td><span class="math inline">\(O^-\)</span></td>
<td>6.6%</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A^+\)</span></td>
<td>35.7%</td>
</tr>
<tr class="even">
<td><span class="math inline">\(A^-\)</span></td>
<td>6.3%</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(B^+\)</span></td>
<td>8.5%</td>
</tr>
<tr class="even">
<td><span class="math inline">\(B^-\)</span></td>
<td>1.5%</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(AB^+\)</span></td>
<td>3.4%</td>
</tr>
<tr class="even">
<td><span class="math inline">\(AB^-\)</span></td>
<td>0.6%</td>
</tr>
</tbody>
</table>
<p>What is the probability of observing 4 <span class="math inline">\(O^+\)</span>, 4 <span class="math inline">\(A^+\)</span>, 1 <span class="math inline">\(B^+\)</span>, and 1 <span class="math inline">\(AB^+\)</span> donors among 10 total donors? This can be found using the multinomial PMF as: <span class="math display">\[
\frac{10!}{4!0!4!0!1!0!1!} \cdot 0.347^4 \cdot 0.066^0 \cdot 0.357^4 \cdot 0.063^0 \cdot 0.085^1 \cdot 0.015^0 \cdot 0.034^1 \approx 0.00579
\]</span></p>
</div>
</div>
<p>The above PMFs convey the distribution of probabilities across outcomes, but what are “typical” values that one is likely to observe for these random variables? There are several so-called measures of center, including: the <span style="color:blue">mode</span> or value with largest mass/density; the <span style="color:blue">median</span> or ‘middle’ value with equal mass/density above and below; and the <span style="color:blue">mean</span> or average of values in the support weighted by density/mass.</p>
<p>The mean is known, formally, as the <span style="color:blue">expected value</span> or simply the ‘expectation’ of a random variable, and defined to be: <span class="math display">\[
\mathbb{E}X = \begin{cases}
  \sum_{x \in \mathbb{R}} x P(X = x), &amp;X \text{ is discrete} \\
  \int_\mathbb{R} x f(x) dx, &amp;X \text{ is continuous}
\end{cases}
\]</span> The expectation exists when <span class="math inline">\(X\)</span> is absolutely summable/integrable, <em>i.e.</em>, when <span class="math inline">\(\sum_{x \in \mathbb{R}} |x|P(X = x) &lt; \infty\)</span> in the discrete case or <span class="math inline">\(\int_\mathbb{R} |x| f(x) dx &lt; \infty\)</span> in the continuous case.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise: expectations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Show that:</p>
<ol type="i">
<li>If <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span> then <span class="math inline">\(\mathbb{E}X = p\)</span></li>
<li>If <span class="math inline">\(X \sim \text{binomial}(n, p)\)</span> then <span class="math inline">\(\mathbb{E}X = np\)</span></li>
<li>If <span class="math inline">\(X \sim \text{geometric}(p)\)</span> then <span class="math inline">\(\mathbb{E}X = \frac{1 - p}{p}\)</span></li>
</ol>
</div>
</div>
<p><strong>Poisson distribution</strong>. Consider a binomial probability <span class="math inline">\({n \choose x}p^x (1 - p)^{n - x}\)</span>. If the expectation <span class="math inline">\(np\)</span> is held constant at <span class="math inline">\(\lambda\)</span> while <span class="math inline">\(n \rightarrow \infty\)</span>, the probability tends to: <span class="math display">\[
\lim_{n \rightarrow\infty} {n \choose x}p^x (1 - p)^{n - x}
= \lim_{n \rightarrow \infty} \frac{n!}{(n - x)!n^x} \cdot \frac{\lambda^x}{x!} \cdot \left(1 - \frac{\lambda}{n}\right)^n \cdot \left(1 - \frac{\lambda}{n}\right)^{-x}
= \frac{\lambda^x e^{-\lambda}}{x!}
\]</span> This is a PMF for <span class="math inline">\(x = 0, 1, 2, \dots\)</span> and <span class="math inline">\(\lambda &gt; 0\)</span>, since it is obviously nonnegative and: <span class="math display">\[
\sum_{x = 0}^\infty \frac{\lambda^x e^{-\lambda}}{x!} = e^{-\lambda} \sum_{x = 0}^\infty \frac{\lambda^x}{x!} = e^{-\lambda}e^\lambda = 1
\]</span> If a random variable <span class="math inline">\(X\)</span> has this PMF, then <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>. Think of this distribution as pertaining to the outcome of infinitely many Bernoulli trials with a finite expected total number of successes. For a Poisson random variable, <span class="math inline">\(\mathbb{E}X = \lambda\)</span>: <span class="math display">\[
\mathbb{E}X
= \sum_{x = 0}^\infty x \frac{\lambda^x e^{-\lambda}}{x!}
= \sum_{x = 1}^\infty \frac{\lambda^x e^{-\lambda}}{(x - 1)!}
= \lambda\sum_{x = 1}^\infty \frac{\lambda^{x - 1} e^{-\lambda}}{(x - 1)!}
= \lambda\sum_{x = 0}^\infty \frac{\lambda^{x} e^{-\lambda}}{x!}
= \lambda
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise: web traffic
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Poisson distribution is often used to model count data. Suppose you’re recording the number of visits to your website each day for a year, and the average number of daily visits is 32.7, so you decide to model the random variable <span class="math inline">\(X\)</span>, which records the number of daily visits, as Poisson with parameter <span class="math inline">\(\lambda = 32.7\)</span>.</p>
<ol type="i">
<li>Find the following probabilities according to your model: <span class="math inline">\(P(X = 0), P(X \leq 20), P(X &gt; 40), P(X &gt; 100)\)</span>.</li>
<li>If you assume visits on each day are independent, how many days would you expect to observe no visits in a year? Under 20 visits? Over 40 visits? Over 100 visits?</li>
<li>If your year of data shows 30 days with over 100 visits, do you think the Poisson is a good model?</li>
</ol>
<p>We’ll work through the solution in class.</p>
</div>
</div>
</section>
<section id="basic-continuous-distributions" class="level3">
<h3 class="anchored" data-anchor-id="basic-continuous-distributions">Basic continuous distributions</h3>
<p>Here we’ll look at a few elementary continuous distributions.</p>
<p><strong>Uniform distribution</strong>. The continuous uniform distribution corresponds to drawing a real number at random from an interval <span class="math inline">\((a, b)\)</span>: the density is constant on the specified interval and zero elsewhere. <span class="math inline">\(X \sim \text{uniform}(a, b)\)</span> if <span class="math inline">\(X\)</span> has the PDF: <span class="math display">\[
f(x) = \frac{1}{b - a}
\;,\qquad x \in (a, b)
\;,\; -\infty &lt; a &lt; b &lt; \infty
\]</span> Including one or both endpoints in the interval does not change the probabilities of any events; even though this will produce different densities, the CDF will be the same. The CDF in either case is: <span class="math display">\[
F(x) = \int_{-\infty}^x \frac{1}{b - a}dz = \begin{cases}
0 &amp;,\; x \leq a \\
\frac{x - a}{b - a} &amp;,\; x \in (a, b) \\
1 &amp;,\; x \geq b
\end{cases}
\]</span></p>
<p>Another way of looking at the matter of endpoints is that since <span class="math inline">\(P(X = x) = 0\)</span> for every <span class="math inline">\(x \in \mathbb{R}\)</span>, countably many points may be ‘removed’ without fundamentally alterning the probability distribution. At any rate, for some problems, it makes more sense contextually to specify a uniform distribution on a closed interval, so you may occasionally see <span class="math inline">\(X \sim \text{uniform}[a, b]\)</span>, though this is less common.</p>
<p>The expectation of a uniform random variable is the midpoint of the interval: <span class="math display">\[
\int_\mathbb{R} x f(x) dx = \frac{1}{b - a}\int_a^b x dx = \frac{1}{b-a}\left[\frac{x^2}{2}\right]_a^b = \frac{b^2 - a^2}{2(b - a)} = \frac{(b + a)(b - a)}{2(b - a)} = \frac{b + a}{2}
\]</span></p>
<p><strong>Exponential distribution</strong>. The exponential distribution is given by the PDF: <span class="math display">\[
f(x) = \alpha e^{-\alpha x}
\;,\qquad
x &gt; 0
\;,\;
\alpha &gt; 0
\]</span></p>
<p>The CDF is <span class="math inline">\(F(x) = 1 - e^{-\alpha x}\)</span>, and the mean is: <span class="math display">\[
\begin{align*}
\mathbb{E}X
&amp;= \int_\mathbb{R} x f(x) dx \\
&amp;= \int_0^\infty x \alpha e^{-\alpha x}dx \\
&amp;= \alpha \int_0^\infty \left(-\frac{d}{d\alpha} e^{-\alpha x}\right)dx \\
&amp;= \alpha \frac{d}{d\alpha}\left[\frac{e^{-\alpha x}}{\alpha} \right]_0^\infty \\
&amp;= \alpha \frac{d}{d\alpha}\left(0 - \frac{1}{\alpha}\right) \\
&amp;= \alpha \cdot \frac{1}{\alpha^2} \\
&amp;= \frac{1}{\alpha}
\end{align*}
\]</span></p>
<p>The exponential distribution is often used to model waiting times and failure times.</p>
</section>
<section id="the-gaussian-distribution" class="level3">
<h3 class="anchored" data-anchor-id="the-gaussian-distribution">The Gaussian distribution</h3>
<p>The Gaussian or normal distribution is of central importance in statistical inference, and arises in relation to averages. Here we’ll develop the density in the standard case (no parameters) and then introduce the center and scale parameters through a simple linear transformation. We’ll start with an important calculus result.</p>
<p><strong>Theorem (Gaussian integral)</strong>. <span class="math inline">\(\int_\mathbb{R} e^{-z^2}dz = \sqrt\pi\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(I\)</span> denote the integral of interest. Then: <span class="math display">\[
I^2 = \left(\int_\mathbb{R} e^{-z^2}dz\right)\left(\int_\mathbb{R} e^{-w^2}dw\right) = \int_\mathbb{R}\int_\mathbb{R} e^{-(z^2 + w^2)}dzdw
\]</span> Now converting to polar coordinates — that is, applying the transformation <span class="math inline">\(z = r\cos\theta\)</span> and <span class="math inline">\(w = r\sin\theta\)</span> and using the result from calculus that <span class="math inline">\(dzdw = rdrd\theta\)</span> — one obtains: <span class="math display">\[
I^2 = \int_0^{2\pi}\int_0^\infty e^{-r^2}rdrd\theta = \int_0^{2\pi} \left(\left[-\frac{1}{2}e^{-r^2}\right]^\infty_0\right)d\theta = \int_0^{2\pi} \frac{1}{2}d\theta = \pi
\]</span> Taking square roots yields <span class="math inline">\(I = \sqrt\pi\)</span> as required.</p>
</div>
</div>
<p><strong>Gaussian distribution</strong>. The standard Gaussian or normal distribution is given by the PDF <span class="math inline">\(\varphi (x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\)</span> for <span class="math inline">\(x \in \mathbb{R}\)</span>. There is no closed form for the CDF, but we write the CDF as <span class="math inline">\(\Phi(x) = \int_{-\infty}^x \varphi(z)dz\)</span>. If a random variable <span class="math inline">\(Z\)</span> has this PDF/CDF, write <span class="math inline">\(Z \sim N(0, 1)\)</span>.</p>
<p>As a remark, special notation is used for the PDF and CDF of the standard normal/Gaussian because it is used so frequently. <span class="math inline">\(\varphi\)</span> and <span class="math inline">\(\Phi\)</span> are typical. <span class="math inline">\(\varphi\)</span> is a valid PDF since it is evidently non-negative and: <span class="math display">\[
\begin{align*}
\int_\mathbb{R} \varphi(x)dx
&amp;= \frac{1}{\sqrt{2\pi}} \int_\mathbb{R} e^{-\frac{x^2}{2}}dx \\
&amp;= \frac{1}{\sqrt{2\pi}} \int_\mathbb{R} e^{-z^2}\sqrt{2}dz \qquad \left(z^2 = \frac{x^2}{2} \Rightarrow dz = \frac{dx}{\sqrt{2}}\right) \\
&amp;= \sqrt{\frac{2}{2\pi}} \int_\mathbb{R} e^{-z^2}dz \\
&amp;= \sqrt{\frac{2}{2\pi}} \cdot\sqrt{\pi} \\
&amp;= 1
\end{align*}
\]</span></p>
<p>If <span class="math inline">\(Z \sim N(0, 1)\)</span> then the expectation of <span class="math inline">\(Z\)</span> is: <span class="math display">\[
\begin{align*}
\mathbb{E}Z &amp;= \int_\mathbb{R} x\varphi(x)dx \\
&amp;= \int_{-\infty}^0 x\varphi(x)dx + \int_0^\infty x\varphi(x)dx \\
&amp;= -\int_{-\infty}^0 -x\varphi(-x)dx + \int_0^\infty x\varphi(x)dx \\
&amp;= -\int_0^\infty x\varphi(x)dx + \int_0^\infty x\varphi(x)dx \\
&amp;= 0
\end{align*}
\]</span></p>
<p>This argument leverages the fact, immediate from the functional form of <span class="math inline">\(\varphi\)</span>, that <span class="math inline">\(\varphi(x) = \varphi(-x)\)</span>, <em>i.e.</em>, <span class="math inline">\(\varphi\)</span> is an <em>even</em> function.</p>
<p>Now let <span class="math inline">\(Z \sim N(0, 1)\)</span> and define <span class="math inline">\(X = \sigma Z + \mu\)</span> for arbitrary numbers <span class="math inline">\(\sigma &gt; 0\)</span> and <span class="math inline">\(\mu \in \mathbb{R}\)</span>. <span class="math inline">\(\sigma\)</span> is referred to as a scale parameter and <span class="math inline">\(\mu\)</span> is referred to as a location parameter. The CDF of <span class="math inline">\(X\)</span> is: <span class="math display">\[
F_X (x) = P(X \leq x) = P(\sigma Z + \mu \leq x) = P\left(Z \leq \frac{x - \mu}{\sigma}\right) = \Phi\left(\frac{x - \mu}{\sigma}\right)
\]</span> It follows that the PDF is: <span class="math display">\[
f_X (x) = \frac{d}{dx} \Phi\left(\frac{x - \mu}{\sigma}\right) = \frac{1}{\sigma}\varphi\left(\frac{x - \mu}{\sigma}\right) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}\left(x - \mu\right)^2\right\}
\]</span> We write <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> to indicate that <span class="math inline">\(X\)</span> has a Gaussian distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. Furthermore, <span class="math inline">\(X\)</span> has expectation: <span class="math display">\[
\begin{align*}
\mathbb{E}X
&amp;= \int_\mathbb{R} x\cdot\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}\left(x - \mu\right)^2\right\}dx \\
&amp;= \int_\mathbb{R} (\sigma z + \mu)\cdot\frac{1}{\sqrt{2\pi}}\exp\left\{-\frac{1}{2}z^2\right\}dz \qquad \left(z = \frac{x - \mu}{\sigma} \Rightarrow dz = \frac{1}{\sigma}dx\right) \\
&amp;= \sigma\int_\mathbb{R} z \cdot\frac{1}{\sqrt{2\pi}}\exp\left\{-\frac{1}{2}z^2\right\}dz + \mu\int_\mathbb{R} \frac{1}{\sqrt{2\pi}}\exp\left\{-\frac{1}{2}z^2\right\}dz \\
&amp;= \sigma\mathbb{E}Z + \mu \int_\mathbb{R}\varphi(z)dz \\
&amp;= \mu
\end{align*}
\]</span></p>
<p>Notice that <span class="math inline">\(\mathbb{E}X = \mathbb{E}\left(\sigma Z + \mu\right) = \sigma\mathbb{E}Z + \mu\)</span>.</p>
</section>
<section id="more-expectations" class="level3">
<h3 class="anchored" data-anchor-id="more-expectations">More expectations</h3>
<p><strong>Theorem</strong>. If <span class="math inline">\(X\)</span> is a random variable and <span class="math inline">\(g\)</span> is a function, then the expectation of <span class="math inline">\(Y = g(X)\)</span>, provided it exists, is: <span class="math display">\[
\mathbb{E}X = \begin{cases}
  \sum_{x \in \mathbb{R}} g(x) P(X = x) &amp;,\;X \text{ is discrete}\\
  \int_\mathbb{R} g(x)f(x)dx &amp;,\;X \text{ is continuous}\\
\end{cases}
\]</span></p>
<p>The proof requires some advanced techniques, so we’ll skip it. Hogg &amp; Craig provide a sketch in the discrete case and otherwise defer to references. Some texts simply state this as a definition.</p>
<p><strong>Corollaries</strong>. If <span class="math inline">\(X\)</span> is a random variable, <span class="math inline">\(a, b, c\)</span> are constants, and <span class="math inline">\(g_1, g_2\)</span> are functions whose expectations exist, then the following statements are true.</p>
<ol type="i">
<li><span class="math inline">\(\mathbb{E}\left(ag_1(X) + bg_2(X) + c\right) = a\mathbb{E}g_1(X) + b\mathbb{E}g_2(X) + c\)</span>.</li>
<li>If <span class="math inline">\(g_1(x) \geq 0\)</span> for all <span class="math inline">\(x\)</span> in the support of <span class="math inline">\(X\)</span>, then <span class="math inline">\(\mathbb{E}g_1(X) \geq 0\)</span>.</li>
<li>If <span class="math inline">\(g_1(x) \geq g_2(x)\)</span> for all <span class="math inline">\(x\)</span> in the support of <span class="math inline">\(X\)</span>, then <span class="math inline">\(\mathbb{E}g_1(X) \geq \mathbb{E}g_2(X)\)</span>.</li>
<li>If <span class="math inline">\(a \leq g_1(x) \leq b\)</span> then for all <span class="math inline">\(x\)</span> in the support of <span class="math inline">\(X\)</span>, <span class="math inline">\(a \leq \mathbb{E}g_1(X) \leq b\)</span>.</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
</div>
<div class="callout-body-container callout-body">
<p>Notice that (ii) and (iv) are special cases of (iii), so it suffices to prove (i) and (iv). Essentially, these properties follow directly from properties of integration/summation.</p>
<p>For (i), in the discrete case:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left(ag_1(X) + bg_2(X) + c\right)
&amp;= \sum_x \left(ag_1(x) + bg_2(x) + c\right) P(X = x) \\
&amp;= a\sum_x g_1(x)P(X = x) + b\sum_x g_2(x)P(X = x) + c\sum_x P(X = x) \\
&amp;= a\mathbb{E}g_1(X) + b\mathbb{E}g_2(X) + c
\end{align*}
\]</span></p>
<p>In the continuous case, the argument is essentially the same:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left(ag_1(X) + bg_2(X) + c\right)
&amp;= \int_\mathbb{R} \left(ag_1(x) + bg_2(x) + c\right) f(x)dx \\
&amp;= a\int_\mathbb{R} g_1(x)f(x)dx + b\int_\mathbb{R} g_2(x)f(x)dx + c\int_\mathbb{R}f(x)dx \\
&amp;= a\mathbb{E}g_1(X) + b\mathbb{E}g_2(X) + c
\end{align*}
\]</span></p>
<p>For (iii), let <span class="math inline">\(g_2(x) \leq g_1(x)\)</span> for every <span class="math inline">\(x \in \mathbb{R}\)</span>. Then, in the discrete case:</p>
<p><span class="math display">\[
\mathbb{E}g_2(X) = \sum_x g_2(x) P(X = x) \leq \sum_x g_1(x) P(X = x) = \mathbb{E}g_1(X)
\]</span></p>
<p>In the continuous case:</p>
<p><span class="math display">\[
\mathbb{E}g_2(X) = \int_\mathbb{R} g_2(x) f(x) dx \leq \int_\mathbb{R} g_1(x) f(x) dx = \mathbb{E}g_1(X)
\]</span></p>
<p>To obtain (ii), set <span class="math inline">\(g_2(x) \equiv 0\)</span>. Then if <span class="math inline">\(0 \leq g_1(x)\)</span> for every <span class="math inline">\(x\in\mathbb{R}\)</span>, one has <span class="math inline">\(g_2 (x) \leq g_1(x)\)</span> and so <span class="math inline">\(\mathbb{E}g_2(X) \leq \mathbb{E}g_1(X)\)</span>. But <span class="math inline">\(\mathbb{E}g_2(X) = 0\)</span>, so <span class="math inline">\(0 \leq \mathbb{E}g_1(X)\)</span>.</p>
<p>To obtain (iv), set <span class="math inline">\(g_2(x) \equiv a\)</span> and consider a function <span class="math inline">\(g_3(x) \equiv b\)</span>. Then the expectations exist and <span class="math inline">\(\mathbb{E}g_2(X) = a\)</span> and <span class="math inline">\(\mathbb{E}g_3(X) = b\)</span>. apply (iii) to obtain that if <span class="math inline">\(g_2(x) \leq g_1(x) \leq g_3(x)\)</span> for all <span class="math inline">\(x \in \mathbb{R}\)</span>, the expectations are similarly ordered and thus <span class="math inline">\(a \leq \mathbb{E}g_1(X) \leq b\)</span>.</p>
</div>
</div>
<p>These corollaries can often ease calculations. For example, it is immediate that for any random variable whose expectation exists, <span class="math inline">\(\mathbb{E}X^2 \geq 0\)</span>. Similarly, scaling and shifting a random variable scales and shifts the mean: <span class="math inline">\(\mathbb{E}(aX + b) = a\mathbb{E}X + b\)</span>.</p>
<p>The <span style="color:blue">variance</span> of a random variable is defined as the expectation: <span class="math display">\[
\text{var}X = \mathbb{E}\left(X - \mathbb{E}X\right)^2
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Bernoulli variance
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span> then, noting that from above <span class="math inline">\(\mathbb{E}X = p\)</span>, the variance of <span class="math inline">\(X\)</span> is: <span class="math display">\[
\begin{align*}
\text{var}X
&amp;= \mathbb{E}(X - \mathbb{E}X)^2 \\
&amp;= \mathbb{E}(X - p)^2 \\
&amp;= (1 - p)^2 P(X = 1) + (0 - p)^2 P(X = 0) \\
&amp;= (1 - p)^2 p + p^2 (1 - p) \\
&amp;= (1 - p)\left((1 - p)p + p^2\right) \\
&amp;= (1 - p)\left(p - p^2 + p^2\right) \\
&amp;= (1 - p)p
\end{align*}
\]</span></p>
</div>
</div>
<p>While one could calculate the variance directly, as in the example above, this is often a cumbersome calculation in more complex cases. Instead, by the corollary, and noting that <span class="math inline">\(\mathbb{E}g(X)\)</span> is a constant, one can obtain the <span style="color:blue">variance formula</span> as: <span class="math display">\[
\text{var}X = \mathbb{E}\left(X^2 - 2X\mathbb{E}X + \mathbb{E}X\mathbb{E}X\right) = \mathbb{E}X^2 - 2\mathbb{E}X\mathbb{E}X + \mathbb{E}X\mathbb{E}X = \mathbb{E}X^2 - \left(\mathbb{E}X\right)^2
\]</span> Thus, to calculate the variance of a random variable, one simply needs to know <span class="math inline">\(\mathbb{E}X\)</span> and <span class="math inline">\(\mathbb{E}X^2\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Poisson variance
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>, then from before <span class="math inline">\(\mathbb{E}X = \lambda\)</span>, and: <span class="math display">\[
\begin{align*}
\mathbb{E}X^2
&amp;= \sum_{x = 0}^\infty x^2 \frac{\lambda^x e^{-\lambda}}{x!} \\
&amp;= \lambda \sum_{x = 0}^\infty x \frac{\lambda^{x-1} e^{-\lambda}}{(x - 1)!} \\
&amp;= \lambda \sum_{x = 0}^\infty (x + 1) \frac{\lambda^{x} e^{-\lambda}}{x!} \\
&amp;= \lambda \left[\sum_{x = 0}^\infty x \frac{\lambda^{x} e^{-\lambda}}{x!} + 1 \cdot \frac{\lambda^{x} e^{-\lambda}}{x!} \right] \\
&amp;= \lambda \left[\sum_{x = 0}^\infty x \frac{\lambda^{x} e^{-\lambda}}{x!} + \sum_{x = 0}^\infty 1 \cdot \frac{\lambda^{x} e^{-\lambda}}{x!} \right] \\
&amp;= \lambda(\lambda + 1)
\end{align*}
\]</span></p>
<p>Then, by the variance formula: <span class="math display">\[
\text{var}X = \mathbb{E}X^2 - \left(\mathbb{E}X\right)^2 = \lambda(\lambda + 1) - \lambda^2 = \lambda^2 + \lambda - \lambda^2 = \lambda
\]</span></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">© 2023 Trevor Ruiz</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>
[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Course information\nInstructor: Trevor Ruiz (he/him/his) [email]\nClass meetings:\n\n[Section 01/3585] 4:10pm – 5:00pm MTWR Construction Innovations Center Room C201\n[Section 02/3430] 5:10pm – 6:00pm MTWR Construction Innovations Center Room C201\n\nOffice hours: 1:00pm – 3:00pm TR 25-236 and by appointment\nProbability is the mathematics of random events. It is an active field of study and provides the theoretical foundation for the discipline of statistics. This course aims to formalize familiar probability concepts covered in prior coursework (predominantly STAT 305), including probability rules, random variables, common distributions, and expected value. We will build on prior experience and intuition and develop tools to support deeper inquiry and subsequent study of mathematical statistics in STAT 426 and STAT 427. Students will also be introduced to potentially new concepts that play central roles in statistics such as joint distributions, transformations, and conditional expectation. Lectures will provide an exposition of definitions, properties, theorems, and examples, and offer a venue for discussion; students will practice applying this material to solve probability problems on homework assignments and demonstrate fluency with key concepts on in-class exams.\n\n\nCatalog Description\nRigorous development of probability theory. Probability axioms, combinatorial methods, conditional and marginal probability, independence, random variables, univariate and multivariate probability distributions, conditional distributions, transformations, order statistics, expectation and variance. Use of statistical simulation throughout the course. 4 lectures. Prerequisite: MATH 241; MATH 248 or CSC 348; and STAT 305. Recommended: STAT 301.\n\n\nTextbook and references\nThere is no required textbook for the course. However, the overall course organization will closely follow chapters 1 through 4 of DeGroot & Schervish, and exposition of this material will follow Hogg & Craig, which is slightly more formal in style. These books are listed below.\n\n(recommended) DeGroot & Schervish, Probability and Statistics, 4th edition, Addison-Wesley.\n(reference) Hogg, McKean, & Craig, Introduction to Mathematical Statistics, 8th edition, Pearson.\n\nDesk copies of DeGroot & Schervish are available in StatLab (25-107B).\n\n\nLearning outcomes\nThe course aims to enable students to:\n\nuse the axiomatic construction of probability to derive properties of probability measures and conditional probability measures, and apply definitions and properties to solve probability problems \nconstruct probability models for discrete and continuous random variables, develop familiarity with common probability distributions, and use distribution functions to derive properties such as expectations and variances \ndetermine joint distributions for collections of random variables, and use joint distribution functions to (a) derive properties such as covariance and correlation, (b) to determine conditional and marginal distributions, and (c) derive distributions of transformations and functions of one or more random variables \n\n\n\nAssessments\nAttainment of learning outcomes will be measured by evaluation of submitted homework assignments and exams. Letter grades will be assigned based on a weighted average of scores, with the relative weighting approximately as indicated in parentheses below.\n\nHomeworks (50%). Homework assignments will be given approximately weekly (except for exam weeks and holidays). The goal of homework assignments is to provide opportunities for students to reinforce key concepts, definitions, and results through practice and to develop and apply problem-solving abilities. These will be distributed on Thursday each week in which they are assigned and due in class the following Thursday; in general, each assignment will consist of questions pertaining to material discussed in class during the preceding week. Students are encouraged to collaborate on homework assignments, but are expected to prepare submissions individually and indicate in writing on their submission any classmates they collaborated with (see collaboration policy below). Submissions should reflect students’ best effort and will be evaluated based on completeness, organization, and correctness of selected answers. Each student’s lowest homework score will be omitted in grade calculations; otherwise, all homework scores will be weighted equally.\nMidterms (30%). Two midterm exams will be given during the quarter at approximately four-week intervals as shown on the tentative schedule below. The goal of the midterm exams is to assess students’ ability to use key concepts, definitions, and results in furtherance of course learning outcomes. Midterms will be given in class on Thursdays during indicated weeks and consist of 2-3 questions that must be answered individually during the allotted time without consulting anyone except the instructor. Students will be allowed one sheet of notes that they may consult while taking each exam. Answers will be evaluated based on completeness and correctness. While both midterm exams will count towards final grades, whichever exam receives the higher score will be weighted more heavily in grade calculations.\nFinal (20%). A final exam will be given in the usual classroom at the time scheduled by the registrar. The goal of the final exam is to assess students’ integration of the course material in furtherance of course learning oucomes. For Section 01, the final will be held on Wednesday, December 13, 4:10pm — 7:00pm; for Section 02, the final will be held on Friday, December 15, 4:10pm — 7:00pm. The final will consist of 4-6 questions that must be answered individually during the allotted time without consulting anyone except the instructor. Students will be allowed one sheet of notes that they may consult while taking the final. Its scope will be cumulative but emphasis will be on later material. Answers will be evaluated based on completeness and correctness.\n\nEvery effort will be made to return evaluated work within one week of submission. Final exams, per University policy, will be retained by the instructor. Scores will be recorded in Canvas, and students will be responsible for checking their scores to ensure accurate data entry upon receipt of evaluated work.\nVery roughly, letter grades will be assigned as follows. A: 90-100. B: 75-90. C: 60-75. D: 50-60. However, please note that letter grades represent qualitative assessments of attainment of course objectives, and as such these ranges may be adjusted at instructor discretion to ensure apporpriate correspondence with letter grade definitions based on course assessments. Please also note that failure to adhere to course policies — particularly collaboration, academic integrity, and attendance policies — may result in a lower letter grade than would otherwise be assigned.\n\n\nTentative schedule\nSubject to change at instructor discretion.\n\n\n\n\n\n\n\n\n\nWeek\nLecture Topic\nSuggested reading\nAssignments\n\n\n\n\n0 (9/21)\nCourse introduction\n\n\n\n\n1 (9/25)\nProbability axioms and properties\n1.4–1.6\nHW1\n\n\n2 (10/2)\nCounting methods\n1.7–1.10\nHW2\n\n\n3 (10/9)\nConditional probability and independence\n2.1–2.2\nHW3\n\n\n4 (10/16)\nBayes’ theorem\n2.3–2.4\nMidterm 1\n\n\n5 (10/23)\nDiscrete and continuous random variables\n3.1–3.3\nHW4\n\n\n6 (10/30)\nJoint and conditional distributions\n3.4–3.7\nHW5\n\n\n7 (11/6)\nTransformations and order statistics\n3.8–3.9\nHW6\n\n\n8 (11/13)\nExpectation\n4.1–4.2\nMidterm 2\n\n\nFall break (11/20)\n\n\n\n\n\n9 (11/27)\nVariance and covariance\n4.3, 4.6\nHW7\n\n\n10 (12/4)\nConditional expectation\n4.7\n\n\n\nFinals (12/11)\n\n\nFinal\n\n\n\n\n\nCourse policies\n\nCollaboration\nCollaboration within the class (including across class sections) is allowed (and encouraged!) on homework assignments, subject to certain conditions outlined in the paragraph below. Collaborations should not include individuals outside of the class. Students collaborating with a group are expected to prepare their own solutions, in their own words and writing, and should to indicate their collaborators in writing on their submission. This is limited to peers that were consulted closely in completing the assignment; brief or passing interactions are not considered collaborations.\nA collaboration is a shared effort. Students that choose to work together on homework assignments are expected to make material contributions towards producing one or more shared answers or solutions. Material contributions might include participation in discussions, critique of a proposed solution, or presentation of a problem approach. In the absence of such contributions, submitting solutions prepared in a group is not appropriate. The best way to adhere to this policy is to attempt problems individually before consulting others.\n\n\nAttendance\nRegular attendance is essential for success in the course and required per University policy. Each student may miss two class meetings without notice but additional absences should be excusable and students should notify the instructor. Frequent unexcused absences may negatively impact course grades.\n\n\nCommunication and email\nStudents are encouraged to use face-to-face means of communication (office hours, class meetings, appointments) when possible. Email may be used on a secondary basis or when a written record of communication is needed. Every effort is made to respond to email within 48 weekday hours — so a message sent Thursday or Friday may not receive a reply until Monday or Tuesday. Time-sensitive messages should be identified as such in the subject line. Students should wait one week before sending a reminder.\n\n\nTime commitment\nSTAT425 is a four-credit course, which corresponds to a minimum time commitment of 12 hours per week, including lectures, reading, homework assignments, and study time. Some variability in workload by week should be expected, and most students will need to budget a few extra hours each week. However, students can expect to be able to meet course expectations with a time commitment less than 16 hours per week, and are encouraged to notify the instructor if they are regularly exceeding this amount.\n\n\nAssignment scores and final grades\nEvery effort will be made to provide consistent and fair evaluation of student work. Students should notify the instructor of any errors and/or discrepancies in evaluation promptly and on an assignment-by-assignment basis (i.e., not at the end of the quarter) to guarantee consideration for correction. Final grades will only be changed in the case of clerical errors. Attempting to negotiate scores or final grades is not appropriate. Per University policy, faculty have final responsibility for grading criteria and grading judgment and have the right to alter student assessment or other parts of the syllabus during the term. If any student feels their grade is unfairly assigned at the end of the course, they have the right to appeal it according to the procedure outlined here.\n\n\nLate work\nEach student may turn in two homework assignments up to one week late without penalty at any time during the quarter and without notice. Subsequently, assignments turned in up to one week late will be evaluated for 75% credit unless an extension is granted in advance. No late work will be accepted beyond one week after the original due date. This policy applies to homework assignments only, and not exams.\n\n\nAccommodations\nIt is University policy to provide, on a flexible and individualized basis, reasonable accommodations to students who have disabilities that may affect their ability to participate in course activities or to meet course requirements. Accommodation requests should be made through the Disability Resource Center (DRC).\n\n\nConduct and Academic Integrity\nStudents are expected to be aware of and adhere to University policy regarding academic integrity and conduct. Detailed information on these policies, and potential repercussions of policy violations, can be found via the Office of Student Rights & Responsibilities (OSRR).\n\n\nCopyright and distribution of course materials\nAll course materials, including handouts, homework assignments, study guides, course notes, exams, and solutions are subject to copyright; students are not permitted to share or distribute any course materials without the explicit written consent of the instructor. This includes, in particular, uploading materials or prepared solutions to online services and sharing materials or prepared solutions with students who may take the course in a future term. Transgressions of this policy compromise the effectiveness of course assessments and do a disservice to future students."
  },
  {
    "objectID": "content/week3-conditional.html",
    "href": "content/week3-conditional.html",
    "title": "Conditional probability",
    "section": "",
    "text": "Let \\((S, \\mathcal{S}, P)\\) be a probability space and let \\(A \\in \\mathcal{S}\\) be an event with \\(P(A) &gt; 0\\). The conditional probability of any event \\(E\\in\\mathcal{S}\\) given \\(A\\) is defined as: \\[\nP(E\\;|A) = \\frac{P(E\\cap A)}{P(A)}\n\\]\nThis is interpreted as the chance of \\(E\\) provided that \\(A\\) has occurred. Importantly, \\(E|A\\) is not an event; rather, \\(P(\\cdot\\;| A)\\) is a new probability measure. To see this, check the axioms:\n\n(A1) Since \\(P\\) is a probability measure, \\(P(E\\cap A) \\geq 0\\), so it follows \\(P(E\\;|A) = \\frac{P(E\\cap A)}{P(A)} \\geq 0\\).\n(A2) \\(P(S\\;|A) = \\frac{P(S\\cap A)}{P(A)} = \\frac{P(A)}{P(A)} = 1\\)\n(A3) If \\(\\{E_i\\}\\) is a disjoint collection, then \\(\\{E_i \\cap A\\}\\) is also a disjoint collection, so by countable additivity of \\(P\\), one has: \\[\nP\\left(\\bigcup_i E_i \\;\\big|\\; A\\right) = \\frac{P\\left(\\left[\\bigcup_i E_i\\right]\\cap A\\right)}{P(A)} = \\frac{P\\left(\\bigcup_i (E_i\\cap A)\\right)}{P(A)} = \\sum_i \\frac{P(E_i\\cap A)}{P(A)} = \\sum_i P(E_i\\;|A)\n\\]\n\nOne can view \\(P(\\cdot\\;|A)\\) as a probability measure on \\((S, \\mathcal{S})\\), or as a probability measure on \\(\\left(A, \\mathcal{S}^A\\right)\\) where \\(\\mathcal{S}^A = \\{E\\cap A: E \\in \\mathcal{S}\\}\\). Some prefer the latter view, since it aligns with the interpretation that by conditioning on \\(A\\) one is redefining the sample space.\n\nBasic properties\nAn immediate consequence of the definition is that: \\[\nP(E\\cap A) = P(E\\;| A) P(A)\n\\]\nIn fact, this multiplication rule for conditional probabilities can be generalized to an arbitrary finite collection of events: \\[\nP\\left(\\bigcap_{i = 1}^n E_i\\right) = P(E_1) \\times P(E_2\\;|E_1) \\times P(E_3\\;|E_1 \\cap E_2) \\times\\cdots\\times P(E_n\\;| E_1 \\cap \\cdots \\cap E_{n - 1})\n\\]\nOr, written more compactly: \\[\nP\\left(\\bigcap_{i = 1}^n E_i\\right) = P(E_1) \\times \\prod_{i = 2}^n P\\left(E_i \\;\\Bigg| \\bigcap_{j = 1}^{i - 1} E_j \\right)\n\\]\n\n\n\n\n\n\nProof\n\n\n\nApply the definition of conditional probability to the terms in the product on the right hand side to see that: \\[\n\\begin{align*}\nP(E_1) \\times \\prod_{i = 2}^n P\\left(E_i \\;\\Bigg| \\bigcap_{j = 1}^{i - 1} E_j \\right)\n&= P(E_1) \\times \\prod_{j = 2}^n \\left[\\frac{P\\left(\\bigcap_{j = 1}^i E_j\\right)}{P\\left(\\bigcap_{j = 1}^{i - 1} E_j \\right)}\\right] \\\\\n&= P(E_1) \\times \\frac{P\\left(\\bigcap_{j = 1}^2 E_j\\right)}{P\\left( E_1 \\right)}\n  \\times \\frac{P\\left(\\bigcap_{j = 1}^3 E_j\\right)}{P\\left(\\bigcap_{j = 1}^{2} E_j \\right)}\n  \\times\\cdots\n  \\times \\frac{P\\left(\\bigcap_{j = 1}^n E_j\\right)}{P\\left(\\bigcap_{j = 1}^{n - 1} E_j \\right)}\n\\end{align*}\n\\] Then notice that all terms cancel, leaving only \\(P\\left(\\bigcap_{j = 1}^n E_j\\right)\\), and establishing the result.\n\n\nThe multiplication rule provides a convenient way to compute certain probabilities, as in some problems it’s easier to find a conditional probability than an unconditional one.\n\n\n\n\n\n\nExample: (more) poker hands\n\n\n\nConsider drawing 5 cards at random. What’s the probability that all 5 are diamonds?\nThis could be found by computing the total number of ways to draw 5 diamonds out of the total number of ways to draw 5 cards: \\[\n\\frac{{13 \\choose 5}}{{52 \\choose 5}}\n= \\frac{13!}{5!8!}\\times\\frac{5!47!}{52!}\n\\] Or, one can avoid evaluating the factorials and instead notice that the conditional probability of drawing a diamond given having already drawn \\(k\\) diamonds is \\(\\frac{13 - k}{52 - k}\\) — the number of diamonds left as a proportion of the number of cards left — so: \\[\nP\\left(\\text{5 diamonds}\\right) = \\frac{13}{52}\\times\\frac{12}{51}\\times\\frac{11}{50}\\times\\frac{10}{49}\\times\\frac{9}{48} \\approx 0.000495\n\\]\nFor a quick exercise, check the result by simplifying the factorials in the first solution.\nTo see why this is an application of the multiplication rule for conditional probabilities more formally, let \\(E_i = \\{\\text{draw a diamond on the $k$th draw}\\}\\). Then: \\[\n\\begin{align*}\nP\\left(\\text{5 diamonds}\\right)\n&= P(E_1 \\cap E_2 \\cap E_3 \\cap E_4 \\cap E_5) \\\\\n&= P(E_1)\n  \\times P(E_2\\;| E_1)\n  \\times P(E_3\\;| E_1 \\cap E_2) \\\\\n  &\\qquad\\times P(E_4 \\;| E_1 \\cap E_2 \\cap E_3)\n  \\times P(E_5\\;| E_1 \\cap E_2 \\cap E_3 \\cap E_4)\n\\end{align*}\n\\]\n\n\nAnother useful property is the law of total probability: if \\(\\{A_i\\}\\) is a partition of the sample space \\(S\\), then for any event \\(E \\in \\mathcal{S}\\) one has: \\[\nP(E) = \\sum_i P(E\\;| A_i) P(A_i)\n\\]\n\n\n\n\n\n\nProof\n\n\n\nNote that \\(E = E \\cap S = E\\cap \\left[\\bigcup_i A_i\\right] = \\bigcup_i (E \\cap A_i)\\). Since \\(\\{A_i\\}\\) is disjoint, so is \\(\\{E \\cap A_i\\}\\). Then by countable additivity: \\[\nP(E) = P\\left[\\bigcup_i (E \\cap A_i)\\right] = \\sum_i P(E \\cap A_i)\n\\] And by the multiplication rule for conditional probabilities \\(P(E\\cap A_i) = P(E\\;| A_i) P(A_i)\\) so: \\[\nP(E) = \\sum_i P(E \\cap A_i) = \\sum_i P(E\\;| A_i) P(A_i)\n\\]\n\n\n\n\n\n\n\n\nExample: CVD rates\n\n\n\nIt’s estimated that 8% of men and 0.5% of women have color vision deficiency (CVD). Supposing that exactly these proportions appear in a group of 400 women and 200 men, what’s the probability that a randomly selected individual has CVD?\nFrom the problem set-up:\n\n\\(P(\\text{CVD}\\;| M) = 0.08\\) and \\(P(\\text{CVD}\\;| F) = 0.005\\)\n\\(P(M) = \\frac{1}{3}\\) and \\(P(F) = \\frac{2}{3}\\)\n\nNote that these are the probabilities of selecting a person with the specified attributes from this group, assuming all individuals are equally likely to be selected. This is consistent with how we’ve defined probabilities for finite sample spaces. In this case, the sample space is the collection of 600 individuals, and each individual is assigned selection probability \\(\\frac{1}{600}\\). Thus, the meaning of the probability here comes from sampling from this group of people at random; importantly, these are not statements about the chance of having CVD, or being of one sex or the other, or the like.\nThe law of total probability yields: \\[\n\\begin{align*}\nP(\\text{CVD})\n&= P(\\text{CVD}\\;| M) P(M) + P(\\text{CVD}\\;| F) P(F) \\\\\n&= 0.08 \\cdot \\frac{1}{3} + 0.005 \\cdot\\frac{2}{3} \\\\\n&= 0.03\n\\end{align*}\n\\]\n\n\n\n\nIndependence\nIf two events are independent, then the occurrence of one event doesn’t affect the probability of the other. For example, obtaining heads in a coin toss doesn’t affect the chances of obtaining a heads in a subsequent toss.\nBy contrast, if two events are dependent, then the occurrence of one changes the probability of the other. For example, the likelihood of a car accident is higher in heavy rain.\nFor independent events, conditioning on one event does not change the probability of the other. Thus, we say that any two events \\(E\\) and \\(A\\) are independent just in case: \\[\nP(E \\cap A) = P(E)P(A)\n\\]\nIf so, we write \\(E\\perp A\\).\nWhile it might be more intuitive to define independence according to the criterion \\(P(E\\;| A) = P(E)\\), recall that \\(P(E\\;| A)\\) is undefined if \\(P(A) = 0\\). By using instead the (almost equivalent) criterion \\(P(E \\cap A) = P(E)P(A)\\), the concept is still well-defined for events with probability zero.\nNote two facts:\n\nindependent events are not disjoint unless at least one event has probability zero\nevents with probability zero are independent of all other events, including themselves.\n\n\n\n\n\n\n\nProof\n\n\n\nFor the first fact, observe that for any disjoint events \\(A, B\\), one has \\(P(A \\cap B) = P(\\emptyset) = 0\\), so \\(P(A \\cap B) = P(A)P(B)\\) cannot hold unless either \\(P(A) = 0\\) or \\(P(B) = 0\\).\nFor the second fact, if \\(P(A) = 0\\), then for any event \\(B\\), by monotonicity of the probability measure \\(B \\cap A \\subseteq A\\) implies \\(P(B\\cap A) \\leq P(A) = 0\\), so \\(P(B \\cap A) = 0\\). Therefore \\(P(B\\cap A) = P(A)P(B)\\).\n\n\n\n\n\n\n\n\nExample: dice rolls\n\n\n\nConsider rolling two six-sided dice. The sample space for this experiment is \\(S = \\{1, 2, 3, 4, 5, 6\\}^2\\), i.e., all ordered pairs of integers between 1 and 6 corresponding to the physical possibilities for the two dice. If all outcomes are equally likely then \\(P((i, j)) = \\frac{1}{|S|} = \\frac{1}{36}\\) for all \\(i, j \\in \\{1, \\dots, 6\\}\\).\nIn this scenario the value of the first die is independent of the value of the second die. While this is intuitively obvious, to see this probabilistically, let \\(A_x = \\{(i, j) \\in S: i = x\\}\\) denote the event that the first die is an \\(i\\), and let \\(B_y = \\{(i, j)\\in S: j = y\\}\\) denote the event that the second die is a \\(j\\). Now \\(|A_x| = |B_y| = 6\\) for each \\(x\\) and each \\(y\\), so: \\[\nP(A_x) = P(B_x) = \\frac{6}{36} = \\frac{1}{6}\n\\] Then note that for any \\(x, y\\), \\(A_x \\cap B_y = \\{(x, y)\\}\\), so: \\[\nP(A_x \\cap B_y) = \\frac{1}{36} = \\frac{1}{6}\\times\\frac{1}{6} = P(A_x)P(B_y)\n\\]\nThere are many probability measures on this space that are not equally likely but for which \\(A_x, B_y\\) remain independent. For example, the table below shows probabilities assigned to each of the 36 possible rolls that are not equally likely, but it is easy to verify that the product of the probabilities in the row/column headers yields the probability in the corresponding cell.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P(B_1) = \\frac{1}{3}\\)\n\\(P(B_2) = \\frac{1}{6}\\)\n\\(P(B_3) = 0\\)\n\\(P(B_4) = \\frac{1}{6}\\)\n\\(P(B_5) = \\frac{1}{6}\\)\n\\(P(B_6) = \\frac{1}{6}\\)\n\n\n\n\n\\(P(A_1) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_2) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_3) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_4) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_5) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_6) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\nNow consider the following probability measure. Note it is still a valid probability measure because the entries in the table sum to one.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P(B_1) = \\frac{11}{36}\\)\n\\(P(B_2) = \\frac{7}{36}\\)\n\\(P(B_3) = 0\\)\n\\(P(B_4) = \\frac{1}{6}\\)\n\\(P(B_5) = \\frac{1}{6}\\)\n\\(P(B_6) = \\frac{1}{6}\\)\n\n\n\n\n\\(P(A_1) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_2) = \\frac{1}{6}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_3) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_4) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_5) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\\(P(A_6) = \\frac{1}{6}\\)\n\\(\\frac{2}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{0}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\\(\\frac{1}{36}\\)\n\n\n\nCheck your understanding:\n\nAre \\(A_x, B_y\\) independent under this probability measure? How do you check?\nWrite the a table of the probabilities \\(P(A_x | B_1)\\).\n\n\n\nA collection of events \\(\\{E_i\\}\\) is pairwise independent if for every pair of distinct events \\(E_i, E_j\\) one has: \\[\nP(E_i \\cap E_j) = P(E_i)P(E_j)\n\\]\nA collection of events \\(\\{E_i\\}\\) is mutually independent if for every \\(k\\) and every subcollection of \\(k\\) distinct events \\(E_{i_1}, \\dots, E_{i_k}\\): \\[\nP\\left(\\bigcap_{j = 1}^k E_{i_j}\\right) = \\prod_{j = 1}^k P(E_{i_j})\n\\]\n\n\n\n\n\n\nExample\n\n\n\nConsider tossing two coins, so \\(S = \\{HH, HT, TH, TT\\}\\), and assume all outcomes are equally likely. Define the events:\n\\[\n\\begin{align*}\nE_1 &= \\{HH, HT\\} \\quad(\\text{heads on first toss}) \\\\\nE_2 &= \\{HH, TH\\} \\quad(\\text{heads on second toss}) \\\\\nE_3 &= \\{HH, TT\\} \\quad(\\text{tosses match})\n\\end{align*}\n\\]\nNow, \\(P(E_i) = \\frac{1}{2}\\) for each \\(E_i\\), since each has two outcomes, so \\(P(E_i)P(E_j) = \\frac{1}{4}\\). Moreover, \\(P(E_i \\cap E_j) = P(\\{HH\\}) = \\frac{1}{4}\\) for each pair of events \\(i \\neq j\\), since \\(HH\\) is the only shared outcome between any two events. However: \\[\nP(E_1 \\cap E_2 \\cap E_3) = P(\\{HH\\}) = \\frac{1}{4} \\neq \\frac{1}{8} = P(E_1)P(E_2)P(E_3)\n\\] So the events are pairwise independent, but not mutually independent.\n\n\n\n\nBayes’ theorem\nIn many circumstances one may be able to compute \\(P(A\\;| B)\\) but wish to know \\(P(B\\;| A)\\). For example, you might have a good estimate for a diagnostic test of the probability that the test is positive given that a condition, illness, or pathogen is present, but what would really be much more useful to patients is the probability that one has the condition, illness, or pathogen given that they obtained a positive test result. The following theorem provides a means of accomplishing this.\nBayes’ theorem. If \\(\\{E_i\\}\\) is a partition of \\(S\\) and \\(A\\) is an event with nonzero probability, then:\n\\[\nP(E_i\\;|A) = \\frac{P(A\\;| E_i) P(E_i)}{\\sum_i P(A\\;| E_i) P(E_i)}\n\\]\n\n\n\n\n\n\nProof\n\n\n\nThe result follows from the multiplication rule and the law of total probability. Since \\(\\{E_i\\}\\) is a partition, \\(P(A) = \\sum_i P(A\\;| E_i) P(E_i)\\); further, \\(P(E_i \\cap A) = P(A\\;| E_i) P(E_i)\\) Then: \\[\nP(E_i\\;|A) = \\frac{P(E_i \\cap A)}{P(A)} = \\frac{P(A\\;| E_i) P(E_i)}{\\sum_i P(A\\;| E_i) P(E_i)}\n\\]\n\n\nThe probability \\(P(E_i \\;| A)\\) is referred to as the posterior probability and the probability \\(P(E_i)\\) is referred to as the prior probability. The theorem provides a means of obtaining the posterior from the priors (assuming the conditional probabilities \\(P(A\\;| E_i)\\) can be computed) and is sometimes interpreted as a rule for ‘updating’ the probabilities \\(P(E_i)\\).\nAs a special case, note that for any event \\(E\\), \\(\\{E, E^C\\}\\) partition the sample space. Therefore: \\[\nP(E\\;| A) = \\frac{P(A\\;| E) P(E)}{P(A\\;| E) P(E) + P(A\\;| E^C) P(E^C)}\n\\]\n\n\n\n\n\n\nExample: diagnostic test\n\n\n\nConsider a diagnostic test and let \\(A\\) denote the event one has the condition of interest, \\(T_+\\) denote the event one receives a positive test result, and \\(T_-\\) denote the event one receives a negative test result. Assume that laboratory controls indicate that:\n\\[\n\\begin{align*}\nP(T_+ \\;| A) &= 0.99 \\\\\nP(T_- \\;| A) &= 0.01 \\\\\nP\\left(T_+ \\;| A^C\\right) &= 0.05 \\\\\nP\\left(T_- \\;| A^C\\right) &= 0.95\n\\end{align*}\n\\] As an aside, \\(P(T_+\\;| A)\\) is the true positive rate, also known as the sensitivity of the test; \\(P(T_- \\;| A^C)\\) is the true negative rate, also known as the specificity. Note that \\(P(T_+ \\;| A) + P(T_- \\;| A^C) \\neq 1\\), despite the fact that \\(T_+ = (T_-)^C\\); this is because conditioning on different events produces different probability measures.\nSuppose first that the condition has a prevalence of \\(0.5\\%\\) in the general population, so assume \\(P(A) = 0.005\\). Then the probability that one has the condition given a positive test result is: \\[\n\\begin{align*}\nP(A \\;| T_+)\n&= \\frac{P(T_+ \\;| A) P(A)}{P(T_+ \\;| A) P(A) + P\\left(T_+ \\;| A^C\\right) P\\left(A^C\\right)} \\\\\n&= \\frac{(0.99)(0.005)}{(0.99)(0.005) + (0.05)(0.995)}\\\\\n&\\approx 0.0905\n\\end{align*}\n\\] Further, the probability that one has the condition given a negative test result is: \\[\n\\begin{align*}\nP(A \\;| T_-)\n&= \\frac{P(T_- \\;| A) P(A)}{P(T_- \\;| A) P(A) + P\\left(T_- \\;| A^C\\right) P\\left(A^C\\right)}  \\\\\n&= \\frac{(0.01)(0.005)}{(0.01)(0.005) + (0.95)(0.995)} \\\\\n&\\approx 0.0000529\n\\end{align*}\n\\]\nThis usually comes as a surprise, given that the test appears to be fairly accurate, but somewhat prone to false negatives. What happens if the condition is more prevalent, say, present in \\(10\\%\\) of the population? In this scenario:\n\\[\n\\begin{align*}\nP(A \\;| T_+)\n&= \\frac{(0.99)(0.1)}{(0.99)(0.1) + (0.05)(0.9)}\n\\approx 0.688 \\\\\nP(A \\;| T_-)\n&= \\frac{(0.05)(0.1)}{(0.05)(0.1) + (0.95)(0.9)}\n\\approx 0.0012\n\\end{align*}\n\\]\nTry a few other scenarios on your own. First make a guess about how the posterior probability will change, and then check the calculation.\n\nWhat happens if the false positive rate \\(P\\left(T_+\\;| A^C\\right)\\) is higher/lower, keeping everything else fixed?\nWhat happens if the false negative rate \\(P\\left(T_- \\;| A)\\right)\\) is higher/lower, keeping everything else fixed?\nWhat happens if the specificity increases to \\(99.5\\%\\) but the false negative rate increases to \\(10\\%\\)?\nWhat happens if the sensitivity decreases to \\(91\\%\\) but the false positive rate drops to \\(0.1\\%\\)?\n\n\n\nThe odds of an event \\(E\\) usually refers to the ratio \\(\\frac{P(E)}{P\\left(E^C\\right)}\\), but to be more precise, this is really the odds of \\(E\\) relative to its complement \\(E^C\\). More generally, for any two events \\(E_1, E_2\\): \\[\n\\text{odds}\\left(E_1, E_2\\right) = \\frac{P(E_1)}{P(E_2)}\n\\] The value of the odds gives the factor by which \\(E_1\\) is more likely than \\(E_2\\), and:\n\nif \\(\\text{odds}\\left(E_1, E_2\\right) = 1\\), the events are equally likely;\nif \\(\\text{odds}\\left(E_1, E_2\\right) &lt; 1\\), \\(E_2\\) is more likely than \\(E_1\\);\nif \\(\\text{odds}\\left(E_1, E_2\\right) &gt; 1\\), \\(E_1\\) is more likely than \\(E_2\\).\n\nThe conditional odds of one event relative to another is defined identically but with a conditional probability measure: \\[\n\\text{odds}\\left(E_1, E_2 \\;| A\\right) = \\frac{P(E_1 \\;| A)}{P(E_2\\;| A)}\n\\] One way to understand the diagnostic test example is that, although the probability of having the condition given a positive result is low, conditioning on the test result increases the posterior odds substantially.\n\n\n\n\n\n\nExample: diagnostic test (cont’d)\n\n\n\nThe prior odds of having the condition are: \\[\n\\text{odds}\\left(A, A^C\\right) = \\frac{P(A)}{P\\left(A^C\\right)} = \\frac{0.005}{0.995} = 0.0005\n\\] But the posterior odds after conditioning on \\(T_+\\) are: \\[\n\\text{odds}\\left(A, A^C \\;| T_+\\right) = \\frac{P(A\\;| T_+)}{P\\left(A^C \\;| T_+\\right)} = \\frac{0.33}{0.67} \\approx 0.493\n\\] So the odds increase by a factor of \\(\\frac{0.493}{0.0005} \\approx 985\\) by conditioning on a positive test result.\n\n\nThere is an odds form of Bayes’ theorem, which states that under the same conditions as the original result, the posterior odds are the prior odds multiplied by a ‘Bayes factor’: \\[\n\\text{odds}\\left(E_i, E_j\\;| A\\right) = \\text{odds}(E_i, E_j) \\cdot \\frac{P(A\\;| E_i)}{P(A\\;| E_j)}\n\\] The result follows from the definition of conditional odds and application of Bayes’ theorem to the conditional probabilities \\(P(E_i\\;| A)\\), and the proof is left as an exercise.\n\n\n\n\n\n\nExample: the Monty Hall problem\n\n\n\nThis is a classic probability problem named after the original host of the game show Let’s Make a Deal. The scenario is this: on a game show, you’re presented with three doors; behind one door is a car, and behind the other two are goats. If you pick the door concealing a car, you win the car (although no one ever explains whether, if you pick otherwise, you win a goat). You choose a door; then the host opens one of the remaining doors, revealing a goat, and asks you if you want to stay with your original guess, or switch. Are you more likely to win the car if you stay, or if you switch?\nWe’ll work out the solution in class."
  },
  {
    "objectID": "content/hw3.html",
    "href": "content/hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Please prepare your solutions neatly, numbered, and in order; ideally, you’ll write up a final clean copy after completing all problems on scratch paper. Please note that if a prompt includes a question, you’re expected to support answers with reasoning, even if the prompt does not explicitly ask for a justification. Provide your name at the top of your submission, and if you collaborate with other students in the class, please list their names at the top of your submission beneath your own.\n\nLet \\((S, \\mathcal{S}, P)\\) be any probability space. Show that if \\(\\{E_j\\}\\) is a partition of \\(S\\) with \\(P(E_j) &gt; 0\\), then for any events \\(A, B\\): \\[\nP(A\\;| B) = \\sum_j P(A\\;| B \\cap E_j) P(E_j\\;| B)\n\\]\nSickle-cell disease is an inherited condition that causes pain and damage to organs and muscles. It is recessive: people with two copies of the relevant allele have the disease, but people with only one copy are healthy. That is, if \\(A\\) is the sickle-cell allele and \\(a\\) is the neutral allele, people with \\(AA\\) have the disease, and people with \\(Aa\\) or \\(aa\\) do not. Suppose that for some study population the probability that an individual has each combination is as shown below. \\[\n\\begin{align*}\nP(AA) &= 0.02 \\\\\nP(Aa) &= 0.18 \\\\\nP(aa) &= 0.8\n\\end{align*}\n\\] Assume that parents reproduce independently of their having any of these genotypes and inherited alleles are selected independently and with equal probability from each parent.\n\nWhat are the unique combinations of parent genotypes?\nFor each possible combination from (i), find the probability that a child has sickle-cell disease given parent genotypes.\nFind the probability that a child from this population has sickle-cell disease.\n\nSuppose you are analyzing a diagnostic test for a condition that appears in 5% of the population. Let \\(T_+, T_-\\) denote the events that an individual obtaining a positive or negative test result, respectively, and \\(C_+, C_-\\) denote the events that an individual is a positive or negative case. Represent the detection rates for each type of case as follows: \\[\n\\begin{align*}\nP(T_+ \\;| C_+) &= a \\qquad\\text{(true positive rate)}\\\\\nP(T_- \\;| C_+) &= 1 - a \\qquad\\text{(false negative rate)}\\\\\nP(T_+ \\;| C_-) &= 1 - b \\qquad\\text{(false positive rate)}\\\\\nP(T_- \\;| C_-) &= b \\qquad\\text{(true negative rate)}\n\\end{align*}\n\\]\n\nFind the probability that the test correctly diagnoses a randomly chosen individual from the population in terms of \\(a, b\\).\nIf the test achieves an overall accuracy of 90% — so \\(P(\\text{test result is correct}) = \\frac{9}{10}\\) — and the true positive rate is \\(a = \\frac{8}{10}\\), what is the true negative rate \\(b\\)?\nFind \\(P(C_+\\;|T_+)\\) and \\(P(C_- \\;| T_-)\\) in terms of \\(a, b\\).\nIf the true positive rate is \\(a = 0.8\\), what true negative rate \\(b\\) produces a test for which \\(P(C_+\\;| T_+) = 0.9\\)?\nUnder the scenario in (iv), what is \\(P(C_-\\;| T_-)\\)?\nSuppose the test is being redesigned to ensure that \\(P(C_+ \\;|T_+) \\geq \\frac{9}{10}\\). If the best possible true positive rate for the test is \\(a = \\frac{9}{10}\\), what is the maximum false positive rate that achieves the redesign goal?"
  },
  {
    "objectID": "content/week1-probability.html",
    "href": "content/week1-probability.html",
    "title": "Probability measures",
    "section": "",
    "text": "Consider an experiment or random process and define the sample space to be:\n\\[\nS: \\text{ set of all possible outcomes}\n\\]\nAn event is a subset \\(E \\subseteq S\\).1 Its complement is defined as the set \\(E^C = S\\setminus E\\). Note that \\(\\left(E^C\\right)^C = E\\). It is immediate from previous results that, given any events \\(A, B\\):\n\\[\n(A\\cup B)^C = A^C \\cap B^C\n\\qquad\\text{and}\\qquad\n(A\\cap B)^C = A^C \\cup B^C\n\\] Recursive application of this property yields DeMorgan’s laws:\n\\[\n\\left[\\bigcup_{i = 1}^n A_i\\right]^C = \\bigcap A_i^c\n\\qquad\\text{and}\\qquad\n\\left[\\bigcap_{i = 1}^n A_i\\right]^C = \\bigcup A_i^c\n\\]\nThe proof is by induction, and we’ll review it in class. The base case is already established. For the inductive step, one need only reapply the base case replacing \\(A\\) by \\(\\bigcup_{i = 1}^n A_i\\) and \\(B\\) by \\(A_{n + 1}\\).\nTwo events \\(E_1, E_2 \\subseteq S\\) are disjoint just in case they share no outcomes, that is, if \\(E_1 \\cap E_2 = \\emptyset\\).\nA collection of events \\(\\{E_i\\}\\) is mutually disjoint just in case every pair is disjoint, that is, if:\n\\[\nE_i \\cap E_j = \\emptyset\n\\quad\\text{for all}\\quad\ni \\neq j\n\\]\nA partition is a mutually disjoint collection whose union contains an event of interest. Usually, one speaks of a partition of the sample space \\(S\\), i.e., a collection \\(\\{E_i\\}\\) such that:\n\n\\(\\{E_i\\}\\) are mutually disjoint events\n\\(\\bigcup_i E_i = S\\)\n\nNote that \\(\\{E, E^C\\}\\) always form a partition of the sample space for any event \\(E\\).\n\n\n\n\n\n\nCheck your understanding\n\n\n\nLet \\(S = [0, 2], E_1 = (0, 1), E_2 = \\{0, 1, 2\\}, E_3 = (0, 2)\\).\n\nAre \\(E_1\\) and \\(E_2\\) disjoint?\nAre \\(E_2\\) and \\(E_3\\) disjoint?\nDoes the collection \\(\\{E_1, E_2, E_3\\}\\) form a partition of \\(S\\)?\nFind the set \\(E_4\\) such that \\(\\{E_1, E_2, E_4\\}\\) partition \\(S\\)."
  },
  {
    "objectID": "content/week1-probability.html#footnotes",
    "href": "content/week1-probability.html#footnotes",
    "title": "Probability measures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, not every subset of \\(S\\) is necessarily an event. This won’t matter especially for this course, but it’s worth mentioning here.↩︎\nIn these notes \\(P\\) is explicitly defined as having range \\([0, 1]\\), so this property may seem redundant. However, most sources simply define \\(P\\) as a real-valued function, in which case this property must be derived. The property is included here to underscore that the \\([0, 1]\\) range is in fact a consequence of the axioms, and not an assumption.↩︎"
  },
  {
    "objectID": "content/hw1.html",
    "href": "content/hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Please prepare your solutions neatly, numbered, and in order; ideally, you’ll write up a final clean copy after completing all problems on scratch paper. Please note that if a prompt includes a question, you’re expected to support answers with reasoning, even if the prompt does not explicitly ask for a justification. Provide your name at the top of your submission, and if you collaborate with other students in the class, please list their names at the top of your submission beneath your own.\n\n(Serial systems) In a serial system, components are linked together in such a way that the system only works if every component works. For example, consider a string of Christmas lights; if one light goes out, the whole string goes out. Suppose that one has a serial system with \\(k\\) components that all function independently of one another. The state of the system can be represented by a binary vector \\(x = (x_1, \\dots, x_k)\\) where the coordinate \\(x_i\\) indicates whether the \\(i\\)th component is working. The relevant sample space is the set of all possible values of \\(x\\), that is, \\(S = \\{(x_1, \\dots, x_k): x_i \\in \\{0, 1\\}\\}\\), so that the system states are the outcomes, and the events are all possible subsets \\(\\mathcal{S} = 2^S\\). Let \\(E_i \\in \\mathcal{S}\\) denote the event that the \\(i\\)th component works.\n\nExpress the sample space \\(S\\) as a Cartesian product.\nExpress the event \\(E_i\\) as a set in terms of the system states \\(x \\in S\\).\nList two distinct outcomes included in \\(E_1\\) and two distinct outcomes included in \\(E_2\\).\nIs \\(\\{E_i\\}\\) a disjoint collection? Why or why not?\nFind the number of system states \\(|S|\\) and the number of possible events \\(|\\mathcal{S}|\\).\n\nContinuing the example in the previous problem, express each of the following events in terms of the collection \\(\\{E_i\\}\\).\n\nThe first component works and the second component fails.\nThe first three components work.\nThe system works.\nThe system fails.\nExactly one component fails.\n\nConsider the monotone sequences of sets defined by \\(A_n = [0, 1 + \\frac{1}{n})\\) and \\(B_n = [0, 1 - \\frac{1}{n})\\).\n\nIs \\(\\{A_n\\}\\) increasing or decreasing?\nIs \\(\\{B_n\\}\\) increasing or decreasing?\nTrue or false: \\(\\lim_{n \\rightarrow\\infty} A_n = \\lim_{n\\rightarrow\\infty} B_n\\)? Explain. (Hint: \\(x \\in \\bigcup_n C_n\\) just in case \\(x \\in C_n\\) for at least one \\(n\\); similarly, \\(x \\in \\bigcap_n C_n\\) just in case \\(x \\in C_n\\) for every \\(n\\).)\n\nConsider the “experiment” of rolling 2 six-sided dice, and denote the outcomes by pairs \\((i, j)\\) where \\(i, j \\in \\{1, 2, 3, 4, 5, 6\\}\\).\n\nWrite the sample space \\(S\\) for this experiment, assuming the order of the dice does not matter (i.e., \\((3, 2) = (2, 3)\\)), and find \\(|S|\\).\nIf \\(P(E) = 1\\) whenever \\(E = \\{(1, 1)\\}\\) and \\(P(E) = 0\\) otherwise for \\(E \\in 2^S\\), is \\(P\\) a valid probability measure? Why or why not?\nIf \\(P(E) = 1\\) whenever \\((1, 1) \\in E\\) and \\(P(E) = 0\\) otherwise for \\(E \\in 2^S\\), is \\(P\\) a valid probability measure? Why or why not?\n\n(Uniform distribution) [OPTIONAL] Consider the triple \\((S, \\mathcal{S}, P)\\) where:\n\\[\n\\begin{align*}\nS &= [0, 1] \\\\\n\\mathcal{S} &= \\left\\{A \\subseteq S: A \\text{ is a countable union or intersection of open or closed intervals or their complements}\\right\\} \\\\\nP(E) &= \\int_E dx,\\quad E\\in\\mathcal{S} \\qquad\\text{(i.e., total length of $E$)}\n\\end{align*}\n\\]\n\nShow that \\((S, \\mathcal{S}, P)\\) is a probability space by verifying the requisite conditions on \\(\\mathcal{S}\\) and \\(P\\).\nLet \\(\\mathcal{C}\\) denote the Cantor set. Show that \\(P(\\mathcal{C}) = 0\\).\n\nRemark: the integral \\(\\int_E dx\\) is defined as follows:\n\nfor contiguous intervals, \\(\\int_{(a, b)} dx = \\int_{[a, b]} dx = \\int_{[a, b)} dx = \\int_{(a, b]} dx = \\int_a^b dx\\)\nfor disjoint intervals \\(E_i\\), \\(\\int_{\\bigcup_i E_i} dx = \\sum_i \\int_{E_i} dx\\)\n\nLet \\((S, \\mathcal{S}, P)\\) be a probability space, and let \\(\\{E_i\\}\\) be a collection of events. Show that if \\(\\{E_i\\}\\) is a finite or countable partition of any event \\(A \\subseteq S\\), then \\(\\sum_i P(E_i) = P(A)\\).\n(Bonferroni inequality) Use results from class to show that \\(P\\left(\\bigcap_{i = 1}^n E_i\\right) \\geq 1 - \\sum_{i = 1}^n P\\left(E^C\\right)\\)."
  },
  {
    "objectID": "content/week2-counting.html",
    "href": "content/week2-counting.html",
    "title": "Counting rules",
    "section": "",
    "text": "Suppose \\(S = \\{s_1, \\dots, s_n\\}\\) and \\(\\mathcal{S} = 2^S\\). Let \\(\\{p_i\\}\\) be \\(n\\) numbers such that \\(0 \\leq p_i \\leq 1\\) and \\(\\sum_{i = 1}^n p_i = 1\\). Then the set function\n\\[\nP(E) = \\sum_{i: x_i \\in E} p_i \\quad,\\; E\\in \\mathcal{S}\n\\]\nis a probability measure on \\((S, \\mathcal{S})\\), i.e., \\((S, \\mathcal{S}, P)\\) is a probability space.\n\n\n\n\n\n\nProof\n\n\n\nSince by construction \\(\\mathcal{S}\\) is a \\(\\sigma\\)-algebra, it remains only to check that \\(P\\) satisfies the probability axioms.\n\nAxiom 1: \\(P(E) = \\sum_{i: x_i \\in E} p_i \\geq 0\\) since by hypothesis \\(p_i \\geq 0\\).\nAxiom 2: \\(P(S) = \\sum_{i: x_i \\in E} p_i = \\sum_{i = 1}^n p_i = 1\\).\nAxiom 3: let \\(\\{E_j\\}\\) be disjoint and define \\(I_j = \\{i: x_i \\in E_j\\}\\). Note that \\(\\left\\{i: x_i \\in \\bigcup_j E_j\\right\\} = \\bigcup_j I_j\\) and \\(I_j \\cap I_k = \\emptyset\\) for \\(j \\neq k\\). Then: \\[\nP\\left(\\bigcup_j E_j\\right) = \\sum_{\\bigcup_j I_j} p_i = \\sum_j \\sum_{I_j} p_i = \\sum_j P(E_j)\n\\]\n\n\n\nSo, probability measures on finite sample spaces are simply assignments of numbers in \\([0, 1]\\) to each outcome that sum to one.\nNow, if one has equally likely outcomes in a finite sample space, i.e., \\(p_i = p_j\\), then: \\[\nP(E) = \\sum_{i: x_i \\in E} p_i = \\sum_{i: x_i \\in E} \\frac{1}{|S|} = \\frac{|E|}{|S|}\n\\]\nThus, for finite probability spaces with equally likely outcomes, computing event probabilities is a matter of counting. While conceptually straightforward, it is often nontrivial to count the elements in a set. For example, how would you count the number of ways to draw a 3-of-a-kind in a 5-card poker hand? In other words, how many distinct combinations of 5 cards contain exactly three of matching rank and no other matches? For that matter, how many 5-card poker hands are possible? Once you know the answer, finding the probability of a 3-of-a-kind (or obtaining one by drawing 5 cards at random, anyway) is easy; counting the outcomes is the tricky part."
  },
  {
    "objectID": "content/week2-counting.html#footnotes",
    "href": "content/week2-counting.html#footnotes",
    "title": "Counting rules",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe notation \\([x_1, \\dots, x_n]\\) is an ad-hoc expression for the unordered collection consisting of \\(x_1, \\dots, x_n\\) possibly non-distinct elements. Some special notation is needed here because when the collection includes duplicates, it cannot be written as a set. For instance, \\(\\{0, 0, 1\\} = \\{0, 1\\}\\) but \\([0, 0, 1] \\neq [0, 1]\\).↩︎"
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Materials",
    "section": "",
    "text": "Week 0 (9/21/23)\nCourse introduction; syllabus.\n\nClass meetings\nThursday: course introduction [slides]\n\n\nAssignments\nComplete intake survey\n\n\n\nWeek 1 (9/25/23)\nSets and set operations; probability spaces; properties of probability.\n\nSuggested reading\nDG&S 1.4 — 1.6\n\n\nLecture notes\n[sets] [probability measures]\n\n\nClass meetings\nMonday: sets\nTuesday: sets, cont’d\nWednesday: probability spaces\nThursday: properties of probability measures\n\n\nAssignment\nHomework 1 due Thursday 10/5 [solutions]\n\n\n\nWeek 2 (10/2/23)\nProbability on finite sample spaces; counting rules and applications.\n\nSuggested reading\nDG&S 1.7 — 1.10\n\n\nLecture notes\n[counting]\n\n\nClass meetings\nMonday: wrap-up; probability on finite sample spaces\nTuesday: counting principles and multiplication rules\nWednesday: combinations and permutations\nThursday: combinations and permutations\n\n\nAssignment\nHomework 2 due Thursday 10/12 \n\n\n\nWeek 3 (10/9/23)\nConditional probability; independence; Bayes’ theorem.\n\nSuggested reading:\nDG&S 2.1 — 2.3\n\n\nLecture notes\n[conditional probability]\n\n\nClass meetings:\nMonday: conditional probability and basic properties\nTuesday: independence\nWednesday: Bayes’ theorem\nThursday: odds and conditional odds\n\n\nAssignment\n\nHomework 3 due Thursday 10/19\n\n\n\n\nWeek 4 (10/16/23)\nMidterm study guide — check back for updates\n\nSuggested reading:\nDG&S 2.4\n\n\nLecture notes\n[gambler’s ruin]\n\n\nClass meetings:\nMonday: the gambler’s ruin problem\nTuesday: midterm review session\nWednesday: random variables\nThursday: Midterm 1\n\n\nAssignment\n\nNone — enjoy!"
  },
  {
    "objectID": "content/week1-sets.html",
    "href": "content/week1-sets.html",
    "title": "Sets",
    "section": "",
    "text": "Concepts\nA set is a collection of mathematical objects such as numbers, points, functions, or more sets.\nIf an object \\(x\\) is in a set \\(A\\), we say that \\(x\\) is an element of \\(A\\) and write \\(x \\in A\\) to mean “\\(x\\) is in \\(A\\)”. Otherwise, we write \\(x \\not\\in A\\).\nWe write a set by identifying its elements. For example:\n\\[\nA =\\{1, 2, 3, 4, 5\\}\n\\]\nTwo sets \\(A, B\\) are identical just in case they have exactly the same elements:\n\\[\nA = B\n\\quad\\Longleftrightarrow\\quad\nx \\in A \\Leftrightarrow x \\in B\n\\]\n\n\nSpecial sets\nThere are some special notations for the sets of numbers:\n\n\\(\\mathbb{R}\\): real numbers\n\\(\\mathbb{N}\\): natural numbers\n\\(\\mathbb{Z}\\): integers\n\\(\\mathbb{C}\\): complex numbers\n\\(\\mathbb{Q}\\): rational numbers\n\nSimilarly, open, closed, and half-open real intervals are denoted:\n\\[\n\\begin{align*}\n[a, b] = \\{x \\in \\mathbb{R}: a \\leq x \\leq b\\} \\\\\n(a, b] = \\{x \\in \\mathbb{R}: a &lt; x \\leq b\\} \\\\\n[a, b) = \\{x \\in \\mathbb{R}: a \\leq x &lt; b\\} \\\\\n(a, b) = \\{x \\in \\mathbb{R}: a &lt; x &lt; b\\}\n\\end{align*}\n\\]\n\n\nContainment\nIf all the elements of a set \\(A\\) are also in \\(B\\), then we say that \\(A\\) is contained in \\(B\\), or that \\(A\\) is a subset of \\(B\\), and write \\(A \\subseteq B\\). (Containment is a binary relation and defines a partial ordering on the set of all sets.)\n\\[\nA \\subseteq B\n\\quad\\Longleftrightarrow\\quad\nx \\in A \\Rightarrow x \\in B\n\\]\nFurther, \\(A\\) is a proper subset of \\(B\\) just in case there is at least one element of \\(B\\) that is not in \\(A\\):\n\\[\nA \\subset B\n\\quad\\Longleftrightarrow\\quad\nA \\subseteq B \\text{ and } A \\neq B\n\\]\nBoth relations are transitive:\n\nIf \\(A \\subseteq B\\) and \\(B \\subseteq C\\) then \\(A \\subseteq C\\)\nIf \\(A \\subset B\\) and \\(B \\subset C\\) then \\(A \\subset C\\)\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\nLet \\(A = [0, 1)\\) and \\(B = (0, 1]\\) and \\(C = (0, 2)\\)\n\nTrue or false, \\(A \\subset C\\)\nTrue or false, \\(B \\subset C\\)\n\nList all proper subset relations among \\(\\mathbb{R}, \\mathbb{Q}, \\mathbb{N}, \\mathbb{Z}, \\mathbb{C}\\).\n\n\n\n\n\nNull set\nLastly it may happen that a set contains no elements. This set is called the null set (or empty set) and is denoted \\(\\emptyset = \\{ \\}\\).\nFor a small brain teaser, prove that every set contains the null set: for any set \\(A\\), \\(\\emptyset \\subseteq A\\). Intuitively, this is true; but why, based on the definitions, must it hold? (Hint: the conditional “if \\(p\\) then \\(q\\)” is trivially true if \\(p\\) is always false.)\n\n\nSet operations\nThe two fundamental set operations are union and intersection. Let \\(A, B\\) be sets and define:\n\n(union) \\(A \\cup B = \\{x: x \\in A \\text{ or } x \\in B\\}\\)\n(intersection) \\(A \\cap B = \\{x: x \\in A \\text{ and } x \\in B\\}\\)\n\nBoth operations are associative:\n\\[\n\\begin{align*}\nA \\cup (B \\cup C) = (A \\cup B) \\cup C \\\\\nA \\cap (B \\cap C) = (A \\cap B) \\cap C\n\\end{align*}\n\\]\nAnd distributive:\n\\[\n\\begin{align*}\nA \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C) \\\\\nA \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\n\\end{align*}\n\\]\nProofs are direct and left as an exercise: apply definitions of union/intersection and show that the conditions specified by each construction are equivalent. The properties follow, essentially, from the meanings of “and” and “or”.\nThe operation of set difference corresponds to removing the elements of one set from another set:\n\\[\nA \\setminus B = \\{x: x \\in A \\text{ and } x \\not\\in B\\}\n\\]\nSet difference is not distributive over unions and intersections, but rather exhibits the following properties:\n\\[\n\\begin{align*}\nA \\setminus (B \\cap C) = (A \\setminus B) \\cup (A \\setminus C) \\\\\nA \\setminus (B \\cup C) = (A \\setminus B) \\cap (A \\setminus C)\n\\end{align*}\n\\]\nWe’ll review the proof in class.\nLastly, the Cartesian product between sets is the set of all possible pairs of elements from each set:\n\\[\nA \\times B = \\{(a, b): a \\in A, b \\in B\\}\n\\]\nThe \\(n\\)-fold product of a set \\(A\\) with itself is written \\(A^n\\):\n\\[\nA^n = \\{(a_1, \\dots, a_n): a_i \\in A, i = 1, \\dots, n\\}\n\\]\nThis is a handy construction, for example, in representing:\n\n\\(n\\)-dimensional real space (\\(\\mathbb{R}^n\\))\nan \\(n\\)-dimensional unit cube (\\([0, 1]^n\\))\n32-bit integers (\\(\\{0, 1\\}^{32}\\))\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\nLet \\(A = [0, 1)\\) and \\(B = (0, 1]\\) and write the following sets as real intervals.\n\n\\(A \\cup B\\)\n\\(A \\cap B\\)\n\\(B\\setminus A\\)\n\\((A \\cup B) \\setminus (A \\cap B)\\)\n\\((A \\cap B) \\setminus (A \\cup B)\\)\n\n\n\n\n\n\nCardinality\nThe cardinality of a set is the number of elements it contains, and is written \\(|A|\\). We say that:\n\n\\(A\\) is finite if \\(|A| &lt; \\infty\\)\n\\(A\\) is countable if \\(|A| = |\\mathbb{N}|\\)\n\\(A\\) is uncountable if \\(|A| &gt; |\\mathbb{N}|\\)\n\nThe power set of \\(A\\) is the set of all subsets of \\(A\\), and is written:\n\\[\n2^A = \\{B: B \\subseteq A\\}\n\\]\nIf \\(A\\) is finite, \\(|2^A| = 2^{|A|}\\); otherwise, \\(2^A\\) is uncountable. We’ll prove the finite case in class.\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\nIf \\(A = \\{0, 6, 12, 44, 190\\}\\), what is \\(|A|\\)?\nIf \\(A = \\{H, T\\}\\), list all the elements of \\(2^A\\).\nIf \\(A = \\{x \\in \\mathbb{N}: x \\leq 100 \\text{ and } x\\%2 = 0\\}\\), find \\(|A|\\) and \\(|2^A|\\)\n\n\n\n\n\nSequences of sets\nConsider a sequence of sets \\(A_1, A_2, \\dots\\). For short, we write the sequence \\(\\{A_i\\}_{i \\in I}\\) or simply \\(\\{A_i\\}\\), where the index set is implicit from context.\nDefine the union/intersection of the first \\(n\\) sets as follows:\n\\[\n\\begin{align*}\n\\bigcup_{i = 1}^n A_i = A_1 \\cup A_2 \\cup \\cdots \\cup A_n = \\{x: x \\in A_i \\text{ for some } i \\leq n\\} \\\\\n\\bigcap_{i = 1}^n A_i = A_1 \\cap A_2 \\cap \\cdots \\cap A_n = \\{x: x \\in A_i \\text{ for every } i \\leq n\\}\n\\end{align*}\n\\]\nSlightly more generally, one might define the union of a subcollection as \\(\\bigcup_{j \\in J} A_j\\) for some collection of indices in the index set \\(J \\subset \\mathbb{N}\\).\nNow, if the sequence is infinite, the union or intersection of all sets in the sequence is defined as:\n\\[\n\\begin{align*}\n\\bigcup_{n = 1}^\\infty A_n = \\{x: x \\in A_i \\text{ for some } i \\in \\mathbb{N} \\} \\\\\n\\bigcap_{n = 1}^\\infty A_n = \\{x: x \\in A_i \\text{ for every } i \\in \\mathbb{N}\\}\n\\end{align*}\n\\]\nThese are referred to as countable unions and countable intersections.\nWe say that a sequence is monotone if sets are sequentially nested, and more specifically, that the sequence is:\n\nnondecreasing if \\(A_i \\subseteq A_{i + 1}\\) for every \\(i \\in \\mathbb{N}\\)\nincreasing if \\(A_i \\subset A_{i + 1}\\) for every \\(i \\in \\mathbb{N}\\)\nnonincreasing if \\(A_i \\supseteq A_{i + 1}\\) for every \\(i \\in \\mathbb{N}\\)\ndecreasing if \\(A_i \\supset A_{i + 1}\\) for every \\(i \\in \\mathbb{N}\\)\n\nNote that increasing sequences are also nondecreasing, and that decreasing sequences are also nonincreasing. We define the limit of a monotone sequence of sets as the countable union or intersection:\n\\[\n\\begin{align*}\n\\lim_{n\\rightarrow\\infty} A_n = \\bigcap_{n = 1}^\\infty A_n \\quad\\text{(nonincreasing)} \\\\\n\\lim_{n\\rightarrow\\infty} A_n = \\bigcup_{n = 1}^\\infty A_n \\quad\\text{(nondecreasing)}\n\\end{align*}\n\\]\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\nIf \\(\\{A_n\\}\\) is a nondecreasing sequence, what is \\(\\bigcap_{n = 1}^\\infty A_n\\)?\nIf \\(\\{A_n\\}\\) is a nonincreasing sequence, what is \\(\\bigcup_{n = 1}^\\infty A_n\\)?\nIf \\(\\{A_n\\}\\) is a decreasing sequence, what is \\(\\bigcap_{i = 1}^n A_i\\)?\nIf \\(\\{A_n\\}\\) is a decreasing sequence, what is \\(\\bigcap_{n = 1}^\\infty A_n\\)?\nIf \\(\\{A_n\\}\\) is an increasing sequence, what is \\(\\bigcup_{i = 1}^n A_i\\)?\n\n\n\n\n\nThe Cantor set\nThe Cantor set is a subset of the unit interval with the counterintuitive distinction of having uncountably many points, but zero length. It is formed by recursively removing middle thirds, first from the unit interval, and then from each subinterval. This process is illustrated by the picture below.\n\n\n\nRecursively removing middle thirds.\n\n\nEach row represents a set consisting of the union of the shaded intervals, so we are considering a sequence of sets \\(\\{C_n\\}\\) where:\n\\[\n\\begin{align*}\nC_0 &= [0, 1] \\\\\nC_1 &= \\left[0, \\frac{1}{3}\\right] \\cup \\left[\\frac{2}{3}, 1\\right] \\\\\nC_2 &= \\left\\{\\left[0, \\frac{1}{9}\\right] \\cup \\left[\\frac{2}{9}, \\frac{1}{3}\\right]\\right\\} \\cup \\left\\{\\left[\\frac{2}{3}, \\frac{7}{9}\\right] \\cup \\left[\\frac{8}{9}, 1\\right]\\right\\} \\\\\n&\\vdots\n\\end{align*}\n\\] One way to write the recursion explicitly at an arbitrary step \\(n\\) is to consider the left endpoints \\(a_i^{(n - 1)}\\) of the intervals from the previous step in the recursion and express the \\(n\\)th step as:\n\\[\nC_n = \\bigcup_{i = 1}^{2^{n - 1}} \\left\\{ 3^{-n} \\left([0, 1]\\cup[2, 3]\\right) + a_i^{(n - 1)} \\right\\}\n\\]\n(In this expression, \\(c\\times[a, b] + d\\) is shorthand for \\([ca + d, cb + d]\\) and \\(\\times, +\\) distribute over unions.)\nNote that the sequence \\(\\{C_n\\}\\) is monotonic and strictly decreasing. The Cantor set is defined as the limit of this sequence:\n\\[\nC^* = \\lim_{n \\rightarrow \\infty} C_n = \\bigcap_{n = 0}^\\infty C_n\n\\]\nThe total length of any of the sets in the sequence is the sum of the lengths of the component intervals. The component intervals all have the same length \\(3^{-n}\\), so:\n\\[\n\\text{length}(C_n) = \\sum_{i = 1}^{2^n} 3^{-n} = \\left(\\frac{2}{3}\\right)^n\n\\longrightarrow 0 \\quad\n\\text{ as } \\quad n \\rightarrow \\infty\n\\]\nAlthough it requires proof that \\(\\text{length}(\\lim_n C_n) = \\lim_n \\text{length}(C_n)\\), this should seem plausible. (We will prove this result a little later.) Thus, then Cantor set has zero total length:\n\\[\n\\text{length}(C^*) = \\text{length}\\left(\\lim_{n \\rightarrow \\infty} C_n\\right) = \\lim_{n \\rightarrow \\infty}\\text{length}(C_n) = 0\n\\]\nFor this reason, and also from the construction of the Cantor set (as the limit of a sequence of countable unions of geometrically shrinking closed intervals) one would expect that \\(C^*\\) is countable. But in fact one can construct a one-to-one correspondence between the points in \\(C^*\\) and the points in the unit interval \\([0, 1]\\). (The trick is to observe that the ternary (base-3) decimal representation of any point in \\(C^*\\) only utilizes two unique digits and can thus be mapped to a binary decimal representation of a point in the unit interval in a way that is obviously bijective.) This establishes that \\(|C^*| = |[0, 1]|\\) — there are exactly the same number of points in the Cantor set as there are in the unit interval — and therefore that the Cantor set is uncountable!"
  },
  {
    "objectID": "content/midterm1-studyguide.html",
    "href": "content/midterm1-studyguide.html",
    "title": "Midterm 1 study guide",
    "section": "",
    "text": "This may be updated with further advice, suggestions, and the like — check back.\n\nMidterm details and scope\nThe midterm exam will be given in class on Thursday, October 19. Here are the logistical details:\n\n50 minutes in duration\nyou’re allowed one double-sided note sheet on standard 8.5 x 11 paper\nscratch paper will be available\n3 problems in length\nno calculator required\n\nProblems will draw on material from weeks 1-3, plus material covered Monday of week 4. You can expect one probability rules problem (week 1), one counting problem (week 2), and one conditional probability problem (week 3). Problems will emphasize application of course concepts; you need not expect to write\n\n\nTopics to review\n\nProbability rules\n\nset concepts and operations\nset cardinality\nDeMorgan’s laws\nevents, disjoint events, partitions\nprobability axioms\nprobability rules\n\nfinite additivity\ncomplement rule\nmonotonicity\nunion rule\nlimits for monotone sequences of events\ncountable sub-additivity (Boole’s inequality)\nfinite sub-additivity\nBonferroni’s inequality\n\nconstruction of probability measures on finite sample spaces\n\n\n\nCounting\n\nmultiplication rules for counting\ncounting rules for combinations of elements drawn from a finite set\n\nordered and drawn with replacement\nordered and drawn without replacement\nunordered and drawn with replacement\nunordered and drawn without replacement\n\n\n\n\nConditional probability\n\ndefinition of conditional probability\nmultiplication rules for conditional probability\nlaw of total probability\ndefinition of independence, pairwise independence, and mutual independence\nBayes’ theorem\nodds form of Bayes’ theorem\n\n\n\n\nExample problems\n\nSuppose you are performing \\(k\\) hypothesis tests at exact level \\(\\alpha\\), and assume all hypotheses are true. In other words, if \\(R_i\\) denotes the event that the \\(i\\)th hypothesis is rejected, i.e., of making an error, then \\(P(R_i) = \\alpha\\). However, when considering the tests as a group, there is a higher probability of making an error, as the individual error rates compound.It can be shown from the inclusion-exclusion principle that: \\[\nP\\left(\\bigcup_{i = 1}^k R_i\\right) \\geq \\sum_{i = 1}^k P(R_i) - \\sum_{1 \\leq i &lt; j \\leq k} P(R_i \\cap R_j)\n\\]\n\nUse Boole’s inequality to find an upper bound for the probability of making at least one error.\nUse the fact above to derive a lower bound for the probability of making at least one error, assuming that the tests are independent.\nCompute the bound for performing 20 tests at \\(\\alpha = 0.05\\).\nFind the smallest number of tests for which the lower bound in (ii) exceeds \\(\\frac{1}{2}\\). (Hint: write \\(0.05\\) as \\(\\frac{5}{100}\\); you may find it useful to know that \\(41^2 = 1681\\).)\n\n\nRemark: this last part is maybe a little more involved in terms of calculation than what I’d put on an exam.\n\nSuppose you are reading an article on admission rates at a selective college for different demographic groups reporting that 20% of applicants from underrepresented groups were admitted in 2022-2023 compared with 10% of other applicants.\n\nWhat is the probability that a randomly selected incoming student is from an underreppresented group if there were twice as many applicants from non-underrepresented groups?\nWhat is the probability that a randomly selected incoming student is from an underrepresented group if there were ten times as many applicants from non-underrepresented groups?\nUnder the scenario in (i), find the probability that a randomly selected applicant was admitted.\n\nImagine you’re out trick-or-treating with a friend and you arrive at a house handing out only two treats: Kit Kats and Snickers. The homeowner has mixed them up in a large pillowcase. You and your friend can each draw \\(n\\) pieces of candy; assume either treat is equally likely to be selected on each draw. Supposing that when you arrive, there are \\(N &gt; 2n\\) Snickers and \\(M &gt; 2n\\) Kit Kats, and you draw first, find an expression for the probability that you draw \\(k_1\\) Snickers and your friend draws \\(k_2\\) Snickers.\n\nExtra practice: find an expression for the probability that your friend draws \\(k_2\\) Snickers.\n\n\nSuggestions\nReview your notes, homeworks, and the posted lecture notes. Think a bit about what would be most useful to put on your note sheet, and consider prioritizing (i) things you are likely to forget (counting formulae, for instance) and (ii) definitions or results you’d like to have handy for reference (e.g., definition of conditional probability). You don’t need to use all of the space you’re allowed; sometimes fewer notes that are easier to navigate are more useful than dense notes.\nTry some example problems from the book. Look to the end of chapters 1 and 2, and pick a few that seem comfortable and relate to specific material you’d like to practice. No need to choose difficult problems — this can actually be counterproductive. Remember, you want practice and repetition, not a struggle.\nWhen you sit to take the test, read the whole exam first before beginning work. Take a moment to think, and start with the problem you feel most confident in — this can help to build a bit of momentum to carry you through any portions you find challenging."
  },
  {
    "objectID": "content/week4-gambler.html",
    "href": "content/week4-gambler.html",
    "title": "The gambler’s ruin problem",
    "section": "",
    "text": "This is a classic probability problem equivalent to a simple random walk on the nonnegative integers. The set-up is that a gambler plays a game where on each play, they win one dollar with probability \\(p\\) and lose one dollar with probability \\(1-p\\). They keep playing until either they go broke or win \\(k\\) dollars. What is the probability that they leave a winner?\nThe sample space in this problem is all possible sequences of plays. This is a big collection, and we won’t be able to resolve the probabilities of events by counting arguments. However, we do know, given that the gambler has \\(j\\) dollars after some play, their possible fortune at the next play and the associated probabilities; this information can be used to resolve the problem.\nDefine \\(X_n\\) to be the gambler’s fortune after \\(n\\) plays, for \\(n = 0, 1, 2, \\dots\\); \\(X_n = j\\) defines the event that the gambler has \\(j\\) dollars at play \\(n\\), for each \\(j\\) and \\(n\\). The problem set-up is that for every \\(n, j\\): \\[\nP(X_{n + 1} = x \\;| X_n = j)\n= \\begin{cases}\n  p, &x = j + 1 \\\\\n  1 - p, &x = j - 1\n\\end{cases}\n\\]\nNow we are interested in the probability that the gambler wins \\(k\\) dollars; denote this event by \\(W_k\\). This will change over the course of play and depend on how much they have at any given time. So consider: \\[\na_j = P(W_k \\;| X_n = j)\n\\qquad\\text{for each } j = 0, \\dots, k\n\\]\nNote that \\(a_0 = 0\\), since the gambler cannot win if they have no money left to play (i.e., \\(W_k \\cap \\{X_n = 0\\} = \\emptyset\\) for every \\(n\\)), and \\(a_k = 1\\), since if the gambler has \\(k\\) dollars they have won (i.e., \\(W_k \\supset \\{X_n = k\\}\\) for every \\(n\\)). Now, the probability of winning depends only on the most recent play; the plays before are irrelevant. Thus, \\(P(W_k \\;| X_{n + 1}, X_n) = P(W_k \\;| X_{n + 1})\\) (This is known as the Markov property.) Using the law of total (conditional) probability (see HW3 for a proof), we have that for each \\(j\\): \\[\n\\begin{align*}\nP(W_k \\;| X_n = j)\n&= P(W_k \\;| X_{n + 1} = j + 1) P(X_{n + 1} = j + 1\\;| X_n = j) \\\\\n&\\qquad + P(W_k \\;| X_{n + 1} = j - 1) P(X_{n + 1} = j - 1\\;| X_n = j)\n\\end{align*}\n\\] And therefore \\(a_j = pa_{j + 1} + (1 - p)a_{j - 1}\\). Some rearrangement yields that: \\[\na_{j + 1} - a_j = (a_j - a_{j - 1})\\left(\\frac{1 - p}{p}\\right)\n\\]\nSince \\(a_0 = 0\\), \\(a_2 - a_1 = a_1\\left(\\frac{1 - p}{p}\\right)\\), and then by recursion we obtain: \\[\na_{j + 1} - a_j = a_1 \\left(\\frac{1 - p}{p}\\right)^j\n\\qquad j = 1, 2, \\dots, k - 1\n\\] Then by writing \\(a_{j + 1} - a_1\\) as a telescoping sum \\(\\sum_{i = 1}^j (a_{i + 1} - a_i)\\), we can express \\(a_{j + 1}\\) as a geometric series and obtian: \\[\na_{j + 1} = \\sum_{i = 0}^j a_1 \\left(\\frac{1 - p}{p}\\right)^i\n=\\begin{cases}\na_1\\left[\\frac{1 - \\left(\\frac{1 - p}{p}\\right)^{j + 1}}{1 - \\left(\\frac{1 - p}{p}\\right)}\\right], &p\\neq \\frac{1}{2} \\\\\na_1 (j + 1), &p = \\frac{1}{2}\n\\end{cases}\n\\] Then, using the fact that \\(a_k = 1\\), we get: \\[\na_1 = \\begin{cases}\n\\frac{1 - \\left(\\frac{1 - p}{p}\\right)}{1 - \\left(\\frac{1 - p}{p}\\right)^{k}}, &p\\neq \\frac{1}{2} \\\\\n\\frac{1}{k}, &p = \\frac{1}{2}\n\\end{cases}\n\\] And finally, by substituting this into the expression above for \\(a_{j + 1}\\), we obtain for each \\(j = 0, \\dots, k\\): \\[\na_j = \\begin{cases}\n\\frac{1 - \\left(\\frac{1 - p}{p}\\right)^j}{1 - \\left(\\frac{1 - p}{p}\\right)^{k}}, &p\\neq \\frac{1}{2} \\\\\n\\frac{j}{k}, &p = \\frac{1}{2}\n\\end{cases}\n\\]\nWe can use this to solve the problem. For instance, if the game is fair (\\(p = \\frac{1}{2}\\)) and the gambler starts with $10, their probability of winning $100 is \\(P(W_{100}\\;|X_n = 10) = \\frac{10}{100} = 0.1\\). If \\(p = \\frac{2}{3}\\), the same probability is: \\[\nP(W_{100}\\;| X_n = 10) = \\frac{1 - \\frac{1}{2^{10}}}{1 - \\frac{1}{2^{100}}} \\approx 0.999\n\\]\nIf the gambler could play forever, what is the probability of getting infinitely rich? Take the limit in \\(k\\) to obtain: \\[\n\\lim_{k \\rightarrow \\infty} P(W_k\\;|X_n = j)\n= \\begin{cases}\n0, &p \\leq \\frac{1}{2}\\\\\n1 - \\left(\\frac{1 - p}{p}\\right)^j, &p &gt; \\frac{1}{2}\n\\end{cases}\n\\]\nIt’s actually somewhat interesting to plot the solution path in \\(j\\) for various \\(p, k\\). For instance, when \\(p = 0.49\\), and the game is only barely unfavorable, the probability of winning $200 is negligible until the gambler acquires most of the money they wish to win.\n\nfx &lt;- function(j, k, p){\n  if(p != 0.5){\n  odds &lt;- (1 - p)/p\n  out &lt;- (1 - odds^j)/(1 - odds^k)\n  }else{\n    out &lt;- j/k\n  }\n  return(out)\n}\n\nK &lt;- 200\nplot(0:K, fx(0:K, K, 0.49), type = 'l', xlab = 'j', ylab = 'P(W_200 | X_n = j)', main = 'p = 0.499')\n\n\n\n\nFor an extension of the problem, consider finding the minimum \\(j\\) such that \\(P(W_k \\;| X_n = j) \\geq \\frac{1}{2}\\) in terms of \\(p\\), in other words, the smallest amount of money to start with that ensures favorable odds of winning, given \\(p\\). Or, try plotting solution paths in \\(p\\)."
  },
  {
    "objectID": "content/hw2.html",
    "href": "content/hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Please prepare your solutions neatly, numbered, and in order; ideally, you’ll write up a final clean copy after completing all problems on scratch paper. Please note that if a prompt includes a question, you’re expected to support answers with reasoning, even if the prompt does not explicitly ask for a justification. Provide your name at the top of your submission, and if you collaborate with other students in the class, please list their names at the top of your submission beneath your own.\n\n(Hypergeometric distribution) Imagine a clown car with 50 clowns; suppose that 20 of them are happy clowns and 30 of them are sad clowns.\n\nIf 10 clowns exit the car sequentially and at random, what is the probability that exactly 3 are sad clowns?\nIf 10 clowns exit the car sequentially and at random, what is the probability that exactly \\(k\\) are sad clowns? (Assume \\(0\\leq k \\leq 10\\).)\nIf \\(n\\) clowns exit the car sequentially and at random, what is the probability that exactly \\(k\\) are sad clowns? (Assume \\(0 \\leq k \\leq n\\) and \\(n &lt; 20\\).)\nIf the car contains \\(N\\) happy clowns and \\(M\\) sad clowns and \\(n \\leq N + M\\) exit the car sequentially and at random, what is the probability that \\(k\\) are happy clowns (for \\(0 \\leq k \\leq \\text{min}(n, N)\\))?\n\nConsider rolling two six-sided dice. The sample space is \\(S = \\{1, 2, 3, 4, 5, 6\\}^2\\). Assuming the dice are fair, each outcome \\((i, j)\\) has equal probability \\(p\\). Consider the event that the dice sum to \\(k\\): \\(E_k = \\{(i, j)\\in S: i + j = k\\}\\).\n\nFind \\(|S|\\) and \\(p\\).\nFind \\(|E_k|\\) in terms of \\(k\\).\nMake a table of the probabilities \\(P(E_k)\\).\nInterpret the event \\(\\bigcup_{k = 1}^m E_k\\) in words and find \\(P\\left(\\bigcup_{k = 1}^m E_k\\right)\\) (assuming \\(1\\leq m \\leq 12\\)).\nFind the probability of rolling a sum smaller than or equal to 8.\n\n\nHint: you may find the proof of SWR2 helpful in answering part (ii); however, there are multiple ways to solve the problem.\n\nVerify the following identities.\n\n\\({n \\choose k} = \\frac{k + 1}{n - k}{n \\choose k + 1}\\)\n\\({n + m \\choose m} = {n + m \\choose n}\\)\n\\({n \\choose 1} = {n \\choose n - 1} = n\\)\n\\({n \\choose k} = {n \\choose n - k}\\)\n\\({n \\choose k} = \\frac{n}{k}{n - 1 \\choose k - 1}\\)\n\nConsider a lottery where players can choose 12 numbers between 0 and 50 (including 0 and 50) and one winning combination is drawn by randomly selecting one number at a time. Suppose there are three prizes: the biggest prize is awarded to a match of all numbers in the winning combination in sequence; the second biggest prize is awarded to a match of all numbers in the winning combination, but not in sequence; and a smaller cash prize is awarded for matching all but one number in the winning combination, and not necessarily in sequence.\n\nWhat is the probability of winning each prize if player selections (and the winning combination) can include each number no more than once?\nWhat is the probability of winning any of the prizes?\n\n(Matching problem) Suppose that you have \\(n\\) letters addressed to distinct recipients and \\(n\\) envelopes addressed accordingly, and the letters are placed in the envelopes at random and mailed. Let \\(A_i = i\\text{th letter is placed in the correct envelope}\\).\n\nFind the probability that the \\(i\\)th letter is placed in the correct envelope: determine \\(P(A_i)\\).\nFind the probability that the \\(i\\)th and \\(j\\)th letters are placed in the correct envelopes: determine \\(P(A_i \\cap A_j)\\) assuming \\(1 \\leq i &lt; j \\leq n\\).\nFind \\(\\sum_{1 \\leq i &lt; j \\leq n} P(A_i \\cap A_j)\\). (Hint: how many ways are there to choose two letters?)\nFind the probability that the \\(i\\)th, \\(j\\)th, and \\(k\\)th letters are placed in the correct envelopes: determine \\(P(A_i \\cap A_j \\cap A_k)\\) assuming \\(1 \\leq i &lt; j &lt; k \\leq n\\).\nFind \\(\\sum_{1 \\leq i &lt; j &lt; k \\leq n} P(A_i \\cap A_j \\cap A_k)\\). (Hint: how many ways are there to choose three letters?)\nFind the probability that an arbitrary subcollection of \\(i\\) letters (say, letters \\(j_1, \\dots, j_i\\)) are all placed in the right envelopes: determine \\(P(A_{j_1}\\cap A_{j2} \\cap \\cdots \\cap A_{ji})\\) assuming \\(1 \\leq j_1 &lt; \\cdots &lt; j_i \\leq n\\).\nFind \\(\\sum_{1 \\leq j_1 &lt; \\cdots &lt; j_i \\leq 1} P(A_{j_1} \\cap \\cdots \\cap A_{j_i})\\).\nUse the inclusion-exclusion formula to find the probability that at least one letter is mailed to the correct recipient. What is the limit of this probability as \\(n \\rightarrow \\infty\\)?"
  },
  {
    "objectID": "content/week0-intro.html#hi-im-trevor",
    "href": "content/week0-intro.html#hi-im-trevor",
    "title": "Welcome to STAT 425!",
    "section": "Hi I’m Trevor",
    "text": "Hi I’m Trevor\nYou can call me Trevor (if you like).\n\nPronouns: he/him/his\nHometown: Galesville, MD\nFun fact: I like to juggle (clubs, balls)\nOffice: Building 25 Room 236 (25-236)\nEmail: truiz01@calpoly.edu\nWeb: tdruiz.com"
  },
  {
    "objectID": "content/week0-intro.html#warm-up",
    "href": "content/week0-intro.html#warm-up",
    "title": "Welcome to STAT 425!",
    "section": "Warm-up",
    "text": "Warm-up\nIn groups of 3-4:\n\ngo around and say your name, hometown, and current class standing\nwrite down 3 words, concepts, facts, statements, ideas, or phrases you associate with probability (can be one per person or by consensus, your choice)\n\nAfter 5 minutes I’ll go around the room, ask you to share, and transcribe."
  },
  {
    "objectID": "content/week0-intro.html#what-is-probability",
    "href": "content/week0-intro.html#what-is-probability",
    "title": "Welcome to STAT 425!",
    "section": "What is probability?",
    "text": "What is probability?\nBased on our collective facts, can we…\n\nidentify common themes?\nposit a rough definition or description consistent with our ideas?"
  },
  {
    "objectID": "content/week0-intro.html#interpretations-of-probability",
    "href": "content/week0-intro.html#interpretations-of-probability",
    "title": "Welcome to STAT 425!",
    "section": "Interpretations of probability",
    "text": "Interpretations of probability\nThe expression \\(Pr(\\text{Millicent will go for a hike}) = 0.8\\) represents the statement ‘there’s an 80% chance Millicent will go for a hike’. But what does that mean?\n\n(subjective) we are 80% certain that Millicent will go for a hike\n(objective) if we keep presenting Millicent with the same opportunity and circumstances, she’ll go for a hike on 8 out of every 10 occasions"
  },
  {
    "objectID": "content/week0-intro.html#interpretations-of-probability-1",
    "href": "content/week0-intro.html#interpretations-of-probability-1",
    "title": "Welcome to STAT 425!",
    "section": "Interpretations of probability",
    "text": "Interpretations of probability\nThe subjective and objective interpretations of probability have names.\n\nepistemic interpretation: probability statements indicate degrees of belief\naleatory interpretation: probability statements indicate frequencies of occurrence\n\nThe mathematics we’ll study are agnostic on the question of interpretation, though classical examples tend to align more naturally with the aleatory view (games of chance, dice rolls, etc.)."
  },
  {
    "objectID": "content/week0-intro.html#an-informal-definition",
    "href": "content/week0-intro.html#an-informal-definition",
    "title": "Welcome to STAT 425!",
    "section": "An informal definition",
    "text": "An informal definition\nProbability is the mathematics of random events.\nIn this class we’ll develop a formal language for probability in terms of outcomes and events in a sample space:\n\noutcome — result of an experiment or random process\nevent — statement about outcomes\nsample space — collection of all possible outcomes\n\nUsing that language, we’ll construct familiar concepts of random variables, distributions, and their properties.\nSince you already have some exposure/intuition from STAT 305 (or similar), the challenging part of this class for most of you will be the novel formalism and the level of problem-solving expected."
  },
  {
    "objectID": "content/week0-intro.html#stat-425-at-a-glance",
    "href": "content/week0-intro.html#stat-425-at-a-glance",
    "title": "Welcome to STAT 425!",
    "section": "STAT 425 at a glance",
    "text": "STAT 425 at a glance\nScope: we’ll cover probability axioms and properties, random variables, univariate and joint distributions and their properties.\nFormat: we’ll develop a set of course notes in class on the whiteboard following outlines that will be posted in advance of each class meeting; these will cover definitions, properties, theorems, and worked examples.\nMaterials: bring note-taking apparatus (pen/paper, tablet, etc.) and an internet-connected device to each class meeting. There is no required textbook, and the recommended text is available in StatLab (25-107B).\nAssessments: weekly homeworks (7); monthly midterms (2); quarterly final exam (1)."
  },
  {
    "objectID": "content/week0-intro.html#learning-outcomes",
    "href": "content/week0-intro.html#learning-outcomes",
    "title": "Welcome to STAT 425!",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n(…with emphasis on key phrases)\n\nuse the axiomatic construction of probability to derive properties of probability measures and conditional probability measures, and apply definitions and properties to solve probability problems \nconstruct probability models for discrete and continuous random variables, develop familiarity with common probability distributions, and use distribution functions to derive properties such as expectations and variances \ndetermine joint distributions for collections of random variables, and use joint distribution functions to (a) derive properties such as covariance and correlation, (b) to determine conditional and marginal distributions, and (c) derive distributions of transformations and functions of one or more random variables \n\nNote the frequent occurrence of the word use! I want you to understand and be able to use the concepts we discuss in class."
  },
  {
    "objectID": "content/week0-intro.html#assessment-and-evaluation",
    "href": "content/week0-intro.html#assessment-and-evaluation",
    "title": "Welcome to STAT 425!",
    "section": "Assessment and evaluation",
    "text": "Assessment and evaluation\n\nHomeworks (50%). Assigned/collected Thursdays; collaboration allowed; evaluated based on completeness, organization, and correctness of selected answers; lowest score dropped from final grade calculation.\nMidterms (30%). Given in class Thursdays of weeks 4 and 8; note sheet allowed; evaluated based on completeness and correctness; higher score weighted more heavily in final grade calculation.\nFinal exam (20%). Administered as scheduled by Registrar; note sheet allowed; cumulative but emphasizes later material; evaluated based on completeness and correctness.\n\nWe aim to return graded work within one week, except for the final exam, which I’ll keep.\nLetter grades (to within \\(\\pm 5\\)): A: 90-100. B: 75-90. C: 60-75. D: 50-60."
  },
  {
    "objectID": "content/week0-intro.html#select-policies",
    "href": "content/week0-intro.html#select-policies",
    "title": "Welcome to STAT 425!",
    "section": "Select policies",
    "text": "Select policies\n\nTime. Expect about 12-16 hours per week, including class time. Let me know if you’re exceeding this amount so I can help.\nCollaboration. Allowed within the class only. Collaborations must reflect a legitimate shared effort, and names of collaborators should be written on submissions.\nAttendance. Everyone is allowed two ‘free’ absences without notice at any time for any reason. Subsequent absences must be excusable and you should notify me by email.\nDeadlines. Everyone is allowed two ‘free’ late homework submissions without notice at any time for any reason. Subsequent late submissions will be evaluated for 75% credit."
  },
  {
    "objectID": "content/week0-intro.html#select-policies-1",
    "href": "content/week0-intro.html#select-policies-1",
    "title": "Welcome to STAT 425!",
    "section": "Select policies",
    "text": "Select policies\n\nGrades. Please report any discrepancies/errors in evaluation promptly (within 1 week of receiving an evaluated submission). Attempting to negotiate grades or seeking reevaluations after final grades are posted is not appropriate.\nCommunication. I prefer office hours and before/in/after class discussion for most matters. I try to respond to email within 48 weekday hours. Please wait a week before sending reminders, unless it’s time sensitive.\nConduct. Instances of academic dishonesty will be reported to OSRR and consequences may range from loss of credit to automatic failure, depending on the severity of the act."
  },
  {
    "objectID": "content/week0-intro.html#general-advice",
    "href": "content/week0-intro.html#general-advice",
    "title": "Welcome to STAT 425!",
    "section": "General advice",
    "text": "General advice\nI want each of you to succeed in this course and am here to help. I have a few recommendations:\n\nspend 15-30 minutes skimming the suggested textbook sections each week in the StatLab\ntake advantage of the collaboration policy and work with classmates on homework\nuse your free absences and late submissions wisely\ncome to me early with any obstacles or difficulties you’re facing\ntake advantage of office hours; it’s time each week that I set aside specifically for you"
  },
  {
    "objectID": "content/week0-intro.html#for-next-time",
    "href": "content/week0-intro.html#for-next-time",
    "title": "Welcome to STAT 425!",
    "section": "For next time",
    "text": "For next time\n\nComplete intake survey\nCheck the website before class for a lecture outline\n\n\n\n\n© 2023 Trevor Ruiz"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Announcements\n\n\n\n\nOffice hours moved to 25-107G for more space; also, time changed to W-Th 1pm-3pm due to room availability\nPractice problems added to midterm study guide\nWeek 4 notes posted\n\n\n\nThis is the class website for Probability Theory (STAT 425) offered at Cal Poly in Fall 2023. Note that this site is for both sections (01-3585 held 4:10pm MTWR and 02-3430 held 5:10pm MTWR).\nMaterial posted on this site is intended for use by currently enrolled students for course-related purposes only. Please do not reproduce any material on this site without permission."
  }
]